library(caret)
install.packages("caret")
install.packages("ggplot2")
install.packages("e1071")
library(caret)
library(e1071)
library(ggplot2)
library(randomForest)
install.packages("randomForest")
install.packages("pheatmap")
library(caret)
library(e1071)
library(ggplot2)
library(randomForest)
library(pheatmap)
library(caret)
library(e1071)
library(ggplot2)
library(randomForest)
library(pheatmap)
data <- read.csv("/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/Crop_recommendation.csv")
head(data)
set.seed(123)
# See the head and tail of data
head(data)
# Check for missing values
sum(is.na(data))
# See the distribution of data
summary(data)
ggplot(data, aes(x=ph, y=label, color=label)) + geom_point()
ggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()
features <- data[,1:7]
target <- data$label
set.seed(123)
trainIndex <- createDataPartition(target, p=0.8, list=FALSE)
trainData <- features[trainIndex, ]
trainLabels <- target[trainIndex]
testData <- features[-trainIndex, ]
testLabels <- target[-trainIndex]
train_control <- trainControl(method="cv", number=10)
# Train the models
set.seed(123)
model_rf <- train(trainData, trainLabels, trControl=train_control, method="rf")
model_svm <- train(trainData, trainLabels, trControl=train_control, method="svmRadial")
model_nb <- train(trainData, trainLabels, trControl=train_control, method="naive_bayes")
# Ensure 'label' is a factor in both training and testing sets
trainLabels <- as.factor(trainLabels)
testLabels <- as.factor(testLabels)
features <- data[,1:7]
target <- data$label
set.seed(123)
trainIndex <- createDataPartition(target, p=0.8, list=FALSE)
trainData <- features[trainIndex, ]
trainLabels <- target[trainIndex]
testData <- features[-trainIndex, ]
testLabels <- target[-trainIndex]
train_control <- trainControl(method="cv", number=10)
# Train the models
set.seed(123)
model_rf <- train(trainData, trainLabels, trControl=train_control, method="rf")
model_svm <- train(trainData, trainLabels, trControl=train_control, method="svmRadial")
model_nb <- train(trainData, trainLabels, trControl=train_control, method="naive_bayes")
# Ensure 'label' is a factor in both training and testing sets
trainLabels <- as.factor(trainLabels)
testLabels <- as.factor(testLabels)
pred_rf <- predict(model_rf, testData)
# Generate confusion matrices
cm_rf <- confusionMatrix(pred_rf, testLabels)
# Convert it to the numeric form and visualize using the heatmap
cm_rf_numeric <- as.matrix(cm_rf)
cm_rf_numeric[] <- as.numeric(cm_rf_numeric)
pheatmap(cm_rf_numeric, display_numbers = T)
pred_svm <- predict(model_svm, testData)
# Generate confusion matrices
cm_svm <- confusionMatrix(pred_svm, testLabels)
# Print confusion matrices
pred_nb <- predict(model_nb, testData)
# Generate confusion matrices
cm_nb <- confusionMatrix(pred_nb, testLabels)
# Convert it to a numeric matrix and creating heatmap based on this:
cm_nb_numeric <- as.matrix(cm_nb)
cm_nb_numeric[] <- as.numeric(cm_nb_numeric)
pheatmap(cm_nb_numeric, display_numbers = T)
mis_rf <- 1 - cm_rf$overall['Accuracy']
mis_svm <- 1 - cm_svm$overall['Accuracy']
mis_nb <- 1 - cm_nb$overall['Accuracy']
# Create a data frame to store the misclassification rates
misclassification <- data.frame(
Model = c("Random Forest", "SVM", "Naive Bayes"),
Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)
)
# Print the misclassification rates
print(misclassification)
# Summarize the results
results <- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))
summary(results)
# Plot the results
dotplot(results, metric = "Accuracy")
# Train the model
model_rf <- train(trainData, trainLabels, trControl=train_control, method="rf", ntree=100)
newData <- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)
prediction <- predict(model_rf, newData)
print(paste("The suitable crop for Blackstone Area is:", prediction))
print(paste("The suitable crop for Blackstone Area is:", prediction))
install.packages("fpc")
library(fpc)
# Load the data into R
data <- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')
# Lets see the couple of rows of this data
head(data)
str(data)
data_num <- data[, sapply(data, is.numeric)]
head(data_num)
med <- data_num[-4]
set.seed(0)  # Setting seed
Dbscan_cl <- dbscan(med, eps = 0.45, MinPts = 5)
Dbscan_cl
# Table
table(Dbscan_cl$cluster, data_num$age)
# Plotting Cluster
plot(Dbscan_cl, med, main = "DBScan")
# Load packages
library(caTools)
install.packages("caTools")
# Load packages
library(caTools)
library(ggplot2)
library(dplyr)
library(broom)
install.packages("broom")
# Load packages
library(caTools)
library(ggplot2)
library(dplyr)
library(broom)
library(ggpubr)
install.packages("ggpubr")
# Load packages
library(caTools)
library(ggplot2)
library(dplyr)
library(broom)
library(ggpubr)
library(corrplot)
install.packages("corrplot")
library(reshape2)
# Load packages
library(caTools)
library(ggplot2)
library(dplyr)
library(broom)
library(ggpubr)
library(corrplot)
library(reshape2)
# Load the data into R
data <- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')
# Lets see the couple of rows of this data
head(data)
# Check for missing values
sum(is.na(data))
# Handle missing values (if any)
# data <- na.omit(data)  # drops rows with missing values
# data <- data[complete.cases(data), ]  # drops rows with missing values
# data$column <- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean
# Visualize the data
ggplot(data, aes(x=bmi, y=charges)) +
geom_point() +
geom_smooth(method=lm, col="red") +
labs(x="Body Mass Index",
y="Insurance Charges",
title="Charge Vs BMI") +
theme_minimal()
# Subset the data to include only numeric columns
data_numeric <- data[, sapply(data, is.numeric)]
# Calculate the correlation matrix
correlation_matrix <- cor(data_numeric)
# Print the correlation matrix
print(correlation_matrix)
# Melt the correlation matrix into a long format
data_melt <- melt(correlation_matrix)
# Create a heatmap with ggplot2
ggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +
geom_tile() +
geom_text(aes(label = round(value, 2)), size = 4) +
scale_fill_gradient2(low="blue", high="red", mid="white",
midpoint=0, limit=c(-1,1), space="Lab",
name="Pearson\nCorrelation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1,
size = 12, hjust = 1),
axis.text.y = element_text(size = 12))
# Create a violin plot for 'age'
ggplot(data, aes(x=sex, y=charges)) +
geom_violin(fill="yellow") +
labs(title="Distribution of Charges by Sex", x="Sex", y="Charges")
# Create a violin plot for 'charges' vs 'sex'
p1 <- ggplot(data, aes(x=sex, y=charges)) +
geom_violin(fill="yellowgreen") +
labs(title="Violin plot of Charges vs Sex", x="Sex", y="Charges")
# Create a violin plot for 'charges' vs 'smoker'
p2 <- ggplot(data, aes(x=smoker, y=charges)) +
geom_violin(fill="goldenrod1") +
labs(title="Violin plot of Charges vs Smoker", x="Smoker", y="Charges")
# Print the plots
print(p1)
print(p2)
# Split the data into training and test sets
set.seed(0)
sampleSplit <- sample.split(Y=data$bmi, SplitRatio=0.7)
trainSet <- subset(x=data, sampleSplit==TRUE)
testSet <- subset(x=data, sampleSplit==FALSE)
# Fit the model
model <- lm(formula=bmi ~ ., data=trainSet)
# Split the data into training and test sets
summary(model)
modelResiduals <- as.data.frame(residuals(model))
ggplot(modelResiduals, aes(residuals(model)))+
geom_histogram(fill = 'mediumorchid1', color = 'black')
preds <- predict(model, testSet)
modelEval <- cbind(testSet$bmi, preds)
colnames(modelEval) <- c('Acual', 'Predicted')
modelEval <- as.data.frame(modelEval)
head(modelEval)
# Calculate error metrics
mse <- mean((modelEval$Acual - modelEval$Predicted)^2)
rmse <- sqrt(mse)
print(rmse)
df <- iris3
head(df)
df <- iris
head(df)
reticulate::repl_python()
reticulate::repl_python()
