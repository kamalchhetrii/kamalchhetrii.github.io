{
  "hash": "38a9770775e52201e19121ee54e9ad1b",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory and Random Variables in Machine Learning\"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-18\"\ncategories:\n  - R\n  - Code\n  - Analysis\n---\n\n\n### Introduction\n\nProbability theory serves as a cornerstone for Machine Learning (ML). It offers an approach to expressing statements and formulating the learning problem. In this blog post we will explore the aspects of probability theory and random variables and their significant impact on ML. Additionally we will provide an example, in R to demonstrate these concepts.\n\n## Probability Theory\n\nProbability theory is a field, within mathematics that tackles the concept of uncertainty. It offers a structure to help us quantify our beliefs or uncertainties. The probability of an event serves as a metric to gauge the likelihood of its occurrence.\n\nIn machine learning we frequently encounter situations where dealing with uncertainty becomes essential. Take the case of constructing a spam classifier for instance. We often find ourselves unsure whether an incoming email should be classified as spam or not. Probability theory acts as a framework to navigate through this uncertainty and make decisions.\n\n## Probability Fundamentals:\n\n### a. Sets, events, and probability:\n\nA **set** is a collection of distinct objects, elements, or points. For example, the set of all fruits in a basket or the set of all students in a class.\n\nA **subset** is a set that is contained within another set. If every element of set A is contained in set B, we denote this by A⊂B. For example, the set of all apples in a basket is a subset of the set of all fruits in the basket.\n\nTwo sets A and B are **mutually exclusive** (or disjoint) if they have no elements in common. This is denoted by A∩B=∅. For example, the set of all apples and the set of all oranges in a basket are mutually exclusive because there are no elements that are both an apple and an orange.\n\nA **random experiment** is an experiment whose result or outcome is uncertain before it is performed. For example, tossing a coin is a random experiment because we are uncertain whether the outcome will be a head or a tail.\n\nThe set of all possible outcomes of a random experiment is called the **sample space**, usually denoted by S or Ω. Each outcome is called a sample point in the sample space. For example, in a coin toss experiment, the sample space S = {Head, Tail}.\n\nAn **event** is a subset of the sample space, i.e., a collection of possible outcomes to which a probability can be assigned. Events that cannot be decomposed are called simple events, otherwise, they are called compound events. For example, getting a head in a coin toss experiment is a simple event.\n\n### b. Conditional probability:\n\nIn machine learning, understanding probability and statistics is crucial. One such concept is conditional probability, which refers to the probability of an event given that another event has occurred. Conditional probability is the probability of an event (A), given that another (B) has already occurred. If the event of interest is A and event B is known or assumed to have occurred, the conditional probability of A given B is usually written as P(A \\| B).\n\nThe formula for conditional probability is defined as:\n\nP(A/B)= P (A∩B)/P(B)\n\n​\n\n![](https://ars.els-cdn.com/content/image/3-s2.0-B9780128200254000063-f06-10-9780128200254.jpg)\n\n[Figure source:](https://www.sciencedirect.com/topics/mathematics/conditional-probability)\n\n## Random Variables\n\nA random variable refers to a variable that takes on values based on the outcomes of an event. There are two types of variables; discrete and continuous. A discrete random variable has an countable number of values while a continuous random variable can have an infinite number of possible values, within a range along the real number line.\n\nWhen it comes to machine learning random variables can represent elements. For instance in a spam classifier a random variable may indicate whether an email is classified as spam (1) or not (0).\n\nTo better understand the comparison, between continuous variables you can refer to the table provided below.\n\n![](https://www.researchgate.net/publication/337071456/figure/tbl1/AS:822449109233664@1573098556987/Discrete-and-continuous-random-variables.png)\n\nFigure credit: [Discrete vs continuous](https://www.researchgate.net/publication/337071456_Dynamic_Location_Referencing_Probability-Based_Decision_System/figures?lo=1&utm_source=google&utm_medium=organic)\n\n## Probability Distributions\n\nA probability distribution describes how a random variable is distributed. It tells us what the probabilities of each outcome are. For discrete random variables, we use a probability mass function (PMF). For continuous random variables, we use a probability density function (PDF).\n\nWith the facts at hand, one can utilize the decision tree depicted in the picture below to get an understanding of some common probability distributions:\n\n![](https://tinyheero.github.io/assets/prob-distr/overview-prob-distr.png)\n\nFigure credit: [Probabilistic approaches to risk by Aswath Damodaran](http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf).\n\n## Discrete Probability Distributions:\n\nA discrete probability distribution applies to scenarios where the set of possible outcomes is discrete. Common examples of discrete probability distributions include the Bernoulli, Binomial, and Poisson distributions.\n\nExample: Binomial distribution\n\nLet's create a hypothetical scenario where we toss a coin 10 times. We are interested in the number of times we get heads. This scenario follows a binomial distribution. Lets do the data analysis for better understanding of it.\n\n[**Load necessary libraries:**]{.underline}\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_4dbe0405e12059b9757bf6e3c7d84312'}\n\n```{.r .cell-code}\n# Load necessary library\nlibrary(ggplot2)\n```\n:::\n\n\n[**Set the random seed for reproducibility:**]{.underline}\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_329ea6b450d3c3d4647393b34b2ad5bf'}\n\n```{.r .cell-code}\n# Set parameters\nsize <- 10\nprob <- 0.5\n\n# Generate binomial data\nset.seed(123)\ndata <- rbinom(1000, size, prob)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\") +\n  labs(x = \"Number of Heads\", y = \"Frequency\", title = \"Binomial Distribution\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Continuous Probability Distributions:\n\nA continuous probability distribution applies to scenarios where the set of possible outcomes is an interval of real numbers. Common examples of continuous probability distributions include the Normal, Exponential, and Beta distributions.\n\nExample: Normal distiribution\n\nLet's understand it using a hypothetical data where we measure the heights of a large group of individuals. The heights of individuals in a large population often follow a normal distribution.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_7f281d9a94fc3ef00fecb3d36852f636'}\n\n```{.r .cell-code}\n# Set parameters\nmean <- 170\nsd <- 10\n\n# Generate normal data\nset.seed(123)\ndata <- rnorm(1000, mean, sd)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\", aes(y = ..density..)) +\n  geom_density(alpha = 0.2, fill = \"#FF6666\") +\n  labs(x = \"Height (cm)\", y = \"Density\", title = \"Normal Distribution\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nUnderstanding the probability using a real example:\n\nConsider a segment of 100 base pairs on the mouse genome. Among these 100 base pairs, there are 18 A's, 22 T's, 33 C's, and 27 G's. What is the probability of observing base A, T, C, and G in the mouse genome?\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_dc8022f92cb9cc3a2377d36e1f27fffb'}\n\n```{.r .cell-code}\n# Number of each base\nnum_A <- 18\nnum_T <- 22\nnum_C <- 33\nnum_G <- 27\n```\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_f5215b04272bf4d9bed2e8c5f5824c65'}\n\n```{.r .cell-code}\n# Total number of base pairs\ntotal_bp <- 100\n\n# Calculate probabilities\nprob_A <- num_A / total_bp\nprob_T <- num_T / total_bp\nprob_C <- num_C / total_bp\nprob_G <- num_G / total_bp\n\n# Print probabilities\nprint(paste(\"Probability of A: \", prob_A))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Probability of A:  0.18\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Probability of T: \", prob_T))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Probability of T:  0.22\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Probability of C: \", prob_C))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Probability of C:  0.33\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Probability of G: \", prob_G))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Probability of G:  0.27\"\n```\n:::\n\n```{.r .cell-code}\n# Visualize the result\nbarplot(c(prob_A, prob_T, prob_C, prob_G), names.arg = c(\"A\", \"T\", \"C\", \"G\"),\n        xlab = \"Base\", ylab = \"Probability\", main = \"Probability of Each Base\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Conclusion:\n\nProbability theory and random variables are essential tools. Understanding discrete and continuous probability distributions is crucial in machine learning. They provide a foundation for many machine learning algorithms and statistical tests. They provide a framework to handle uncertainty and formulate learning problems. Understanding these concepts can help you build more effective and robust machine learning models.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-6_ec0f920d466404c3e209f20a0223c84a'}\n\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-7_7e8c99c7cb267d9a1ab55bb5ca45c40c'}\n\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-8_50c842a67bb1a77ec187ad99672462e0'}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}