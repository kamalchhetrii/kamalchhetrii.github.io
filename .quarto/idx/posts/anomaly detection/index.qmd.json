{"title":"Anomaly Detection in Machine Learning","markdown":{"yaml":{"title":"Anomaly Detection in Machine Learning","author":"Kamal Chhetri","date":"2023-10-18","categories":["R","Code","Analysis"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n## Introduction\n\nAnomaly detection, also known as outlier detection, is a fascinating aspect of machine learning. It involves identifying data points, events, or observations that deviate significantly from the norm. These anomalies can often provide critical and actionable insights in various domains, such as fraud detection in banking, intrusion detection in network security, and fault detection in critical systems.\n\n## What is Anomaly Detection?\n\nAnomaly detection is the process of identifying unexpected items or events in datasets, which differ from the norm. In other words, it's about finding the 'outliers' in your data. For example, in a manufacturing context, an anomalous event could be a sudden increase in defective products.\n\n## Types of Anomalies\n\nThere are three main types of anomalies:\n\n1.  **Point Anomalies**: A single instance of data is anomalous if it's too far off from the rest. For example, spending \\$100 on food every day during the holiday season is normal, but may be odd otherwise.\n\n2.  **Contextual Anomalies**: The abnormality is context-specific. This type of anomaly is common in time-series data. For example, spending \\$100 on food during the holiday season is normal, but may be odd otherwise.\n\n3.  **Collective Anomalies**: A set of data instances collectively helps in detecting anomalies. For example, someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack.\n\n## Anomaly Detection Techniques\n\nThere are several techniques used for anomaly detection, each with its strengths and weaknesses. Some of the most popular methods include:\n\n1.  **Statistical Methods**: These methods model the normal data behavior using statistical parameters like mean, median, mode, variance, etc. Any data instance that doesn't fit this model is considered an anomaly.\n\n2.  **Machine Learning-Based Methods**: These include techniques like clustering, classification, and nearest neighbors. These methods can either be supervised (labels are available) or unsupervised (no labels).\n\n3.  **Time Series Analysis**: This is particularly useful for sequential data, where some pattern or trend is expected. Techniques used here include state space models, decomposition methods, etc.\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190418023034/781ff66c-b380-4a78-af25-80507ed6ff26.jpeg){alt=\"In this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\"}\n\nLet's get familiar with the fundamental idea underlying the hyperparameters before working with the data. We must examine a few of the hyperparameters that characterize the DBScan job in order to comprehend the idea of the core points. min_samples is the first hyperparameter (HP). This is the bare minimum of core points required for cluster formation. The second crucial HP is 'eps'. \"eps\" is the greatest separation that two samples must have in order to be grouped together. Although border points are somewhat farther from the cluster center, they are nonetheless part of the same cluster as core points. All other data points are referred to as \"Noise Points\" because they are unrelated to any cluster. They require more research because they may be unusual or not.\n\n## About the data:\n\nThere are seven columns and 1338 rows in the data, which indicates that there are seven separate variables. The remaining seven variables---age, sex, bmi, children, smoker, region, and charges. You can access the [data from here](https://github.com/kamalchhetrii/kamalchhetrii.github.io/blob/main/insurance.csv):\n\n## Import necessary libraries:\n\n```{r}\nlibrary(fpc)\nlibrary(ggplot2)\nlibrary(dbscan)\nlibrary(cluster)\nlibrary(FNN)\nlibrary(corrplot)\n\n```\n\n## Import the data set and visualize the data:\n\n```{r}\n\ndata <- read.csv(\"/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/insurance.csv\")\n\n```\n\n```{r}\nhead(data)\n```\n\n### Lets see if the data contains any null values:\n\n```{r}\nsum(is.na(data))\n```\n\nThis data set does not contain null values. Thus, we don't have to deal with it further.\n\n### \n\n## Explanatory analysis:\n\n```{r}\n# Select the relevant columns\ndata_selected <- data[, c('age', 'bmi', 'charges', 'children')]\n\n# Calculate the correlation matrix\ncorrs <- cor(data_selected)\n\n# Create a heatmap with correlation values\ncorrplot(corrs, method=\"color\", type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", # Change correlation coefficients color to white\n         tl.col=\"black\", tl.srt=45, # Text label color and rotation\n         col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(200), # Change color scheme\n         title=\"Correlation Heatmap\") # Add title\n\n\n```\n\nFrom this, we can see there is not much strong correlation between the different variables.\n\n### Lets see the distribution of charges variable:\n\n```{r}\n# Create a histogram of charges\nggplot(data, aes(x=charges)) +\n  geom_histogram(binwidth=1000, color=\"black\", fill=\"lightblue\") +\n  labs(title=\"Distribution of Charges\", x=\"Charges\", y=\"Count\") +\n  theme_minimal()\n```\n\nFrom this histogram, we can see the distribution of our charges data which is left skewed and there is the possibility that this data set contains the outlier.\n\n```{r}\n\n\n```\n\nLet's explore the data set for charges for males and female:\n\n```{r}\n# Create a boxplot of charges by sex\nggplot(data, aes(x=sex, y=charges, color=sex)) +\n  geom_boxplot() +  # Include outliers\n  geom_jitter(width=0.2, alpha=0.5) +  # Add jittered points for better visualization\n  labs(title=\"Charges by Sex\", x=\"Sex\", y=\"Charges\") +\n  theme_minimal() +\n  scale_color_manual(values=c(\"blue\", \"darkgreen\"))  # Specify colors for each sex\n\n```\n\nNow, lets see the charges by bmi for further data exploration:\n\n```{r}\n\n\n```\n\n```{r}\n# Create a scatter plot of BMI vs charges\nggplot(data, aes(x=bmi, y=charges, color=sex)) +\n  geom_point(alpha=0.5) +  # Add points with transparency for better visualization\n  labs(title=\"BMI vs Charges\", x=\"BMI\", y=\"Charges\") +\n  theme_minimal() +\n  scale_color_manual(values=c(\"red\", \"blue\"))  # Specify colors for each sex\n\n```\n\n### \n\n## Calculate the epsilon value using K-distance graph:\n\n```{r}\n# Load necessary libraries\nlibrary(FNN)\n\n# Select the relevant columns\ndata_selected <- data[, c('age', 'bmi')]\n\n# Standardize the data\ndata_std <- scale(data_selected)\n\n# Compute the nearest neighbors\nk <- 2  # 2 because the point itself is included\nknn_dist <- knn.dist(data_std, k=k)\n\n# Sort the distances\nknn_dist <- sort(knn_dist[,k], decreasing=FALSE)  # Exclude the distance to the point itself\n\n# Plot the k-distance graph\nplot(knn_dist, main=\"K-distance Graph\", xlab=\"Data Points sorted by distance\", ylab=\"Epsilon\")\n\n\n```\n\nThe optimum value of epsilon is at the point of maximum curvature in the K-Distance Graph, which is 0.6 in this case. Domain knowledge affects minPoints' value. I'm using 10 minPoints at this time around.\n\nBased on the above calculation, we can do the DBSCAN clustering\n\n```{r}\n# Select the relevant columns\ndata_selected <- data[, c('age', 'bmi')]\n\n# Standardize the data\ndata_std <- scale(data_selected)\n\n# Perform DBSCAN clustering\ndbscan_res <- dbscan(data_std, eps=0.6, minPts=10)  \nprint(dbscan_res)\n\n```\n\n## \n\n```{r}\n\n```\n\n## Visualization:\n\n```{r}\n# Add the DBSCAN results to our data\ndata$cluster <- as.factor(dbscan_res$cluster)\n\n# Define colors for each cluster\ncolors <- rainbow(length(unique(dbscan_res$cluster)))\nnames(colors) <- levels(data$cluster)\n\n# Create the plot\nggplot(data, aes(x=age, y=bmi, color=cluster)) +\n  geom_point() +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  labs(x=\"Age\", y=\"BMI\", color=\"Cluster\") +\n  theme(legend.position=\"none\")\n```\n\nFrom the above scatterplot, we can see that three data points are the noise and one cluster with no outliers.\n\n```{r}\n# Create an outliers data frame\noutliers <- data[data$cluster == -1, ]\n\n# Print the outliers\nprint(outliers)\n\n```\n\n```{r}\n\n```\n\n## \n\n```{r}\n\n\n```\n\n## \n\n```{r}\n\n```\n\n## \n\n```{r}\n\n```\n\n## \n\n```{r}\n\n\n```\n\n## \n\n```{r}\n\n```\n\n## \n\n```{r}\n\n\n```\n\n## \n\n```{r}\n\n```\n\n```{r}\n\n\n```\n\n```{r}\n\n\n```\n\n```{r}\n\n\n```\n\n```{r}\n\n\n```\n","srcMarkdownNoYaml":"\n\n### Introduction\n\n## Introduction\n\nAnomaly detection, also known as outlier detection, is a fascinating aspect of machine learning. It involves identifying data points, events, or observations that deviate significantly from the norm. These anomalies can often provide critical and actionable insights in various domains, such as fraud detection in banking, intrusion detection in network security, and fault detection in critical systems.\n\n## What is Anomaly Detection?\n\nAnomaly detection is the process of identifying unexpected items or events in datasets, which differ from the norm. In other words, it's about finding the 'outliers' in your data. For example, in a manufacturing context, an anomalous event could be a sudden increase in defective products.\n\n## Types of Anomalies\n\nThere are three main types of anomalies:\n\n1.  **Point Anomalies**: A single instance of data is anomalous if it's too far off from the rest. For example, spending \\$100 on food every day during the holiday season is normal, but may be odd otherwise.\n\n2.  **Contextual Anomalies**: The abnormality is context-specific. This type of anomaly is common in time-series data. For example, spending \\$100 on food during the holiday season is normal, but may be odd otherwise.\n\n3.  **Collective Anomalies**: A set of data instances collectively helps in detecting anomalies. For example, someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack.\n\n## Anomaly Detection Techniques\n\nThere are several techniques used for anomaly detection, each with its strengths and weaknesses. Some of the most popular methods include:\n\n1.  **Statistical Methods**: These methods model the normal data behavior using statistical parameters like mean, median, mode, variance, etc. Any data instance that doesn't fit this model is considered an anomaly.\n\n2.  **Machine Learning-Based Methods**: These include techniques like clustering, classification, and nearest neighbors. These methods can either be supervised (labels are available) or unsupervised (no labels).\n\n3.  **Time Series Analysis**: This is particularly useful for sequential data, where some pattern or trend is expected. Techniques used here include state space models, decomposition methods, etc.\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190418023034/781ff66c-b380-4a78-af25-80507ed6ff26.jpeg){alt=\"In this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\"}\n\nLet's get familiar with the fundamental idea underlying the hyperparameters before working with the data. We must examine a few of the hyperparameters that characterize the DBScan job in order to comprehend the idea of the core points. min_samples is the first hyperparameter (HP). This is the bare minimum of core points required for cluster formation. The second crucial HP is 'eps'. \"eps\" is the greatest separation that two samples must have in order to be grouped together. Although border points are somewhat farther from the cluster center, they are nonetheless part of the same cluster as core points. All other data points are referred to as \"Noise Points\" because they are unrelated to any cluster. They require more research because they may be unusual or not.\n\n## About the data:\n\nThere are seven columns and 1338 rows in the data, which indicates that there are seven separate variables. The remaining seven variables---age, sex, bmi, children, smoker, region, and charges. You can access the [data from here](https://github.com/kamalchhetrii/kamalchhetrii.github.io/blob/main/insurance.csv):\n\n## Import necessary libraries:\n\n```{r}\nlibrary(fpc)\nlibrary(ggplot2)\nlibrary(dbscan)\nlibrary(cluster)\nlibrary(FNN)\nlibrary(corrplot)\n\n```\n\n## Import the data set and visualize the data:\n\n```{r}\n\ndata <- read.csv(\"/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/insurance.csv\")\n\n```\n\n```{r}\nhead(data)\n```\n\n### Lets see if the data contains any null values:\n\n```{r}\nsum(is.na(data))\n```\n\nThis data set does not contain null values. Thus, we don't have to deal with it further.\n\n### \n\n## Explanatory analysis:\n\n```{r}\n# Select the relevant columns\ndata_selected <- data[, c('age', 'bmi', 'charges', 'children')]\n\n# Calculate the correlation matrix\ncorrs <- cor(data_selected)\n\n# Create a heatmap with correlation values\ncorrplot(corrs, method=\"color\", type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", # Change correlation coefficients color to white\n         tl.col=\"black\", tl.srt=45, # Text label color and rotation\n         col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(200), # Change color scheme\n         title=\"Correlation Heatmap\") # Add title\n\n\n```\n\nFrom this, we can see there is not much strong correlation between the different variables.\n\n### Lets see the distribution of charges variable:\n\n```{r}\n# Create a histogram of charges\nggplot(data, aes(x=charges)) +\n  geom_histogram(binwidth=1000, color=\"black\", fill=\"lightblue\") +\n  labs(title=\"Distribution of Charges\", x=\"Charges\", y=\"Count\") +\n  theme_minimal()\n```\n\nFrom this histogram, we can see the distribution of our charges data which is left skewed and there is the possibility that this data set contains the outlier.\n\n```{r}\n\n\n```\n\nLet's explore the data set for charges for males and female:\n\n```{r}\n# Create a boxplot of charges by sex\nggplot(data, aes(x=sex, y=charges, color=sex)) +\n  geom_boxplot() +  # Include outliers\n  geom_jitter(width=0.2, alpha=0.5) +  # Add jittered points for better visualization\n  labs(title=\"Charges by Sex\", x=\"Sex\", y=\"Charges\") +\n  theme_minimal() +\n  scale_color_manual(values=c(\"blue\", \"darkgreen\"))  # Specify colors for each sex\n\n```\n\nNow, lets see the charges by bmi for further data exploration:\n\n```{r}\n\n\n```\n\n```{r}\n# Create a scatter plot of BMI vs charges\nggplot(data, aes(x=bmi, y=charges, color=sex)) +\n  geom_point(alpha=0.5) +  # Add points with transparency for better visualization\n  labs(title=\"BMI vs Charges\", x=\"BMI\", y=\"Charges\") +\n  theme_minimal() +\n  scale_color_manual(values=c(\"red\", \"blue\"))  # Specify colors for each sex\n\n```\n\n### \n\n## Calculate the epsilon value using K-distance graph:\n\n```{r}\n# Load necessary libraries\nlibrary(FNN)\n\n# Select the relevant columns\ndata_selected <- data[, c('age', 'bmi')]\n\n# Standardize the data\ndata_std <- scale(data_selected)\n\n# Compute the nearest neighbors\nk <- 2  # 2 because the point itself is included\nknn_dist <- knn.dist(data_std, k=k)\n\n# Sort the distances\nknn_dist <- sort(knn_dist[,k], decreasing=FALSE)  # Exclude the distance to the point itself\n\n# Plot the k-distance graph\nplot(knn_dist, main=\"K-distance Graph\", xlab=\"Data Points sorted by distance\", ylab=\"Epsilon\")\n\n\n```\n\nThe optimum value of epsilon is at the point of maximum curvature in the K-Distance Graph, which is 0.6 in this case. Domain knowledge affects minPoints' value. I'm using 10 minPoints at this time around.\n\nBased on the above calculation, we can do the DBSCAN clustering\n\n```{r}\n# Select the relevant columns\ndata_selected <- data[, c('age', 'bmi')]\n\n# Standardize the data\ndata_std <- scale(data_selected)\n\n# Perform DBSCAN clustering\ndbscan_res <- dbscan(data_std, eps=0.6, minPts=10)  \nprint(dbscan_res)\n\n```\n\n## \n\n```{r}\n\n```\n\n## Visualization:\n\n```{r}\n# Add the DBSCAN results to our data\ndata$cluster <- as.factor(dbscan_res$cluster)\n\n# Define colors for each cluster\ncolors <- rainbow(length(unique(dbscan_res$cluster)))\nnames(colors) <- levels(data$cluster)\n\n# Create the plot\nggplot(data, aes(x=age, y=bmi, color=cluster)) +\n  geom_point() +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  labs(x=\"Age\", y=\"BMI\", color=\"Cluster\") +\n  theme(legend.position=\"none\")\n```\n\nFrom the above scatterplot, we can see that three data points are the noise and one cluster with no outliers.\n\n```{r}\n# Create an outliers data frame\noutliers <- data[data$cluster == -1, ]\n\n# Print the outliers\nprint(outliers)\n\n```\n\n```{r}\n\n```\n\n## \n\n```{r}\n\n\n```\n\n## \n\n```{r}\n\n```\n\n## \n\n```{r}\n\n```\n\n## \n\n```{r}\n\n\n```\n\n## \n\n```{r}\n\n```\n\n## \n\n```{r}\n\n\n```\n\n## \n\n```{r}\n\n```\n\n```{r}\n\n\n```\n\n```{r}\n\n\n```\n\n```{r}\n\n\n```\n\n```{r}\n\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","page-layout":"full","theme":{"light":["flatly","light.scss"],"dark":["darkly","dark.scss"]},"grid":{"sidebar-width":"220px","body-width":"1600px","margin-width":"250px"},"title-block-banner":true,"title":"Anomaly Detection in Machine Learning","author":"Kamal Chhetri","date":"2023-10-18","categories":["R","Code","Analysis"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}