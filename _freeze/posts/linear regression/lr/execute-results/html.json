{
  "hash": "61d48fec7b96fb9053f651b1733bfb81",
  "result": {
    "markdown": "---\ntitle: \"Understanding Linear and Non-Linear Regression\"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-18\"\ncategories:\n  - R\n  - Code\n  - Analysis\n---\n\n\n# A. Linear regression:\n\n# Introduction:\n\nLinear regression is a powerful statistical method that allows us to examine the relationship between two (simple linear regression) or more (multiple linear regression) variables. A key feature of linear regression is its simplicity and interpretability. It's used in various fields, including machine learning, most medical fields, and social sciences.\n\nLinear regression models the relationship between two variables by fitting a linear equation to observed data. The steps to perform multiple linear regression are almost identical to those of simple linear regression.\n\n![](https://editor.analyticsvidhya.com/uploads/375512.jpg)\n\n[**Types of regression:**]{.underline}\n\na\\. Simple linear regression: In this case, we use only one input variable\n\nb\\. Multiple linear regression: In this case, We use multiple input variable\n\n### The Dataset:\n\nThere are seven columns and 1338 rows in the data, which indicates that there are seven separate variables. The remaining six variables---age, sex, bmi, children, smoker, and region---are independent variables, whereas charges is the target variable in this instance. To examine the data, multiple linear regression must be fitted because there are several independent variables. You can access this data from [here.](https://github.com/kamalchhetrii/kamalchhetrii.github.io/blob/main/insurance.csv)\n\n### Assumptions:\n\nThere are several assumption for the linear regression model that I am using in this blog. They are explained below, lets take a look.\n\n[**Linearity:**]{.underline} The relationship between the independent and dependent variables is linear. This assumption can be checked by plotting the variables and examining the data scatter.\n\n[**Independence:**]{.underline} The residuals (the differences between the observed and predicted values) are independent. In other words, there is not a specific pattern in the residuals.\n\n[**Homoscedasticity:**]{.underline} The variance of the residuals is constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all predicted values.\n\n[**Normality:**]{.underline} The residuals are normally distributed. If this assumption is violated, then the confidence intervals may not be accurate.\n\n[**Absence of multicollinearity:**]{.underline} In the case of multiple linear regression, the independent variables should not be too highly correlated with each other.\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-1_13f465b83531446284eb65c28a250662'}\n\n:::\n\n\n## **Load necessary libraries:**\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-2_8726d719c891470687167ff6ac10a0e1'}\n\n```{.r .cell-code}\n# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\nlibrary(reshape2)\nlibrary(Metrics)\nlibrary(minpack.lm)\n```\n:::\n\n\n### Load the data:\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-3_7546e411f6172e0df9649efc725aa166'}\n\n```{.r .cell-code}\n# Load the data into R\ndata <- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n```\n:::\n:::\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-4_ad2d872aa7a431531ad670dd8100e99b'}\n\n```{.r .cell-code}\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ...\n```\n:::\n:::\n\n\n### Load and clean the data:\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-5_be011b8fe6a37eff7b3c793ee7e44398'}\n\n```{.r .cell-code}\n# Check for missing values\nsum(is.na(data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n# Handle missing values (if any)\n# data <- na.omit(data)  # drops rows with missing values\n# data <- data[complete.cases(data), ]  # drops rows with missing values\n# data$column <- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n```\n:::\n\n\nIn my case, the data does not contain the null values; however, if you select the data with the null values and want to analyze the data, it is important to deal with the null values before proceeding further analysis. Different ways to deal with the null values in R is given in the code above.\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-6_5fdd2b1b1bd74fcb3ff811691ca1177a'}\n\n:::\n\n\nUpon executing the above code, we got the value 0, which means there are no missing values in the data. Since we don't have any null or missing values in this data, as indicated above. So, no further handling of missing data is needed.\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-7_0a3831dd41890a5740e18fe3cf988573'}\n\n:::\n\n\n### Visualize the data: for better understanding of the data before proceeding further:\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-8_778a9700c049ad4d86269208cdb95e5c'}\n\n```{.r .cell-code}\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nLets take a look for the correlation between these two features: Our data contains one column \"Species\" which is non-numeric. Thus, we will firstly subset the data that doesnot contains \"Species\" column.\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-9_7b08daadcdac55a67fd3d47e5f01a912'}\n\n```{.r .cell-code}\n# Subset the data to include only numeric columns\ndata_numeric <- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix <- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n```\n:::\n:::\n\n\n### Create the heatmap for better visualization:\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-10_abd06f61ee90d36d7f370b738fb5f80e'}\n\n```{.r .cell-code}\n# Melt the correlation matrix into a long format\ndata_melt <- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n```\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWe can see no strong correlation between different variables from the heatmap created using the above code. However, this part is made to visualize the data only.\n\n### Lets explore the data for charges by region:\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-11_ad35bbe08dfe49df848645ba80dad8e9'}\n\n```{.r .cell-code}\n# Sum 'charges' grouped by 'region'\ncharges <- aggregate(data$charges, by=list(data$region), FUN=sum)\n\n# Sort the data frame by 'charges' in ascending order\ncharges <- charges[order(charges$x, decreasing=FALSE), ]\n\n# Create a bar plot\nggplot(charges[1:5, ], aes(x=Group.1, y=x, fill=x)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  labs(title=\"Sum of Charges by Region\", x=\"Region\", y=\"Charges\") +\n  theme_minimal() +\n  scale_fill_gradient(low = \"lightgreen\", high = \"blue\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n```\n:::\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nTherefore, the Southeast has the highest overall medical costs, whereas the Southwest has the lowest.\n\nIt is always interesting to explore the data; in this case, I am interested in seeing how smoking habits affect the charges associated with insurance. Let's explore this below:\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-12_39a071dd80b5caa9afb0c4ddd77b2209'}\n\n```{.r .cell-code}\n# Create a scatter plot for 'age' vs 'charges' colored by 'smoker'\np1 <- ggplot(data, aes(x=age, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"red\", \"blue\")) +\n  labs(title=\"Scatter plot of Charges vs Age\", x=\"Age\", y=\"Charges\")\n\n# Create a scatter plot for 'bmi' vs 'charges' colored by 'smoker'\np2 <- ggplot(data, aes(x=bmi, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"red\", \"blue\")) +\n  labs(title=\"Scatter plot of Charges vs BMI\", x=\"BMI\", y=\"Charges\")\n\n# Create a scatter plot for 'children' vs 'charges' colored by 'smoker'\np3 <- ggplot(data, aes(x=children, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"green\", \"purple\")) +\n  labs(title=\"Scatter plot of Charges vs Children\", x=\"Children\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(p2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(p3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n:::\n\n\nOverall, the insurance charge is significantly higher for an individual who smokes.\n\n## \n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-13_c1f5a59b1cfb210aaab3895d8b9246c3'}\n\n:::\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-14_e8c3cfc1b45b00ef5b72ddd21d0a872f'}\n\n```{.r .cell-code}\n# Create a box plot for 'charges' vs 'sex'\np1 <- ggplot(data, aes(x=sex, y=charges)) +\n  geom_boxplot(fill=\"yellowgreen\") +\n  labs(title=\"Box plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a box plot for 'charges' vs 'smoker'\np2 <- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_boxplot(fill=\"goldenrod1\") +\n  theme_minimal() +\n  labs(title=\"Box plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n```\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nprint(p2)\n```\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\nAccording to the plot, the average insurance cost for men and women is roughly the same at \\$5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about \\$5,000. The minimum insurance premium for smokers is \\$5,000.\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-15_c9de772764a919a90e1d8aee577dd4c5'}\n\n```{.r .cell-code}\n# Create the box plot\nggplot(data, aes(x = as.factor(children), y = charges, fill = sex)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +\n  theme_minimal() +\n  labs(title = \"Box plot of charges vs children\", x = \"Children\", y = \"Charges\", fill = \"Sex\")\n```\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWe can see some outliers data in different children age group.\n\n## **Building the model and prediction:**\n\nFrom the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\n\n[**Model evaluation:**]{.underline} After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse) and R squared value.\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-16_bb7290dfccb575346dd5cfcd8f8e5113'}\n\n```{.r .cell-code}\n# Set the seed for reproducibility\nset.seed(0)\n\n# Create the independent variable data frame\nx <- data[, !(names(data) %in% \"charges\")]\n\n# Create the dependent variable vector\ny <- data$charges\n\n# Split the data into training and testing sets\nsplit <- sample.split(y, SplitRatio = 0.8)\nx_train <- x[split, ]\ny_train <- y[split]\nx_test <- x[!split, ]\ny_test <- y[!split]\n\n# Fit a linear regression model\nLin_reg <- lm(y_train ~ ., data = cbind(x_train, y_train))\n\n# Print the intercept and coefficients\nprint(paste(\"Intercept: \", Lin_reg$coefficients[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Intercept:  -12169.8051075584\"\n```\n:::\n\n```{.r .cell-code}\nprint(paste(\"Coefficients: \", Lin_reg$coefficients[-1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Coefficients:  262.305667539361\"  \"Coefficients:  -6.522038182275\"  \n[3] \"Coefficients:  344.253140133108\"  \"Coefficients:  453.766808768054\" \n[5] \"Coefficients:  23996.309146369\"   \"Coefficients:  -751.161736087778\"\n[7] \"Coefficients:  -1129.0287998095\"  \"Coefficients:  -1085.05633010975\"\n```\n:::\n\n```{.r .cell-code}\n# Calculate R-squared on the test set\ny_pred <- predict(Lin_reg, newdata = x_test)\nSSE <- sum((y_pred - y_test)^2)\nSST <- sum((mean(y_train) - y_test)^2)\nR_squared <- 1 - SSE/SST\nprint(paste(\"R-squared: \", R_squared))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"R-squared:  0.736401619028354\"\n```\n:::\n\n```{.r .cell-code}\n# Calculate RMSE\nrmse_val <- rmse(y_test, y_pred)\nprint(paste(\"RMSE: \", rmse_val))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"RMSE:  5903.79948175291\"\n```\n:::\n:::\n\n\n[**Lets learn little bit about rmse:**]{.underline} The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\n\nDuring model evaluation, RMSE serves as a measure to understand the model's performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\n\nIn this case, the R-squared value is approximately 0.7364, which means that our model explains about 73.64% of the variance in the charges.\n\n# \n\n# B. Non- linear regression model:\n\nNon-linear regression is a form of regression analysis in which observational data are modeled by a function which is a non-linear combination of the model parameters and depends on one or more independent variables. The data is fitted by a method of successive approximations. Non-linear regression models are generally assumed to be parametric, where the model is described as a nonlinear equation.\n\nNon-linear regression is used when the relationship between the independent and dependent variables is not linear, or when the data is not normally distributed, or involves complex relationships. Non-linear regression can capture complex patterns and interactions and provide impressive results in performance, stability, and precision.\n\n### Data:\n\nYou can access the data [from here](https://www.kaggle.com/datasets/iansurii/china-gdp-dataset). This is the data about the china's GDP per year which is growing in an exponential rate.\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-17_84aa5990d3fd3810f2bbf5740a84341c'}\n\n```{.r .cell-code}\ndf <- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kamalchhetrii.github.io/china_gdp.csv\")\n```\n:::\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-18_30175b6312e3fe1478a3d6826ad998b7'}\n\n```{.r .cell-code}\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Year       Value\n1 1960 59184116489\n2 1961 49557050183\n3 1962 46685178504\n4 1963 50097303271\n5 1964 59062254890\n6 1965 69709153115\n```\n:::\n:::\n\n\n### Explanatory visualization:\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-19_8f4bf39aa4e0c6057df4c8b1e5f0c08d'}\n\n```{.r .cell-code}\n# Define the data\nx_data <- df[[\"Year\"]]\ny_data <- df[[\"Value\"]]\n\n# Create the plot\nggplot(df, aes(x = x_data, y = y_data)) +\n  geom_point(color = \"darkgreen\") +\n  labs(x = \"Year\", y = \"GDP\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### Building the model:\n\nThe logistic function appears to be a reasonable approximation based on the plot's first appearance. This is because the logistic function begins slowly, grows more rapidly in the middle, and then declines once more in the conclusion.\n\n![](https://hvidberrrg.github.io/deep_learning/activation_functions/assets/sigmoid_function.png)\n\nFigure source: [**The sigmoid function (a.k.a. the logistic function) and its derivative**](https://hvidberrrg.github.io/deep_learning/activation_functions/sigmoid_function_and_derivative.html)\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-20_5691536afedf2028cb590d0762c57828'}\n\n```{.r .cell-code}\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Define the parameters\nbeta_1 <- 0.10\nbeta_2 <- 1990.0\n\n# Apply the logistic function\nY_pred <- sigmoid(x_data, beta_1, beta_2)\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create a dataframe for plotting\ndf <- data.frame(x = x_data, y = Y_pred * 15000000000000., y_actual = y_data)\n\n# Plot the initial prediction against the data points\nggplot(df, aes(x = x)) +\n  geom_line(aes(y = y), color = \"red\") +\n  geom_point(aes(y = y_actual), color = \"darkgreen\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\")\n```\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nNow, we can see the sigmoid and the our actual data plotted in here. We have find the best parameter for our data. We can fit our sigmoid function to the data using curve_fit, which applies non-linear least squares. It is necessary to normalize our x and y variable before proceeding to further data analysis.\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-21_11b20d210fa13dfd32d004e7536d1d13'}\n\n```{.r .cell-code}\n# Normalize the data\nxdata <- x_data / max(x_data)\nydata <- y_data / max(y_data)\n```\n:::\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-22_6e96a0f6c5dc93263b131384985b8443'}\n\n```{.r .cell-code}\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Initial parameter values\nstart <- c(Beta_1 = 1, Beta_2 = 1)\n\n# Non-linear least squares fit\nfit <- nlsLM(ydata ~ sigmoid(xdata, Beta_1, Beta_2), start = start)\n\n# Print the final parameters\nprint(coef(fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Beta_1      Beta_2 \n690.4517142   0.9972071 \n```\n:::\n:::\n\n\n## Lets visualize the result obtained from model:\n\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-23_7dbcc16c3db51b373c94099014757bd6'}\n\n```{.r .cell-code}\n# Define the x values\nx <- seq(from = 1960, to = 2015, length.out = 55)\nx <- x / max(x)\n\n# Calculate the y values\ny <- sigmoid(x, coef(fit)[1], coef(fit)[2])\n\n# Create a dataframe for plotting\ndf <- data.frame(x = c(xdata, x), y = c(ydata, y), group = rep(c(\"data\", \"fit\"), each = 55))\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create the plot\nggplot(df, aes(x = x, y = y, color = group)) +\n  geom_point(data = df[df$group == \"data\", ]) +\n  geom_line(data = df[df$group == \"fit\", ], size = 1.5) +\n  scale_color_manual(values = c(\"darkgreen\", \"blue\")) +\n  labs(x = \"Year\", y = \"GDP\", color = \"Legend\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](lr_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='lr_cache/html/unnamed-chunk-24_e87d1720a6c449316984f5af095757d2'}\n\n```{.r .cell-code}\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Split data into train/test\nset.seed(123) # for reproducibility\nmsk <- runif(nrow(df)) < 0.8\ntrain_x <- xdata[msk]\ntest_x <- xdata[!msk]\ntrain_y <- ydata[msk]\ntest_y <- ydata[!msk]\n\n# Initial parameter values\nstart <- c(Beta_1 = 1, Beta_2 = 1)\n\n# Build the model using train set\nfit <- nlsLM(train_y ~ sigmoid(train_x, Beta_1, Beta_2), start = start)\n\n# Print the final parameters\nprint(coef(fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Beta_1      Beta_2 \n696.7862319   0.9971532 \n```\n:::\n\n```{.r .cell-code}\n# Predict using test set\ny_hat <- predict(fit, list(x = test_x))\n\nprint(y_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 5.594781e-08 7.907452e-08 1.117609e-07 3.155369e-07 4.459679e-07\n [6] 8.908616e-07 1.259109e-06 2.515184e-06 3.554862e-06 5.024300e-06\n[11] 7.101141e-06 1.418508e-05 2.004854e-05 2.833562e-05 7.999641e-05\n[16] 1.130602e-04 2.258226e-04 3.191394e-04 4.510000e-04 6.373074e-04\n[21] 9.005089e-04 1.272272e-03 3.583718e-03 5.057599e-03 7.133306e-03\n[26] 1.005230e-02 1.414875e-02 1.988103e-02 2.787006e-02 3.894186e-02\n[31] 5.416702e-02 7.488098e-02 1.026564e-01 1.391845e-01 1.860159e-01\n[36] 2.441358e-01 3.134227e-01 3.921713e-01 4.769605e-01 6.455921e-01\n[41] 7.202477e-01 7.844284e-01 8.372127e-01 8.790649e-01\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}