{
  "hash": "89fa63e245af7c21db0a0bb8400101ad",
  "result": {
    "markdown": "---\ntitle: \"Probability Theory and Random Variables in Machine Learning\"\nauthor: \"Kamal Chhetri\"\ndate: \"2023-10-18\"\ncategories:\n  - R\n  - Code\n  - Analysis\n---\n\n\n### Introduction\n\nProbability theory is a fundamental pillar of Machine Learning (ML). It provides a framework to represent uncertain statements and formulate the learning problem in a principled way. In this blog post, we will delve into the basics of probability theory and random variables, and how they play a crucial role in ML. We will also illustrate these concepts with an example in R.\n\n## Probability Theory\n\nProbability theory is a branch of mathematics that deals with uncertainty. It provides a mathematical framework for quantifying our belief or uncertainty. The probability of an event is a measure of the likelihood that the event will occur.\n\nIn ML, we often have to deal with uncertainty. For example, when we build a spam classifier, we are uncertain about whether a new incoming email is spam or not. Probability theory provides a solid foundation to reason about this uncertainty.\n\n## Random Variables\n\nA random variable is a variable whose possible values are outcomes of a random phenomenon. There are two types of random variables: discrete and continuous. A discrete random variable has a countable number of possible values. A continuous random variable has an uncountable number of possible values, often represented by an interval on the real number line.\n\nIn the context of ML, random variables could represent various things. For example, in a spam classifier, a random variable could represent whether an email is spam (1) or not (0).\n\n![](https://www.researchgate.net/publication/337071456/figure/tbl1/AS:822449109233664@1573098556987/Discrete-and-continuous-random-variables.png)\n\nFigure credit: [Discrete vs continuous](https://www.researchgate.net/publication/337071456_Dynamic_Location_Referencing_Probability-Based_Decision_System/figures?lo=1&utm_source=google&utm_medium=organic)\n\n## Probability Distributions\n\nA probability distribution describes how a random variable is distributed. It tells us what the probabilities of each outcome are. For discrete random variables, we use a probability mass function (PMF). For continuous random variables, we use a probability density function (PDF).\n\n![](https://tinyheero.github.io/assets/prob-distr/overview-prob-distr.png)\n\nFigure credit: [Probabilistic approaches to risk by Aswath Damodaran](http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf).\n\n## Discrete Probability Distributions:\n\nA discrete probability distribution applies to scenarios where the set of possible outcomes is discrete. Common examples of discrete probability distributions include the Bernoulli, Binomial, and Poisson distributions.\n\nExample: Binomial distribution\n\nLet\\'s create a hypothetical scenario where we toss a coin 10 times. We are interested in the number of times we get heads. This scenario follows a binomial distribution. Lets do the data analysis for better understanding of it.\n\n[**Load necessary libraries:**]{.underline}\n\n\n::: {.cell hash='probability_cache/html/unnamed-chunk-1_9fc7fd7a7555cb21c21c4e1f1cf27f37'}\n\n```{.r .cell-code}\n# Load necessary library\nlibrary(ggplot2)\n```\n:::\n\n\n[**Set the random seed for reproducibility:**]{.underline}\n\n\n::: {.cell hash='probability_cache/html/unnamed-chunk-2_af116118ceb295ff57ff0a1c8f569f6f'}\n\n```{.r .cell-code}\n# Set parameters\nsize <- 10\nprob <- 0.5\n\n# Generate binomial data\nset.seed(123)\ndata <- rbinom(1000, size, prob)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\") +\n  labs(x = \"Number of Heads\", y = \"Frequency\", title = \"Binomial Distribution\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](probability_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Continuous Probability Distributions:\n\nA continuous probability distribution applies to scenarios where the set of possible outcomes is an interval of real numbers. Common examples of continuous probability distributions include the Normal, Exponential, and Beta distributions.\n\nExample: Normal distiribution\n\nLet\\'s create a hypothetical scenario where we measure the heights of a large group of individuals. The heights of individuals in a large population often follow a normal distribution.\n\n\n::: {.cell hash='probability_cache/html/unnamed-chunk-3_8d25c5e28b8fe02928a0d94546123086'}\n\n```{.r .cell-code}\n# Set parameters\nmean <- 170\nsd <- 10\n\n# Generate normal data\nset.seed(123)\ndata <- rnorm(1000, mean, sd)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\", aes(y = ..density..)) +\n  geom_density(alpha = 0.2, fill = \"#FF6666\") +\n  labs(x = \"Height (cm)\", y = \"Density\", title = \"Normal Distribution\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `after_stat(density)` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](probability_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Conclusion:\n\nProbability theory and random variables are essential tools. Understanding discrete and continuous probability distributions is crucial in machine learning. They provide a foundation for many machine learning algorithms and statistical tests. They provide a framework to handle uncertainty and formulate learning problems. Understanding these concepts can help you build more effective and robust machine learning models.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}