{"title":"Understanding DBSCAN Clustering Analysis in Machine Learning","markdown":{"yaml":{"title":"Understanding DBSCAN Clustering Analysis in Machine Learning","author":"Kamal Chhetri","date":"2023-11-24","categories":["R","Code","Analysis"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data's density.\n\n## How DBSCAN Works\n\nDBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least **`minPts`** within a radius of **`eps`** from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset.\n\n## Advantages of DBSCAN\n\nDBSCAN has several advantages over other clustering algorithms:\n\n1.  **No need to specify the number of clusters**: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\n\n2.  **Ability to find arbitrarily shaped clusters**: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\n\n3.  **Robustness to noise**: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria.\n\n## Disadvantages of DBSCAN\n\nDespite its advantages, DBSCAN also has some limitations:\n\n1.  **Difficulty handling varying densities**: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single **`eps`** and **`minPts`** value may not be suitable for all clusters.\n\n2.  **Sensitivity to parameter settings**: The results of DBSCAN can be significantly affected by the settings of **`eps`** and **`minPts`**. Choosing appropriate values for these parameters can be challenging.\n\nIn general, density-based clustering algorithms can be quite successful for a wide range of clustering tasks, particularly when the data is shaped and has different densities. When using the algorithm with a specific dataset, it is crucial to pay close attention to the parameters and take the algorithm's constraints into account.\n\nAccording to the research report, the concept of dense regions forms the basis of DBSCAN. It is assumed that points in dense locations make up natural clusters. The term \"dense region\" has to be defined for this. These two parameters are necessary for the DBSCAN algorithm to function.\n\n-   Eps, ε: distance\n\n-   MinPts: The bare minimum of points within a given distance Eps\n\n![In this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.](https://media.geeksforgeeks.org/wp-content/uploads/20190418023034/781ff66c-b380-4a78-af25-80507ed6ff26.jpeg)\n\n## Data:\n\nIn this blog post, we will do the clustering analysis using the DBSCAN clustering method. Regarding the choice of this algorithm is explain above. Please have a look if you want to learn more. Regarding the data, [you can access this data from here](https://www.kaggle.com/code/anujachintyabiswas/mall-customer-hierarchical-kmeans-clustering/input).\n\n#### Load the necessary libraries:\n\n```{r}\nlibrary(fpc) \nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(corrplot)\nlibrary(dbscan)\n\n```\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \n```\n\n## Load the data into R:\n\n```{r}\ndata <- read.csv(\"/Users/test/Downloads/Mall_customers.csv\")\nhead(data)\n\nstr(data)\n```\n\n# Preprocessing the data:\n\n```{r}\ndata_num <- data[, sapply(data, is.numeric)]\nhead(data_num)\n\n\n```\n\n## Data Cleaning:\n\n```{r}\n# To see if the given dataset contains any null values or not\nsum(is.na(data_num))\n\n```\n\n## Explanatory analysis\n\n```{r}\n# Select the relevant columns\ndata_selected <- data_num[, c('Age', 'Annual_Income', 'Spending_Score')]\n\n# Calculate the correlation matrix\ncorrs <- cor(data_selected)\n\n# Create a heatmap with correlation values\ncorrplot(corrs, method=\"color\", type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", # Add correlation coefficients\n         tl.col=\"black\", tl.srt=45) # Text label color and rotation\n```\n\nFrom the above correlation table, we can see that either we have negative values which indicated that they are negatively correlated or very low values indicating not a strong correlation between them\n\n#### lets explore the data more:\n\n##### Distribution of the variables:\n\n```{r}\n# Create a histogram for 'age'\nggplot(data, aes(x=Age)) +\n  geom_histogram(bins = 30, fill =  \"blue\", color = \"black\") +\n  labs(title=\"Distribution of Age\", x=\"Age\", y=\"Count\")\n\n# Create a histogram for 'income'\nggplot(data, aes(x=Annual_Income)) +\n  geom_histogram(bins = 30, fill = 'green', color = 'black') +\n  labs(title=\"Distribution of Income\", x=\"Income\", y=\"Count\")\n\n\n#Create a histogram for spending score:\nggplot(data, aes(x=Spending_Score))+\n  geom_histogram(bins = 30, fill='darkgreen', color='black')+\n  labs(title= \"Distribution of Spending Score\", x='spending score', y= 'count')\n```\n\nFrom the above distribution, We can see that the age group near 30-40 has the highest density, most customers have income in the range of 50-80k, and most customers have a spending score of 50.\n\n[Lets see the box plot for Gender by Spending Score:]{.underline}\n\n```{r}\n# Subset the data\nmale_charges <- data[data$Gender == \"Male\", \"Spending_Score\"]\nfemale_charges <- data[data$Gender == \"Female\", \"Spending_Score\"]\n# Create a data frame for plotting\nplot_data <- data.frame(\n  Gender = rep(c(\"Male\", \"Female\"), times = c(length(male_charges), length(female_charges))),\n  Charges = c(male_charges, female_charges)\n)\n# Create the box plot\nggplot(plot_data, aes(x = Gender, y = Charges, fill = Gender)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Box Plot of Charges by Gender\", x = \"Gender\", y = \"Spending Score\", fill = \"Gender\")\n\n```\n\n```{r}\n#Average Spending score by gender:\n# Calculate the average bmi for each region\navg_bmi_by_region <- data %>%\n  group_by(Gender) %>%\n  summarise(Average_Spending_score = mean(Spending_Score))\n\nprint(avg_bmi_by_region)\n```\n\n[Lets see the relationship between Annual income and spending score:]{.underline}\n\n```{r}\n# Create a scatter plot with regression line\nggplot(data, aes(x=Annual_Income, y=Spending_Score)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, color=\"red\") +\n  labs(title=\"Relation between Annual Income and Spending Score\", x=\"Annual Income\", y=\"Spending Score\")\n\n```\n\nCalculate and print the correlation coefficient:\n\n```{r}\ncorrelation <- cor(data$Annual_Income, data$Spending_Score)\nprint(paste(\"Correlation coefficient: \", correlation))\n```\n\nFrom this, we can see that there is no correlation between Annual income vs Spending Score.\n\n# Perform DBSCAN Clustering:\n\n```{r}\n# Perfom DBSCAN:\n# Select the relevant columns\n# Copy the data\ndf <- data\n\n# Drop the 'CustomerID' column\ndf$CustomerID <- NULL\n\n# Replace 'Gender' values\ndf$Gender <- ifelse(df$Gender == \"Male\", 0, 1)\n\n\n```\n\n```{r}\n#Calculate Sihoute score:\n# Load the necessary libraries\nlibrary(dbscan)\nlibrary(cluster)\n\n\n# Define the range of parameter values\neps_values <- seq(11, 16, 1)\nmin_samples_values <- seq(5, 13, 1)\n\n# Initialize vectors to store the results\nclusters <- c()\nsil_score <- c()\n\n\n```\n\n## Lets calculate Silhouette Score:\n\nBefore dealing with the calculation of the Silhouette Score, lets get acquainted with what the Silhouette Score is and its importance to us while doing the DBSCAN clustering:\n\nThe Silhouette Score measures how similar an object is to its cluster compared to others. It ranges from -1 to 1, where a high value indicates that the object is well-matched to its cluster and poorly matched to neighboring clusters. The clustering configuration is appropriate if most objects have a high value. If many points have a low or negative value, the clustering configuration may have too many clusters.\n\nThe silhouette score provides a succinct graphical representation of how well each object lies within its cluster. It is a way to track the validity of the clusters formed by the algorithm. It can be particularly useful in the context of DBSCAN, as the algorithm does not explicitly minimize or maximize any particular objective function.\n\n```{r}\n# Loop over all combinations of parameter values\nfor (eps in eps_values) {\n  for (min_samples in min_samples_values) {\n    # Perform DBSCAN clustering\n    dbscan_result <- dbscan(df, eps = eps, minPts = min_samples)\n    \n    # Append the number of clusters to the 'clusters' vector\n    clusters <- c(clusters, max(dbscan_result$cluster))\n    \n    # Calculate the silhouette score and append it to the 'sil_score' vector\n    sil_score <- c(sil_score, cluster.stats(dist(df), dbscan_result$cluster)$avg.silwidth)\n  }\n}\n\n# Create a data frame with the results\ndbscan_df <- expand.grid(Eps = eps_values, Min_Samples = min_samples_values)\ndbscan_df$Number_of_Clusters <- clusters\ndbscan_df$Silhouette_Score <- sil_score\n\n# Print the data frame\nprint(dbscan_df)\n\n```\n\n```{r}\n\n# Convert 'Silhouette_Score' to numeric\ndbscan_df$Silhouette_Score <- as.numeric(as.character(dbscan_df$Silhouette_Score))\n\n# Find the maximum silhouette score\nmax_sil_score <- max(dbscan_df$Silhouette_Score)\n\n# Filter the data frame for the maximum silhouette score\ndbscan_df[dbscan_df$Silhouette_Score == max_sil_score, ]\n\n\n```\n\nThe stronger the clustering, the closer the result is near 1. We have reached a maximum Silhouette Score of 0.3302256 with Eps = 15 and Min Samples = 12. To get the best clustering, we will fit these values into the DBSCAN algorithm.\n\n[Perform DBSCAN clustering:]{.underline}\n\n```{r}\ndbscan_result <- dbscan(df, eps = 15, minPts = 12)\n\n# Add the cluster assignments to the data frame\ndf$DBSCAN_Clusters <- dbscan_result$cluster\n\n# Sort the data frame by the 'DBSCAN_Clusters' column\ndf <- df[order(df$DBSCAN_Clusters), ]\n\n# Print the data frame\nprint(head(df))\n\n```\n\n```{r}\n# Convert 'DBSCAN_Clusters' to character\ndf$DBSCAN_Clusters <- as.character(df$DBSCAN_Clusters)\n\n# Replace '-1' with 'Outliers'\ndf$DBSCAN_Clusters[df$DBSCAN_Clusters == \"-1\"] <- \"Outliers\"\n\n\n# Create the scatter plot\np <- ggplot(df, aes(x = Annual_Income, y = Spending_Score, color = DBSCAN_Clusters)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"black\", \"darkred\", \"#0091F7\", \"darkgreen\", \"#F7F700\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 20),\n    legend.title = element_text(size = 12),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12)\n  ) +\n  labs(\n    title = \"DBSCAN Clusters\",\n    x = \"Annual Income\",\n    y = \"Sum of Spending Scores\",\n    color = \"Clusters\"\n  )\n\n# Print the plot\nprint(p)\n```\n\n### Conclusion:\n\nAs we can see, the lack of substantial density in our data causes DBSCAN to perform poorly. The black label indicates outliers, so it will mostly appear as such. Having a more dense data structure means we can get better performance from DBSCAN clustering.\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data's density.\n\n## How DBSCAN Works\n\nDBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least **`minPts`** within a radius of **`eps`** from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset.\n\n## Advantages of DBSCAN\n\nDBSCAN has several advantages over other clustering algorithms:\n\n1.  **No need to specify the number of clusters**: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\n\n2.  **Ability to find arbitrarily shaped clusters**: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\n\n3.  **Robustness to noise**: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria.\n\n## Disadvantages of DBSCAN\n\nDespite its advantages, DBSCAN also has some limitations:\n\n1.  **Difficulty handling varying densities**: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single **`eps`** and **`minPts`** value may not be suitable for all clusters.\n\n2.  **Sensitivity to parameter settings**: The results of DBSCAN can be significantly affected by the settings of **`eps`** and **`minPts`**. Choosing appropriate values for these parameters can be challenging.\n\nIn general, density-based clustering algorithms can be quite successful for a wide range of clustering tasks, particularly when the data is shaped and has different densities. When using the algorithm with a specific dataset, it is crucial to pay close attention to the parameters and take the algorithm's constraints into account.\n\nAccording to the research report, the concept of dense regions forms the basis of DBSCAN. It is assumed that points in dense locations make up natural clusters. The term \"dense region\" has to be defined for this. These two parameters are necessary for the DBSCAN algorithm to function.\n\n-   Eps, ε: distance\n\n-   MinPts: The bare minimum of points within a given distance Eps\n\n![In this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.](https://media.geeksforgeeks.org/wp-content/uploads/20190418023034/781ff66c-b380-4a78-af25-80507ed6ff26.jpeg)\n\n## Data:\n\nIn this blog post, we will do the clustering analysis using the DBSCAN clustering method. Regarding the choice of this algorithm is explain above. Please have a look if you want to learn more. Regarding the data, [you can access this data from here](https://www.kaggle.com/code/anujachintyabiswas/mall-customer-hierarchical-kmeans-clustering/input).\n\n#### Load the necessary libraries:\n\n```{r}\nlibrary(fpc) \nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(corrplot)\nlibrary(dbscan)\n\n```\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \n```\n\n## Load the data into R:\n\n```{r}\ndata <- read.csv(\"/Users/test/Downloads/Mall_customers.csv\")\nhead(data)\n\nstr(data)\n```\n\n# Preprocessing the data:\n\n```{r}\ndata_num <- data[, sapply(data, is.numeric)]\nhead(data_num)\n\n\n```\n\n## Data Cleaning:\n\n```{r}\n# To see if the given dataset contains any null values or not\nsum(is.na(data_num))\n\n```\n\n## Explanatory analysis\n\n```{r}\n# Select the relevant columns\ndata_selected <- data_num[, c('Age', 'Annual_Income', 'Spending_Score')]\n\n# Calculate the correlation matrix\ncorrs <- cor(data_selected)\n\n# Create a heatmap with correlation values\ncorrplot(corrs, method=\"color\", type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", # Add correlation coefficients\n         tl.col=\"black\", tl.srt=45) # Text label color and rotation\n```\n\nFrom the above correlation table, we can see that either we have negative values which indicated that they are negatively correlated or very low values indicating not a strong correlation between them\n\n#### lets explore the data more:\n\n##### Distribution of the variables:\n\n```{r}\n# Create a histogram for 'age'\nggplot(data, aes(x=Age)) +\n  geom_histogram(bins = 30, fill =  \"blue\", color = \"black\") +\n  labs(title=\"Distribution of Age\", x=\"Age\", y=\"Count\")\n\n# Create a histogram for 'income'\nggplot(data, aes(x=Annual_Income)) +\n  geom_histogram(bins = 30, fill = 'green', color = 'black') +\n  labs(title=\"Distribution of Income\", x=\"Income\", y=\"Count\")\n\n\n#Create a histogram for spending score:\nggplot(data, aes(x=Spending_Score))+\n  geom_histogram(bins = 30, fill='darkgreen', color='black')+\n  labs(title= \"Distribution of Spending Score\", x='spending score', y= 'count')\n```\n\nFrom the above distribution, We can see that the age group near 30-40 has the highest density, most customers have income in the range of 50-80k, and most customers have a spending score of 50.\n\n[Lets see the box plot for Gender by Spending Score:]{.underline}\n\n```{r}\n# Subset the data\nmale_charges <- data[data$Gender == \"Male\", \"Spending_Score\"]\nfemale_charges <- data[data$Gender == \"Female\", \"Spending_Score\"]\n# Create a data frame for plotting\nplot_data <- data.frame(\n  Gender = rep(c(\"Male\", \"Female\"), times = c(length(male_charges), length(female_charges))),\n  Charges = c(male_charges, female_charges)\n)\n# Create the box plot\nggplot(plot_data, aes(x = Gender, y = Charges, fill = Gender)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Box Plot of Charges by Gender\", x = \"Gender\", y = \"Spending Score\", fill = \"Gender\")\n\n```\n\n```{r}\n#Average Spending score by gender:\n# Calculate the average bmi for each region\navg_bmi_by_region <- data %>%\n  group_by(Gender) %>%\n  summarise(Average_Spending_score = mean(Spending_Score))\n\nprint(avg_bmi_by_region)\n```\n\n[Lets see the relationship between Annual income and spending score:]{.underline}\n\n```{r}\n# Create a scatter plot with regression line\nggplot(data, aes(x=Annual_Income, y=Spending_Score)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, color=\"red\") +\n  labs(title=\"Relation between Annual Income and Spending Score\", x=\"Annual Income\", y=\"Spending Score\")\n\n```\n\nCalculate and print the correlation coefficient:\n\n```{r}\ncorrelation <- cor(data$Annual_Income, data$Spending_Score)\nprint(paste(\"Correlation coefficient: \", correlation))\n```\n\nFrom this, we can see that there is no correlation between Annual income vs Spending Score.\n\n# Perform DBSCAN Clustering:\n\n```{r}\n# Perfom DBSCAN:\n# Select the relevant columns\n# Copy the data\ndf <- data\n\n# Drop the 'CustomerID' column\ndf$CustomerID <- NULL\n\n# Replace 'Gender' values\ndf$Gender <- ifelse(df$Gender == \"Male\", 0, 1)\n\n\n```\n\n```{r}\n#Calculate Sihoute score:\n# Load the necessary libraries\nlibrary(dbscan)\nlibrary(cluster)\n\n\n# Define the range of parameter values\neps_values <- seq(11, 16, 1)\nmin_samples_values <- seq(5, 13, 1)\n\n# Initialize vectors to store the results\nclusters <- c()\nsil_score <- c()\n\n\n```\n\n## Lets calculate Silhouette Score:\n\nBefore dealing with the calculation of the Silhouette Score, lets get acquainted with what the Silhouette Score is and its importance to us while doing the DBSCAN clustering:\n\nThe Silhouette Score measures how similar an object is to its cluster compared to others. It ranges from -1 to 1, where a high value indicates that the object is well-matched to its cluster and poorly matched to neighboring clusters. The clustering configuration is appropriate if most objects have a high value. If many points have a low or negative value, the clustering configuration may have too many clusters.\n\nThe silhouette score provides a succinct graphical representation of how well each object lies within its cluster. It is a way to track the validity of the clusters formed by the algorithm. It can be particularly useful in the context of DBSCAN, as the algorithm does not explicitly minimize or maximize any particular objective function.\n\n```{r}\n# Loop over all combinations of parameter values\nfor (eps in eps_values) {\n  for (min_samples in min_samples_values) {\n    # Perform DBSCAN clustering\n    dbscan_result <- dbscan(df, eps = eps, minPts = min_samples)\n    \n    # Append the number of clusters to the 'clusters' vector\n    clusters <- c(clusters, max(dbscan_result$cluster))\n    \n    # Calculate the silhouette score and append it to the 'sil_score' vector\n    sil_score <- c(sil_score, cluster.stats(dist(df), dbscan_result$cluster)$avg.silwidth)\n  }\n}\n\n# Create a data frame with the results\ndbscan_df <- expand.grid(Eps = eps_values, Min_Samples = min_samples_values)\ndbscan_df$Number_of_Clusters <- clusters\ndbscan_df$Silhouette_Score <- sil_score\n\n# Print the data frame\nprint(dbscan_df)\n\n```\n\n```{r}\n\n# Convert 'Silhouette_Score' to numeric\ndbscan_df$Silhouette_Score <- as.numeric(as.character(dbscan_df$Silhouette_Score))\n\n# Find the maximum silhouette score\nmax_sil_score <- max(dbscan_df$Silhouette_Score)\n\n# Filter the data frame for the maximum silhouette score\ndbscan_df[dbscan_df$Silhouette_Score == max_sil_score, ]\n\n\n```\n\nThe stronger the clustering, the closer the result is near 1. We have reached a maximum Silhouette Score of 0.3302256 with Eps = 15 and Min Samples = 12. To get the best clustering, we will fit these values into the DBSCAN algorithm.\n\n[Perform DBSCAN clustering:]{.underline}\n\n```{r}\ndbscan_result <- dbscan(df, eps = 15, minPts = 12)\n\n# Add the cluster assignments to the data frame\ndf$DBSCAN_Clusters <- dbscan_result$cluster\n\n# Sort the data frame by the 'DBSCAN_Clusters' column\ndf <- df[order(df$DBSCAN_Clusters), ]\n\n# Print the data frame\nprint(head(df))\n\n```\n\n```{r}\n# Convert 'DBSCAN_Clusters' to character\ndf$DBSCAN_Clusters <- as.character(df$DBSCAN_Clusters)\n\n# Replace '-1' with 'Outliers'\ndf$DBSCAN_Clusters[df$DBSCAN_Clusters == \"-1\"] <- \"Outliers\"\n\n\n# Create the scatter plot\np <- ggplot(df, aes(x = Annual_Income, y = Spending_Score, color = DBSCAN_Clusters)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"black\", \"darkred\", \"#0091F7\", \"darkgreen\", \"#F7F700\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 20),\n    legend.title = element_text(size = 12),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12)\n  ) +\n  labs(\n    title = \"DBSCAN Clusters\",\n    x = \"Annual Income\",\n    y = \"Sum of Spending Scores\",\n    color = \"Clusters\"\n  )\n\n# Print the plot\nprint(p)\n```\n\n### Conclusion:\n\nAs we can see, the lack of substantial density in our data causes DBSCAN to perform poorly. The black label indicates outliers, so it will mostly appear as such. Having a more dense data structure means we can get better performance from DBSCAN clustering.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","page-layout":"full","theme":{"light":["flatly","light.scss"],"dark":["darkly","dark.scss"]},"grid":{"sidebar-width":"220px","body-width":"1600px","margin-width":"250px"},"title-block-banner":true,"title":"Understanding DBSCAN Clustering Analysis in Machine Learning","author":"Kamal Chhetri","date":"2023-11-24","categories":["R","Code","Analysis"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}