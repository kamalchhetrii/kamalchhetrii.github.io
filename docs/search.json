[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Kamal Chhetri, a PhD student in Plant Pathology at Virginia Tech University. My journey in the field of science began with a Master’s degree in Biotechnology from West Virginia State University.\nOriginally from Nepal, I’ve always been fascinated by the intricate workings of plant life and the ecosystem. This fascination led me to pursue a career in Plant Pathology, where I’m currently delving into the complex interactions between plants and their environment.\nThrough this blog, I aim to share machine learning and its application to the agriculture field. Enjoy my blog\n\n\n Back to top"
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html",
    "href": "posts/Classification_crop  suitability/crop.html",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#introduction",
    "href": "posts/Classification_crop  suitability/crop.html#introduction",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#random-forest-rf",
    "href": "posts/Classification_crop  suitability/crop.html#random-forest-rf",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Random Forest (RF)",
    "text": "Random Forest (RF)\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#support-vector-machines-svm",
    "href": "posts/Classification_crop  suitability/crop.html#support-vector-machines-svm",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It’s effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#naive-bayes-nb",
    "href": "posts/Classification_crop  suitability/crop.html#naive-bayes-nb",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Naive Bayes (NB)",
    "text": "Naive Bayes (NB)\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area’s average temperature, humidity, pH level, rainfall, and NPK values."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#the-data",
    "href": "posts/Classification_crop  suitability/crop.html#the-data",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "The Data",
    "text": "The Data\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type.\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R’s head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R’s is.na() function."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#data-visualization",
    "href": "posts/Classification_crop  suitability/crop.html#data-visualization",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo get a better understanding of the data, I visualized it using R’s ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#data-processing",
    "href": "posts/Classification_crop  suitability/crop.html#data-processing",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Processing",
    "text": "Data Processing\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#model-selection",
    "href": "posts/Classification_crop  suitability/crop.html#model-selection",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Model Selection",
    "text": "Model Selection\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\nLoad necessary libraries:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pheatmap)\n\nLoad the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/Crop_recommendation.csv\")\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\nset.seed(123)\n\nSplit the data into training and testing sets:\n\n# See the head and tail of data\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n\nCreate a data frame to store test data and predicted labels:\n\n# See the distribution of data \nsummary(data)\n\n       N                P                K           temperature    \n Min.   :  0.00   Min.   :  5.00   Min.   :  5.00   Min.   : 8.826  \n 1st Qu.: 21.00   1st Qu.: 28.00   1st Qu.: 20.00   1st Qu.:22.769  \n Median : 37.00   Median : 51.00   Median : 32.00   Median :25.599  \n Mean   : 50.55   Mean   : 53.36   Mean   : 48.15   Mean   :25.616  \n 3rd Qu.: 84.25   3rd Qu.: 68.00   3rd Qu.: 49.00   3rd Qu.:28.562  \n Max.   :140.00   Max.   :145.00   Max.   :205.00   Max.   :43.675  \n    humidity           ph           rainfall         label          \n Min.   :14.26   Min.   :3.505   Min.   : 20.21   Length:2200       \n 1st Qu.:60.26   1st Qu.:5.972   1st Qu.: 64.55   Class :character  \n Median :80.47   Median :6.425   Median : 94.87   Mode  :character  \n Mean   :71.48   Mean   :6.469   Mean   :103.46                     \n 3rd Qu.:89.95   3rd Qu.:6.924   3rd Qu.:124.27                     \n Max.   :99.98   Max.   :9.935   Max.   :298.56                     \n\n\nSee the relationship between different parameters such as Ph vs label or precipitation vs crop:\n\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n\n\n\n\n\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n\n\n\n\nData processing: separate feature and target variable:\n\nfeatures &lt;- data[,1:7]\ntarget &lt;- data$label\n\nSplit the data into training and testing sets:\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(target, p=0.8, list=FALSE)\ntrainData &lt;- features[trainIndex, ]\ntrainLabels &lt;- target[trainIndex]\ntestData &lt;- features[-trainIndex, ]\ntestLabels &lt;- target[-trainIndex]\n\nDefine training control:\n\ntrain_control &lt;- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm &lt;- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb &lt;- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels &lt;- as.factor(trainLabels)\ntestLabels &lt;- as.factor(testLabels)\n\nMake predictions on the test data:\nUsing Random Forest:\n\npred_rf &lt;- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf &lt;- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric &lt;- as.matrix(cm_rf)\ncm_rf_numeric[] &lt;- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n\n\n\n\nFor SVM\n\npred_svm &lt;- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm &lt;- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\nFor NB\n\npred_nb &lt;- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb &lt;- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric &lt;- as.matrix(cm_nb)\ncm_nb_numeric[] &lt;- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n\n\nCalculate and print misclassification rates:\n\nmis_rf &lt;- 1 - cm_rf$overall['Accuracy']\nmis_svm &lt;- 1 - cm_svm$overall['Accuracy']\nmis_nb &lt;- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification &lt;- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n          Model Misclassification_Rate\n1 Random Forest            0.004545455\n2           SVM            0.018181818\n3   Naive Bayes            0.004545455\n\n\n\n# Summarize the results\nresults &lt;- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: RF, SVM, NB \nNumber of resamples: 10 \n\nAccuracy \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9886364 0.9943182 0.9943182 0.9954545 0.9985795    1    0\nSVM 0.9659091 0.9730114 0.9829545 0.9829545 0.9928977    1    0\nNB  0.9772727 0.9943182 0.9943182 0.9943182 1.0000000    1    0\n\nKappa \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9880952 0.9940476 0.9940476 0.9952381 0.9985119    1    0\nSVM 0.9642857 0.9717262 0.9821429 0.9821429 0.9925595    1    0\nNB  0.9761905 0.9940476 0.9940476 0.9940476 1.0000000    1    0\n\n\n\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n\n\n\n\nMaking Predictions:\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n# Train the model\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData &lt;- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction &lt;- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\n\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\nConclusion\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#introduction-1",
    "href": "posts/anomaly detection/anomaly.html#introduction-1",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection, also known as outlier detection, is a fascinating aspect of machine learning. It involves identifying data points, events, or observations that deviate significantly from the norm. These anomalies can often provide critical and actionable insights in various domains, such as fraud detection in banking, intrusion detection in network security, and fault detection in critical systems."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#what-is-anomaly-detection",
    "href": "posts/anomaly detection/anomaly.html#what-is-anomaly-detection",
    "title": "Anomaly Detection in Machine Learning",
    "section": "What is Anomaly Detection?",
    "text": "What is Anomaly Detection?\nAnomaly detection is the process of identifying unexpected items or events in datasets, which differ from the norm. In other words, it’s about finding the ‘outliers’ in your data. For example, in a manufacturing context, an anomalous event could be a sudden increase in defective products."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#types-of-anomalies",
    "href": "posts/anomaly detection/anomaly.html#types-of-anomalies",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Types of Anomalies",
    "text": "Types of Anomalies\nThere are three main types of anomalies:\n\nPoint Anomalies: A single instance of data is anomalous if it’s too far off from the rest. For example, spending $100 on food every day during the holiday season is normal, but may be odd otherwise.\nContextual Anomalies: The abnormality is context-specific. This type of anomaly is common in time-series data. For example, spending $100 on food during the holiday season is normal, but may be odd otherwise.\nCollective Anomalies: A set of data instances collectively helps in detecting anomalies. For example, someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#anomaly-detection-techniques",
    "href": "posts/anomaly detection/anomaly.html#anomaly-detection-techniques",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Anomaly Detection Techniques",
    "text": "Anomaly Detection Techniques\nThere are several techniques used for anomaly detection, each with its strengths and weaknesses. Some of the most popular methods include:\n\nStatistical Methods: These methods model the normal data behavior using statistical parameters like mean, median, mode, variance, etc. Any data instance that doesn’t fit this model is considered an anomaly.\nMachine Learning-Based Methods: These include techniques like clustering, classification, and nearest neighbors. These methods can either be supervised (labels are available) or unsupervised (no labels).\nTime Series Analysis: This is particularly useful for sequential data, where some pattern or trend is expected. Techniques used here include state space models, decomposition methods, etc."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#about-the-data",
    "href": "posts/anomaly detection/anomaly.html#about-the-data",
    "title": "Anomaly Detection in Machine Learning",
    "section": "About the data:",
    "text": "About the data:\nThe data collection includes 284,807 credit card purchases made by European cardholders over a 48-hour period. The legitimate purchases significantly distort this data set, with only 492 of these sales being fraudulent. The purchase metrics, which comprise 28 variables namely V1 through V28, have been anonymized and modified to remove any personal information due to the sensitive nature of the data.\nLoad necessary libraries:\n\nlibrary(fpc)\nlibrary(ggplot2)\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:fpc':\n\n    dbscan\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\n\n\ndata &lt;- read.csv(\"/Users/test/Downloads/creditcard.csv\")\n\n\nhead(data)\n\n  Time         V1          V2        V3         V4          V5          V6\n1    0 -1.3598071 -0.07278117 2.5363467  1.3781552 -0.33832077  0.46238778\n2    0  1.1918571  0.26615071 0.1664801  0.4481541  0.06001765 -0.08236081\n3    1 -1.3583541 -1.34016307 1.7732093  0.3797796 -0.50319813  1.80049938\n4    1 -0.9662717 -0.18522601 1.7929933 -0.8632913 -0.01030888  1.24720317\n5    2 -1.1582331  0.87773675 1.5487178  0.4030339 -0.40719338  0.09592146\n6    2 -0.4259659  0.96052304 1.1411093 -0.1682521  0.42098688 -0.02972755\n           V7          V8         V9         V10        V11         V12\n1  0.23959855  0.09869790  0.3637870  0.09079417 -0.5515995 -0.61780086\n2 -0.07880298  0.08510165 -0.2554251 -0.16697441  1.6127267  1.06523531\n3  0.79146096  0.24767579 -1.5146543  0.20764287  0.6245015  0.06608369\n4  0.23760894  0.37743587 -1.3870241 -0.05495192 -0.2264873  0.17822823\n5  0.59294075 -0.27053268  0.8177393  0.75307443 -0.8228429  0.53819555\n6  0.47620095  0.26031433 -0.5686714 -0.37140720  1.3412620  0.35989384\n         V13        V14        V15        V16         V17         V18\n1 -0.9913898 -0.3111694  1.4681770 -0.4704005  0.20797124  0.02579058\n2  0.4890950 -0.1437723  0.6355581  0.4639170 -0.11480466 -0.18336127\n3  0.7172927 -0.1659459  2.3458649 -2.8900832  1.10996938 -0.12135931\n4  0.5077569 -0.2879237 -0.6314181 -1.0596472 -0.68409279  1.96577500\n5  1.3458516 -1.1196698  0.1751211 -0.4514492 -0.23703324 -0.03819479\n6 -0.3580907 -0.1371337  0.5176168  0.4017259 -0.05813282  0.06865315\n          V19         V20          V21          V22         V23         V24\n1  0.40399296  0.25141210 -0.018306778  0.277837576 -0.11047391  0.06692807\n2 -0.14578304 -0.06908314 -0.225775248 -0.638671953  0.10128802 -0.33984648\n3 -2.26185710  0.52497973  0.247998153  0.771679402  0.90941226 -0.68928096\n4 -1.23262197 -0.20803778 -0.108300452  0.005273597 -0.19032052 -1.17557533\n5  0.80348692  0.40854236 -0.009430697  0.798278495 -0.13745808  0.14126698\n6 -0.03319379  0.08496767 -0.208253515 -0.559824796 -0.02639767 -0.37142658\n         V25        V26          V27         V28 Amount Class\n1  0.1285394 -0.1891148  0.133558377 -0.02105305 149.62     0\n2  0.1671704  0.1258945 -0.008983099  0.01472417   2.69     0\n3 -0.3276418 -0.1390966 -0.055352794 -0.05975184 378.66     0\n4  0.6473760 -0.2219288  0.062722849  0.06145763 123.50     0\n5 -0.2060096  0.5022922  0.219422230  0.21515315  69.99     0\n6 -0.2327938  0.1059148  0.253844225  0.08108026   3.67     0"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#data-cleaning",
    "href": "posts/anomaly detection/anomaly.html#data-cleaning",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Data cleaning:",
    "text": "Data cleaning:\nRemove the unnecessary column that is not required during the analysis. We can remove the unnecessary column by following:\n\ndata$Time &lt;- NULL"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#convert-categorical-variables-to-factors",
    "href": "posts/anomaly detection/anomaly.html#convert-categorical-variables-to-factors",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Convert categorical variables to factors:",
    "text": "Convert categorical variables to factors:\n\ndata$Class &lt;- as.factor(data$Class)"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#convert-categorical-variable-to-numeric",
    "href": "posts/anomaly detection/anomaly.html#convert-categorical-variable-to-numeric",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Convert categorical variable to numeric",
    "text": "Convert categorical variable to numeric\n\ndata$Class &lt;- as.numeric(as.character(data$Class))"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#check-if-the-data-contains-null-values",
    "href": "posts/anomaly detection/anomaly.html#check-if-the-data-contains-null-values",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Check if the data contains null values:",
    "text": "Check if the data contains null values:\n\nsum(is.na(data))\n\n[1] 0\n\n\nThis means this data doesnot contains any null values. We are good to go with this data and don’t have to remove any null data if there are any."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#data-visualization",
    "href": "posts/anomaly detection/anomaly.html#data-visualization",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Data visualization:",
    "text": "Data visualization:\n\n# Select a subset of variables to plot\nselected_vars &lt;- data[, c(\"V1\", \"V2\", \"V3\", \"V4\", \"V5\")]\n\n# Create a color palette\ncolors &lt;- terrain.colors(length(selected_vars))\n\n# Create the pairs plot with colors\npairs(selected_vars, col=colors)"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#dbscan-clustering-for-anomaly-detection",
    "href": "posts/anomaly detection/anomaly.html#dbscan-clustering-for-anomaly-detection",
    "title": "Anomaly Detection in Machine Learning",
    "section": "DBSCAN clustering for anomaly detection:",
    "text": "DBSCAN clustering for anomaly detection:\n\ndbscan_result &lt;- dbscan(data, eps = 0.3, minPts = 5)"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#visualization-of-dbscan-result",
    "href": "posts/anomaly detection/anomaly.html#visualization-of-dbscan-result",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Visualization of DBSCAN result:",
    "text": "Visualization of DBSCAN result:"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Probability/probability.html",
    "href": "posts/Probability/probability.html",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "",
    "text": "Probability theory is a fundamental pillar of Machine Learning (ML). It provides a framework to represent uncertain statements and formulate the learning problem in a principled way. In this blog post, we will delve into the basics of probability theory and random variables, and how they play a crucial role in ML. We will also illustrate these concepts with an example in R."
  },
  {
    "objectID": "posts/Probability/probability.html#probability-theory",
    "href": "posts/Probability/probability.html#probability-theory",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability theory is a branch of mathematics that deals with uncertainty. It provides a mathematical framework for quantifying our belief or uncertainty. The probability of an event is a measure of the likelihood that the event will occur.\nIn ML, we often have to deal with uncertainty. For example, when we build a spam classifier, we are uncertain about whether a new incoming email is spam or not. Probability theory provides a solid foundation to reason about this uncertainty."
  },
  {
    "objectID": "posts/Probability/probability.html#random-variables",
    "href": "posts/Probability/probability.html#random-variables",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a variable whose possible values are outcomes of a random phenomenon. There are two types of random variables: discrete and continuous. A discrete random variable has a countable number of possible values. A continuous random variable has an uncountable number of possible values, often represented by an interval on the real number line.\nIn the context of ML, random variables could represent various things. For example, in a spam classifier, a random variable could represent whether an email is spam (1) or not (0).\nTo see comparison between discrete and continuous, have a look at the table given below\n\nFigure credit: Discrete vs continuous"
  },
  {
    "objectID": "posts/Probability/probability.html#probability-distributions",
    "href": "posts/Probability/probability.html#probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nA probability distribution describes how a random variable is distributed. It tells us what the probabilities of each outcome are. For discrete random variables, we use a probability mass function (PMF). For continuous random variables, we use a probability density function (PDF).\nWith the facts at hand, one can utilize the decision tree depicted in the picture below to get an understanding of some common probability distributions:\n\nFigure credit: Probabilistic approaches to risk by Aswath Damodaran."
  },
  {
    "objectID": "posts/Probability/probability.html#discrete-probability-distributions",
    "href": "posts/Probability/probability.html#discrete-probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Discrete Probability Distributions:",
    "text": "Discrete Probability Distributions:\nA discrete probability distribution applies to scenarios where the set of possible outcomes is discrete. Common examples of discrete probability distributions include the Bernoulli, Binomial, and Poisson distributions.\nExample: Binomial distribution\nLet's create a hypothetical scenario where we toss a coin 10 times. We are interested in the number of times we get heads. This scenario follows a binomial distribution. Lets do the data analysis for better understanding of it.\nLoad necessary libraries:\n\n# Load necessary library\nlibrary(ggplot2)\n\nSet the random seed for reproducibility:\n\n# Set parameters\nsize &lt;- 10\nprob &lt;- 0.5\n\n# Generate binomial data\nset.seed(123)\ndata &lt;- rbinom(1000, size, prob)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\") +\n  labs(x = \"Number of Heads\", y = \"Frequency\", title = \"Binomial Distribution\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/Probability/probability.html#continuous-probability-distributions",
    "href": "posts/Probability/probability.html#continuous-probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Continuous Probability Distributions:",
    "text": "Continuous Probability Distributions:\nA continuous probability distribution applies to scenarios where the set of possible outcomes is an interval of real numbers. Common examples of continuous probability distributions include the Normal, Exponential, and Beta distributions.\nExample: Normal distiribution\nLet's create a hypothetical scenario where we measure the heights of a large group of individuals. The heights of individuals in a large population often follow a normal distribution.\n\n# Set parameters\nmean &lt;- 170\nsd &lt;- 10\n\n# Generate normal data\nset.seed(123)\ndata &lt;- rnorm(1000, mean, sd)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\", aes(y = ..density..)) +\n  geom_density(alpha = 0.2, fill = \"#FF6666\") +\n  labs(x = \"Height (cm)\", y = \"Density\", title = \"Normal Distribution\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead."
  },
  {
    "objectID": "posts/Probability/probability.html#conclusion",
    "href": "posts/Probability/probability.html#conclusion",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Conclusion:",
    "text": "Conclusion:\nProbability theory and random variables are essential tools. Understanding discrete and continuous probability distributions is crucial in machine learning. They provide a foundation for many machine learning algorithms and statistical tests. They provide a framework to handle uncertainty and formulate learning problems. Understanding these concepts can help you build more effective and robust machine learning models."
  },
  {
    "objectID": "posts/Clustering/clustering.html",
    "href": "posts/Clustering/clustering.html",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data’s density.\n\n\n\nDBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least minPts within a radius of eps from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset.\n\n\n\nDBSCAN has several advantages over other clustering algorithms:\n\nNo need to specify the number of clusters: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\nAbility to find arbitrarily shaped clusters: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\nRobustness to noise: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria.\n\n\n\n\nDespite its advantages, DBSCAN also has some limitations:\n\nDifficulty handling varying densities: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single eps and minPts value may not be suitable for all clusters.\nSensitivity to parameter settings: The results of DBSCAN can be significantly affected by the settings of eps and minPts. Choosing appropriate values for these parameters can be challenging.\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\nLoad necessary libraries\n\nlibrary(fpc) \n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nThe dataset contains information about individuals such as their age, sex, BMI, number of children, smoking status, region, and charges.\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ..."
  },
  {
    "objectID": "posts/Clustering/clustering.html#introduction",
    "href": "posts/Clustering/clustering.html#introduction",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data’s density."
  },
  {
    "objectID": "posts/Clustering/clustering.html#how-dbscan-works",
    "href": "posts/Clustering/clustering.html#how-dbscan-works",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "DBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least minPts within a radius of eps from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset."
  },
  {
    "objectID": "posts/Clustering/clustering.html#advantages-of-dbscan",
    "href": "posts/Clustering/clustering.html#advantages-of-dbscan",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "DBSCAN has several advantages over other clustering algorithms:\n\nNo need to specify the number of clusters: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\nAbility to find arbitrarily shaped clusters: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\nRobustness to noise: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria."
  },
  {
    "objectID": "posts/Clustering/clustering.html#disadvantages-of-dbscan",
    "href": "posts/Clustering/clustering.html#disadvantages-of-dbscan",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Despite its advantages, DBSCAN also has some limitations:\n\nDifficulty handling varying densities: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single eps and minPts value may not be suitable for all clusters.\nSensitivity to parameter settings: The results of DBSCAN can be significantly affected by the settings of eps and minPts. Choosing appropriate values for these parameters can be challenging.\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\nLoad necessary libraries\n\nlibrary(fpc) \n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nThe dataset contains information about individuals such as their age, sex, BMI, number of children, smoking status, region, and charges.\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ..."
  },
  {
    "objectID": "posts/Clustering/clustering.html#remove-label-form-dataset",
    "href": "posts/Clustering/clustering.html#remove-label-form-dataset",
    "title": "DNSCAN Clustering",
    "section": "Remove label form dataset",
    "text": "Remove label form dataset\n\nmed &lt;- data_num[-4] \n\nFitting DBSCAN clustering model:\n\nset.seed(0)  # Setting seed \nDbscan_cl &lt;- dbscan(med, eps = 0.45, MinPts = 5) \nDbscan_cl\n\ndbscan Pts=1338 MinPts=5 eps=0.45\n          0 1 2 3 4 5\nborder 1310 5 2 4 3 4\nseed      0 1 5 1 1 2\ntotal  1310 6 7 5 4 6\n\n\nIn this code, eps is the maximum distance between two samples for them to be considered as in the same neighborhood, and MinPts is the number of samples in a neighborhood for a point to be considered as a core point."
  },
  {
    "objectID": "posts/linear regression/lr.html",
    "href": "posts/linear regression/lr.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nLinear regression is a powerful statistical method that allows us to examine the relationship between two (simple linear regression) or more (multiple linear regression) variables. A key feature of linear regression is its simplicity and interpretability. It’s used in various fields, including machine learning, most medical fields, and social sciences.\nLinear regression models the relationship between two variables by fitting a linear equation to observed data. The steps to perform multiple linear regression are almost identical to those of simple linear regression.\n\nTypes of regression:\na. Simple linear regression: In this case, we use only one input variable\nb. Multiple linear regression: In this case, We use multiple input variable\n\n\nThe Dataset\nI will update about it soon.\n\n\nAssumptions:\nThere are several assumption for the linear regression model that I am using in this blog. They are explained below, lets take a look.\nLinearity: The relationship between the independent and dependent variables is linear. This assumption can be checked by plotting the variables and examining the data scatter.\nIndependence: The residuals (the differences between the observed and predicted values) are independent. In other words, there is not a specific pattern in the residuals.\nHomoscedasticity: The variance of the residuals is constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all predicted values.\nNormality: The residuals are normally distributed. If this assumption is violated, then the confidence intervals may not be accurate.\nAbsence of multicollinearity: In the case of multiple linear regression, the independent variables should not be too highly correlated with each other.\nLoad necessary libraries:\nSet the random seed for reproducibility:\n\n# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(reshape2)\n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nLoad and clean the data:\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n# Handle missing values (if any)\n# data &lt;- na.omit(data)  # drops rows with missing values\n# data &lt;- data[complete.cases(data), ]  # drops rows with missing values\n# data$column &lt;- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n\nSince, we don’t have any null or missing value in this data as indicated above. So, no further handling of missing data is needed.\nVisualize the data: for better understanding of the data before proceeding further\n\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets take a look for the correlation between these two features: Our data contains one column “Species” which is non-numeric. Thus, we will firstly subset the data that doesnot contains “Species” column.\n\n# Subset the data to include only numeric columns\ndata_numeric &lt;- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n\n\nCreate the heatmap for better visualization:\n\n# Melt the correlation matrix into a long format\ndata_melt &lt;- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n\n\n\n\nFrom the heatmap created using the above code, we can see there is no strong correlation between different variables. However, this part is made to visualize the data only.\nBuilding the model: From the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\n\n# Create a violin plot for 'age'\nggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellow\") +\n  labs(title=\"Distribution of Charges by Sex\", x=\"Sex\", y=\"Charges\")\n\n\n\n# Create a violin plot for 'charges' vs 'sex'\np1 &lt;- ggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellowgreen\") +\n  labs(title=\"Violin plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a violin plot for 'charges' vs 'smoker'\np2 &lt;- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_violin(fill=\"goldenrod1\") +\n  labs(title=\"Violin plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n\n\nprint(p2)\n\n\n\n\nAccording to the left plot, the average insurance cost for men and women is roughly the same at $5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about $5,000. The minimum insurance premium for smokers is $5,000.\n\nNeed to check again on these\n\n# Split the data into training and test sets\nset.seed(0)\nsampleSplit &lt;- sample.split(Y=data$bmi, SplitRatio=0.7)\ntrainSet &lt;- subset(x=data, sampleSplit==TRUE)\ntestSet &lt;- subset(x=data, sampleSplit==FALSE)\n\n\n# Fit the model\nmodel &lt;- lm(formula=bmi ~ ., data=trainSet)\n\nBy using a linear combination of all other attributes, we hope to predict the bmi attribute. In this case, we don’t have to worry about the categorical attributes. R automatically manages the categorical attributes.\nLets see the summary of model:\n\n# Split the data into training and test sets\nsummary(model)\n\n\nCall:\nlm(formula = bmi ~ ., data = trainSet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.9234  -3.7146   0.0124   3.4208  20.9629 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.766e+01  6.883e-01  40.181  &lt; 2e-16 ***\nage             -2.573e-02  1.521e-02  -1.692   0.0910 .  \nsexmale          7.230e-01  3.679e-01   1.965   0.0497 *  \nchildren        -2.269e-02  1.504e-01  -0.151   0.8801    \nsmokeryes       -6.980e+00  7.894e-01  -8.841  &lt; 2e-16 ***\nregionnorthwest -2.051e-01  5.294e-01  -0.387   0.6986    \nregionsoutheast  3.923e+00  5.169e-01   7.589 7.58e-14 ***\nregionsouthwest  1.379e+00  5.307e-01   2.599   0.0095 ** \ncharges          2.803e-04  2.771e-05  10.119  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.73 on 967 degrees of freedom\nMultiple R-squared:  0.181, Adjusted R-squared:  0.1742 \nF-statistic: 26.71 on 8 and 967 DF,  p-value: &lt; 2.2e-16\n\n\nThe P-values that are shown in the Pr(&gt;|t|) column are the most intriguing aspect of this situation. The probability significance column Pr(&gt;|t|) in the above table is a crucial factor to take into account because it shows how important probability is for prediction. Here, we took a significance level of 5%  for the calculation.\n\nmodelResiduals &lt;- as.data.frame(residuals(model))\nggplot(modelResiduals, aes(residuals(model)))+\n  geom_histogram(fill = 'mediumorchid1', color = 'black')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe distribution of the residual data is approximately normally distributed.\nMake the prediction:\n\npreds &lt;- predict(model, testSet)\n\nNow, see the further details by creating a table for for actual and prediction values from the above created model.\n\nmodelEval &lt;- cbind(testSet$bmi, preds)\ncolnames(modelEval) &lt;- c('Acual', 'Predicted')\nmodelEval &lt;- as.data.frame(modelEval)\nhead(modelEval)\n\n    Acual Predicted\n4  22.705  33.49025\n14 39.820  33.24960\n16 24.600  29.76384\n23 34.100  32.15970\n24 31.920  30.35069\n27 23.085  30.08882\n\n\n\n\n\nNow lets evaluate the prediction:\nModel evaluation: After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse).\nLets learn little bit about rmse: The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\nDuring model evaluation, RMSE serves as a measure to understand the model’s performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\n\n# Calculate error metrics\nmse &lt;- mean((modelEval$Acual - modelEval$Predicted)^2)\nrmse &lt;- sqrt(mse)\nprint(rmse)\n\n[1] 4.962662\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Machine Learning blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nKamal\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Crop Suitability with Machine Learning using R\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables in Machine Learning\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection in Machine Learning\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nDNSCAN Clustering\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]