[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "I am updating the file, stay tuned !!!\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Research Portfolio",
    "section": "",
    "text": "About Me\nüë®‚Äçüî¨ Kamal Chhetri\nWelcome to my GitHub site !\nI am originally from Nepal and I have completed my MS in Biotechnology (Plant breeding). My research interests lie in plant breeding, host pathogen interactions, genomics, and the integration of machine learning in crop improvement. I am conducting¬†research on fungicide¬†evaluations, early pathogen detection, metagenomics, and decision support system tools. My work focuses on using bioinformatics¬†and statistical analysis¬†to develop decision-support tools for effective agricultural disease management.\nüîç Explore: - üìÇ My Research - ‚úçÔ∏è My Blog - üìß Contact Me\n\n\n\n\n Back to top"
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications",
    "section": "",
    "text": "Pun, L. B., Chhetri, K., Pandey, A., & Poudel, R. (2020). In vitro evaluation of botanical extracts, chemical fungicides and Trichoderma harzianum against Alternaria brassicicola causing leafspot of cabbage. Nepalese Horticulture, 14(1), 68-76.\nChhetri, K., Bishop, C., Langston, D., & Zeng Y., (2025).Evaluation of effects of foliar fungicide treatments and hybrid susceptibility on major corn foliar diseases in Virginia. (In the process)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/DBSCAN/index.html",
    "href": "posts/DBSCAN/index.html",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data‚Äôs density."
  },
  {
    "objectID": "posts/DBSCAN/index.html#introduction",
    "href": "posts/DBSCAN/index.html#introduction",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data‚Äôs density."
  },
  {
    "objectID": "posts/DBSCAN/index.html#how-dbscan-works",
    "href": "posts/DBSCAN/index.html#how-dbscan-works",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "How DBSCAN Works",
    "text": "How DBSCAN Works\nDBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least minPts within a radius of eps from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset."
  },
  {
    "objectID": "posts/DBSCAN/index.html#advantages-of-dbscan",
    "href": "posts/DBSCAN/index.html#advantages-of-dbscan",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Advantages of DBSCAN",
    "text": "Advantages of DBSCAN\nDBSCAN has several advantages over other clustering algorithms:\n\nNo need to specify the number of clusters: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\nAbility to find arbitrarily shaped clusters: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\nRobustness to noise: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria."
  },
  {
    "objectID": "posts/DBSCAN/index.html#disadvantages-of-dbscan",
    "href": "posts/DBSCAN/index.html#disadvantages-of-dbscan",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Disadvantages of DBSCAN",
    "text": "Disadvantages of DBSCAN\nDespite its advantages, DBSCAN also has some limitations:\n\nDifficulty handling varying densities: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single eps and minPts value may not be suitable for all clusters.\nSensitivity to parameter settings: The results of DBSCAN can be significantly affected by the settings of eps and minPts. Choosing appropriate values for these parameters can be challenging.\n\nIn general, density-based clustering algorithms can be quite successful for a wide range of clustering tasks, particularly when the data is shaped and has different densities. When using the algorithm with a specific dataset, it is crucial to pay close attention to the parameters and take the algorithm‚Äôs constraints into account.\nAccording to the research report, the concept of dense regions forms the basis of DBSCAN. It is assumed that points in dense locations make up natural clusters. The term ‚Äúdense region‚Äù has to be defined for this. These two parameters are necessary for the DBSCAN algorithm to function.\n\nEps, Œµ: distance\nMinPts: The bare minimum of points within a given distance Eps\n\n\n\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R."
  },
  {
    "objectID": "posts/DBSCAN/index.html#data",
    "href": "posts/DBSCAN/index.html#data",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Data:",
    "text": "Data:\nIn this blog post, we will do the clustering analysis using the DBSCAN clustering method. Regarding the choice of this algorithm is explain above. Please have a look if you want to learn more. Regarding the data, you can access this data from here.\n\nLoad the necessary libraries:\n\nlibrary(fpc) \nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(corrplot)\nlibrary(dbscan)"
  },
  {
    "objectID": "posts/DBSCAN/index.html#load-the-data-into-r",
    "href": "posts/DBSCAN/index.html#load-the-data-into-r",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Load the data into R:",
    "text": "Load the data into R:\n\ndata &lt;- read.csv(\"/Users/test/Downloads/Mall_customers.csv\")\nhead(data)\n\n  CustomerID Gender Age Annual_Income Spending_Score\n1          1   Male  19            15             39\n2          2   Male  21            15             81\n3          3 Female  20            16              6\n4          4 Female  23            16             77\n5          5 Female  31            17             40\n6          6 Female  22            17             76\n\nstr(data)\n\n'data.frame':   200 obs. of  5 variables:\n $ CustomerID    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Gender        : chr  \"Male\" \"Male\" \"Female\" \"Female\" ...\n $ Age           : int  19 21 20 23 31 22 35 23 64 30 ...\n $ Annual_Income : int  15 15 16 16 17 17 18 18 19 19 ...\n $ Spending_Score: int  39 81 6 77 40 76 6 94 3 72 ..."
  },
  {
    "objectID": "posts/DBSCAN/index.html#data-cleaning",
    "href": "posts/DBSCAN/index.html#data-cleaning",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Data Cleaning:",
    "text": "Data Cleaning:\n\n# To see if the given dataset contains any null values or not\nsum(is.na(data_num))\n\n[1] 0"
  },
  {
    "objectID": "posts/DBSCAN/index.html#explanatory-analysis",
    "href": "posts/DBSCAN/index.html#explanatory-analysis",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Explanatory analysis",
    "text": "Explanatory analysis\n\n# Select the relevant columns\ndata_selected &lt;- data_num[, c('Age', 'Annual_Income', 'Spending_Score')]\n\n# Calculate the correlation matrix\ncorrs &lt;- cor(data_selected)\n\n# Create a heatmap with correlation values\ncorrplot(corrs, method=\"color\", type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", # Add correlation coefficients\n         tl.col=\"black\", tl.srt=45) # Text label color and rotation\n\n\n\n\nFrom the above correlation table, we can see that either we have negative values which indicated that they are negatively correlated or very low values indicating not a strong correlation between them\n\nlets explore the data more:\n\nDistribution of the variables:\n\n# Create a histogram for 'age'\nggplot(data, aes(x=Age)) +\n  geom_histogram(bins = 30, fill =  \"blue\", color = \"black\") +\n  labs(title=\"Distribution of Age\", x=\"Age\", y=\"Count\")\n\n\n\n# Create a histogram for 'income'\nggplot(data, aes(x=Annual_Income)) +\n  geom_histogram(bins = 30, fill = 'green', color = 'black') +\n  labs(title=\"Distribution of Income\", x=\"Income\", y=\"Count\")\n\n\n\n#Create a histogram for spending score:\nggplot(data, aes(x=Spending_Score))+\n  geom_histogram(bins = 30, fill='darkgreen', color='black')+\n  labs(title= \"Distribution of Spending Score\", x='spending score', y= 'count')\n\n\n\n\nFrom the above distribution, We can see that the age group near 30-40 has the highest density, most customers have income in the range of 50-80k, and most customers have a spending score of 50.\nLets see the box plot for Gender by Spending Score:\n\n# Subset the data\nmale_charges &lt;- data[data$Gender == \"Male\", \"Spending_Score\"]\nfemale_charges &lt;- data[data$Gender == \"Female\", \"Spending_Score\"]\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  Gender = rep(c(\"Male\", \"Female\"), times = c(length(male_charges), length(female_charges))),\n  Charges = c(male_charges, female_charges)\n)\n# Create the box plot\nggplot(plot_data, aes(x = Gender, y = Charges, fill = Gender)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Box Plot of Charges by Gender\", x = \"Gender\", y = \"Spending Score\", fill = \"Gender\")\n\n\n\n\n\n#Average Spending score by gender:\n# Calculate the average bmi for each region\navg_bmi_by_region &lt;- data %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(Average_Spending_score = mean(Spending_Score))\n\nprint(avg_bmi_by_region)\n\n# A tibble: 2 √ó 2\n  Gender Average_Spending_score\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 Female                   51.5\n2 Male                     48.5\n\n\nLets see the relationship between Annual income and spending score:\n\n# Create a scatter plot with regression line\nggplot(data, aes(x=Annual_Income, y=Spending_Score)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, color=\"red\") +\n  labs(title=\"Relation between Annual Income and Spending Score\", x=\"Annual Income\", y=\"Spending Score\")\n\n\n\n\nCalculate and print the correlation coefficient:\n\ncorrelation &lt;- cor(data$Annual_Income, data$Spending_Score)\nprint(paste(\"Correlation coefficient: \", correlation))\n\n[1] \"Correlation coefficient:  0.00990284809403761\"\n\n\nFrom this, we can see that there is no correlation between Annual income vs Spending Score."
  },
  {
    "objectID": "posts/DBSCAN/index.html#lets-calculate-silhouette-score",
    "href": "posts/DBSCAN/index.html#lets-calculate-silhouette-score",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Lets calculate Silhouette Score:",
    "text": "Lets calculate Silhouette Score:\nBefore dealing with the calculation of the Silhouette Score, lets get acquainted with what the Silhouette Score is and its importance to us while doing the DBSCAN clustering:\nThe Silhouette Score measures how similar an object is to its cluster compared to others. It ranges from -1 to 1, where a high value indicates that the object is well-matched to its cluster and poorly matched to neighboring clusters. The clustering configuration is appropriate if most objects have a high value. If many points have a low or negative value, the clustering configuration may have too many clusters.\nThe silhouette score provides a succinct graphical representation of how well each object lies within its cluster. It is a way to track the validity of the clusters formed by the algorithm. It can be particularly useful in the context of DBSCAN, as the algorithm does not explicitly minimize or maximize any particular objective function.\n\n# Loop over all combinations of parameter values\nfor (eps in eps_values) {\n  for (min_samples in min_samples_values) {\n    # Perform DBSCAN clustering\n    dbscan_result &lt;- dbscan(df, eps = eps, minPts = min_samples)\n    \n    # Append the number of clusters to the 'clusters' vector\n    clusters &lt;- c(clusters, max(dbscan_result$cluster))\n    \n    # Calculate the silhouette score and append it to the 'sil_score' vector\n    sil_score &lt;- c(sil_score, cluster.stats(dist(df), dbscan_result$cluster)$avg.silwidth)\n  }\n}\n\n# Create a data frame with the results\ndbscan_df &lt;- expand.grid(Eps = eps_values, Min_Samples = min_samples_values)\ndbscan_df$Number_of_Clusters &lt;- clusters\ndbscan_df$Silhouette_Score &lt;- sil_score\n\n# Print the data frame\nprint(dbscan_df)\n\n   Eps Min_Samples Number_of_Clusters Silhouette_Score\n1   11           5                  5       0.15950489\n2   12           5                  4       0.18222045\n3   13           5                  5       0.04014986\n4   14           5                  4       0.14733884\n5   15           5                  4       0.13127178\n6   16           5                  4       0.09690751\n7   11           6                  3       0.03839425\n8   12           6                  4      -0.04371114\n9   13           6                  4      -0.04716035\n10  14           6                  5       0.17636596\n11  15           6                  3       0.18050065\n12  16           6                  4       0.20657473\n13  11           7                  4       0.20473300\n14  12           7                  4       0.15845864\n15  13           7                  3       0.19532476\n16  14           7                  3       0.19532476\n17  15           7                  3       0.09531062\n18  16           7                  3       0.05953447\n19  11           8                  3       0.19456187\n20  12           8                  4       0.23057899\n21  13           8                  4       0.12729075\n22  14           8                  3       0.19988329\n23  15           8                  4       0.21660975\n24  16           8                  4       0.18699359\n25  11           9                  3       0.24719452\n26  12           9                  3       0.21578135\n27  13           9                  4       0.19421971\n28  14           9                  2       0.26949650\n29  15           9                  4       0.27034017\n30  16           9                  4       0.24966531\n31  11          10                  3       0.25886559\n32  12          10                  3       0.21178070\n33  13          10                  3       0.19155599\n34  14          10                  4       0.22168976\n35  15          10                  3       0.25894850\n36  16          10                  3       0.23688910\n37  11          11                  2       0.27196809\n38  12          11                  2       0.27248722\n39  13          11                  4       0.28921007\n40  14          11                  3       0.26889522\n41  15          11                  4       0.15181851\n42  16          11                  3       0.21849659\n43  11          12                  4       0.24945312\n44  12          12                  4       0.21884485\n45  13          12                  3       0.26591736\n46  14          12                  1       0.32699544\n47  15          12                  1       0.33022559\n48  16          12                  2       0.27662364\n49  11          13                  2       0.25255951\n50  12          13                  3       0.22481604\n51  13          13                  2       0.26248415\n52  14          13                  2       0.22931199\n53  15          13                  2       0.21113928\n54  16          13                  3       0.20679545\n\n\n\n# Convert 'Silhouette_Score' to numeric\ndbscan_df$Silhouette_Score &lt;- as.numeric(as.character(dbscan_df$Silhouette_Score))\n\n# Find the maximum silhouette score\nmax_sil_score &lt;- max(dbscan_df$Silhouette_Score)\n\n# Filter the data frame for the maximum silhouette score\ndbscan_df[dbscan_df$Silhouette_Score == max_sil_score, ]\n\n   Eps Min_Samples Number_of_Clusters Silhouette_Score\n47  15          12                  1        0.3302256\n\n\nThe stronger the clustering, the closer the result is near 1. We have reached a maximum Silhouette Score of 0.3302256 with Eps = 15 and Min Samples = 12. To get the best clustering, we will fit these values into the DBSCAN algorithm.\nPerform DBSCAN clustering:\n\ndbscan_result &lt;- dbscan(df, eps = 15, minPts = 12)\n\n# Add the cluster assignments to the data frame\ndf$DBSCAN_Clusters &lt;- dbscan_result$cluster\n\n# Sort the data frame by the 'DBSCAN_Clusters' column\ndf &lt;- df[order(df$DBSCAN_Clusters), ]\n\n# Print the data frame\nprint(head(df))\n\n  Gender Age Annual_Income Spending_Score DBSCAN_Clusters\n1      0  19            15             39               0\n3      1  20            16              6               0\n5      1  31            17             40               0\n7      1  35            18              6               0\n8      1  23            18             94               0\n9      0  64            19              3               0\n\n\n\n# Convert 'DBSCAN_Clusters' to character\ndf$DBSCAN_Clusters &lt;- as.character(df$DBSCAN_Clusters)\n\n# Replace '-1' with 'Outliers'\ndf$DBSCAN_Clusters[df$DBSCAN_Clusters == \"-1\"] &lt;- \"Outliers\"\n\n\n# Create the scatter plot\np &lt;- ggplot(df, aes(x = Annual_Income, y = Spending_Score, color = DBSCAN_Clusters)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"black\", \"darkred\", \"#0091F7\", \"darkgreen\", \"#F7F700\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 20),\n    legend.title = element_text(size = 12),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12)\n  ) +\n  labs(\n    title = \"DBSCAN Clusters\",\n    x = \"Annual Income\",\n    y = \"Sum of Spending Scores\",\n    color = \"Clusters\"\n  )\n\n# Print the plot\nprint(p)\n\n\n\n\n\nConclusion:\nAs we can see, the lack of substantial density in our data causes DBSCAN to perform poorly. The black label indicates outliers, so it will mostly appear as such. Having a more dense data structure means we can get better performance from DBSCAN clustering."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Blog Posts\nWelcome to my blog! Below is a list of posts:\n\nAnomaly Detection\nClassification Crop Suitability\nDBSCAN\nLinear Regression\nNon-Linear Regression\nProbability\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Probability/index.html",
    "href": "posts/Probability/index.html",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "",
    "text": "Probability theory serves as a cornerstone for Machine Learning (ML). It offers an approach to expressing statements and formulating the learning problem. In this blog post we will explore the aspects of probability theory and random variables and their significant impact on ML. Additionally we will provide an example, in R to demonstrate these concepts."
  },
  {
    "objectID": "posts/Probability/index.html#probability-theory",
    "href": "posts/Probability/index.html#probability-theory",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability theory is a field, within mathematics that tackles the concept of uncertainty. It offers a structure to help us quantify our beliefs or uncertainties. The probability of an event serves as a metric to gauge the likelihood of its occurrence.\nIn machine learning we frequently encounter situations where dealing with uncertainty becomes essential. Take the case of constructing a spam classifier for instance. We often find ourselves unsure whether an incoming email should be classified as spam or not. Probability theory acts as a framework to navigate through this uncertainty and make decisions."
  },
  {
    "objectID": "posts/Probability/index.html#probability-fundamentals",
    "href": "posts/Probability/index.html#probability-fundamentals",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Probability Fundamentals:",
    "text": "Probability Fundamentals:\n\na. Sets, events, and probability:\nA set is a collection of distinct objects, elements, or points. For example, the set of all fruits in a basket or the set of all students in a class.\nA subset is a set that is contained within another set. If every element of set A is contained in set B, we denote this by A‚äÇB. For example, the set of all apples in a basket is a subset of the set of all fruits in the basket.\nTwo sets A and B are mutually exclusive (or disjoint) if they have no elements in common. This is denoted by A‚à©B=‚àÖ. For example, the set of all apples and the set of all oranges in a basket are mutually exclusive because there are no elements that are both an apple and an orange.\nA random experiment is an experiment whose result or outcome is uncertain before it is performed. For example, tossing a coin is a random experiment because we are uncertain whether the outcome will be a head or a tail.\nThe set of all possible outcomes of a random experiment is called the sample space, usually denoted by S or Œ©. Each outcome is called a sample point in the sample space. For example, in a coin toss experiment, the sample space S = {Head, Tail}.\nAn event is a subset of the sample space, i.e., a collection of possible outcomes to which a probability can be assigned. Events that cannot be decomposed are called simple events, otherwise, they are called compound events. For example, getting a head in a coin toss experiment is a simple event.\n\n\nb. Conditional probability:\nIn machine learning, understanding probability and statistics is crucial. One such concept is conditional probability, which refers to the probability of an event given that another event has occurred. Conditional probability is the probability of an event (A), given that another (B) has already occurred. If the event of interest is A and event B is known or assumed to have occurred, the conditional probability of A given B is usually written as P(A | B).\nThe formula for conditional probability is defined as:\nP(A/B)= P (A‚à©B)/P(B)\n‚Äã\n\nFigure source:"
  },
  {
    "objectID": "posts/Probability/index.html#random-variables",
    "href": "posts/Probability/index.html#random-variables",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable refers to a variable that takes on values based on the outcomes of an event. There are two types of variables; discrete and continuous. A discrete random variable has an countable number of values while a continuous random variable can have an infinite number of possible values, within a range along the real number line.\nWhen it comes to machine learning random variables can represent elements. For instance in a spam classifier a random variable may indicate whether an email is classified as spam (1) or not (0).\nTo better understand the comparison, between continuous variables you can refer to the table provided below.\n\nFigure credit: Discrete vs continuous"
  },
  {
    "objectID": "posts/Probability/index.html#probability-distributions",
    "href": "posts/Probability/index.html#probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nA probability distribution describes how a random variable is distributed. It tells us what the probabilities of each outcome are. For discrete random variables, we use a probability mass function (PMF). For continuous random variables, we use a probability density function (PDF).\nWith the facts at hand, one can utilize the decision tree depicted in the picture below to get an understanding of some common probability distributions:\n\nFigure credit: Probabilistic approaches to risk by Aswath Damodaran."
  },
  {
    "objectID": "posts/Probability/index.html#discrete-probability-distributions",
    "href": "posts/Probability/index.html#discrete-probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Discrete Probability Distributions:",
    "text": "Discrete Probability Distributions:\nA discrete probability distribution applies to scenarios where the set of possible outcomes is discrete. Common examples of discrete probability distributions include the Bernoulli, Binomial, and Poisson distributions.\nExample: Binomial distribution\nLet‚Äôs create a hypothetical scenario where we toss a coin 10 times. We are interested in the number of times we get heads. This scenario follows a binomial distribution. Lets do the data analysis for better understanding of it.\nLoad necessary libraries:\n\n# Load necessary library\nlibrary(ggplot2)\n\nSet the random seed for reproducibility:\n\n# Set parameters\nsize &lt;- 10\nprob &lt;- 0.5\n\n# Generate binomial data\nset.seed(123)\ndata &lt;- rbinom(1000, size, prob)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\") +\n  labs(x = \"Number of Heads\", y = \"Frequency\", title = \"Binomial Distribution\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/Probability/index.html#continuous-probability-distributions",
    "href": "posts/Probability/index.html#continuous-probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Continuous Probability Distributions:",
    "text": "Continuous Probability Distributions:\nA continuous probability distribution applies to scenarios where the set of possible outcomes is an interval of real numbers. Common examples of continuous probability distributions include the Normal, Exponential, and Beta distributions.\nExample: Normal distiribution\nLet‚Äôs understand it using a hypothetical data where we measure the heights of a large group of individuals. The heights of individuals in a large population often follow a normal distribution.\n\n# Set parameters\nmean &lt;- 170\nsd &lt;- 10\n\n# Generate normal data\nset.seed(123)\ndata &lt;- rnorm(1000, mean, sd)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\", aes(y = ..density..)) +\n  geom_density(alpha = 0.2, fill = \"#FF6666\") +\n  labs(x = \"Height (cm)\", y = \"Density\", title = \"Normal Distribution\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\nUnderstanding the probability using a real example:\nConsider a segment of 100 base pairs on the mouse genome. Among these 100 base pairs, there are 18 A‚Äôs, 22 T‚Äôs, 33 C‚Äôs, and 27 G‚Äôs. What is the probability of observing base A, T, C, and G in the mouse genome?\n\n# Number of each base\nnum_A &lt;- 18\nnum_T &lt;- 22\nnum_C &lt;- 33\nnum_G &lt;- 27\n\n\n# Total number of base pairs\ntotal_bp &lt;- 100\n\n# Calculate probabilities\nprob_A &lt;- num_A / total_bp\nprob_T &lt;- num_T / total_bp\nprob_C &lt;- num_C / total_bp\nprob_G &lt;- num_G / total_bp\n\n# Print probabilities\nprint(paste(\"Probability of A: \", prob_A))\n\n[1] \"Probability of A:  0.18\"\n\nprint(paste(\"Probability of T: \", prob_T))\n\n[1] \"Probability of T:  0.22\"\n\nprint(paste(\"Probability of C: \", prob_C))\n\n[1] \"Probability of C:  0.33\"\n\nprint(paste(\"Probability of G: \", prob_G))\n\n[1] \"Probability of G:  0.27\"\n\n# Visualize the result\nbarplot(c(prob_A, prob_T, prob_C, prob_G), names.arg = c(\"A\", \"T\", \"C\", \"G\"),\n        xlab = \"Base\", ylab = \"Probability\", main = \"Probability of Each Base\")"
  },
  {
    "objectID": "posts/Probability/index.html#conclusion",
    "href": "posts/Probability/index.html#conclusion",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Conclusion:",
    "text": "Conclusion:\nProbability theory and random variables are essential tools. Understanding discrete and continuous probability distributions is crucial in machine learning. They provide a foundation for many machine learning algorithms and statistical tests. They provide a framework to handle uncertainty and formulate learning problems. Understanding these concepts can help you build more effective and robust machine learning models."
  },
  {
    "objectID": "research/fungicide.html",
    "href": "research/fungicide.html",
    "title": "Fungicide Efficacy",
    "section": "",
    "text": "Fungicide Efficacy\nFoliar fungicides are a key tool for managing corn diseases like tar spot (Phyllachora maydis) and gray leaf spot (Cercospora zeae‚Äëmaydis). Their effectiveness, however, depends on the active ingredients, timing of application, hybrid susceptibility, and disease pressure.\nMy research in fungicide efficacy focuses on the following areas:\n\nEvaluation of fungicides against Curvularia leaf spot, gray leaf spot, and tar spot disease: Conducted replicated field trials across Virginia to compare SDHI, QoI, and mixed‚Äëmode products under natural infection.\nConducted field trial\nHybrid succeptibility evaluations: Tested six corn hybrids in 2023 and two hybrids in 2024 at Shenandoah County and Montgomery County, assessing disease severity from VT/R1 through R6.\n\nMy approach combines field trials with controlled laboratory assays to provide robust, actionable insights. These results feed into a web‚Äëbased decision‚Äësupport tool for growers and agronomists, helping to optimize application timing, product choice, and economic return.\nFor further reading on resistance‚Äëmanagement strategies, see the Fungicide Resistance Action Committee (FRAC) resources: https://www.frac.info/.\nI‚Äôm actively developing this section‚Äîstay tuned for updates!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Welcome to my research section. Here, you can explore my work in plant pathology, including studies on fungicide efficacy and metagenomics.\nClick on the topics below for more details:\n\nFungicide Efficacy and hybrid succeptibility\n\nMulti-year and multi-location field trial on fungicide¬†evaluations and hybrid susceptibility¬†against three major corn foliar diseases.\n\nMetagenomics & Bioinformatics\n\nStay tune for more update !!!\n\n\n\n Back to top"
  },
  {
    "objectID": "research/metagenomics.html",
    "href": "research/metagenomics.html",
    "title": "Metagenomics & Bioinformatics",
    "section": "",
    "text": "This research investigates the microbial communities associated with corn plants using metagenomic sequencing and bioinformatics analysis. Key focus areas include:\n\nUtilizing metagenomic sequencing methods to profile microbial diversity.\nAnalyzing sequencing data with advanced bioinformatics pipelines.\nInterpreting the implications of microbial community dynamics on plant health.\n\nFor more on metagenomics techniques, check out the resources provided by the National Center for Biotechnology Information (NCBI) :contentReferenceoaicite:1.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Non_Linear regression/index.html#working-mechanism",
    "href": "posts/Non_Linear regression/index.html#working-mechanism",
    "title": "Understanding Non-Linear Regression",
    "section": "Working mechanism:",
    "text": "Working mechanism:\nNon-linear regression models the relationship between a dependent variable and one or more independent variables using a non-linear function. This function is typically a polynomial, exponential, logarithmic, or other non-linear function.\nThe goal of non-linear regression is to find the parameters that minimize the difference between the predicted and actual output values. This is often done using iterative optimization algorithms, such as gradient descent or Newton's method."
  },
  {
    "objectID": "posts/Non_Linear regression/index.html#lets-understand-it-by-example-from-each",
    "href": "posts/Non_Linear regression/index.html#lets-understand-it-by-example-from-each",
    "title": "Understanding Non-Linear Regression",
    "section": "Lets understand it by example from each:",
    "text": "Lets understand it by example from each:\n\nPolynomial Regression (Quadratic relationship)\n\n\nlibrary(ggplot2)\n\n\n# Generate some sample data\nset.seed(123)\nx &lt;- seq(-10, 10, by = 0.1)\ny &lt;- x^2 + rnorm(length(x), sd = 10)\n\n# Fit a non-linear regression model\nmodel &lt;- nls(y ~ a * x^2, start = list(a = 1))\n\n# Print the model summary\nprint(summary(model))\n\n\nFormula: y ~ a * x^2\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na  1.00517    0.01489   67.52   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.533 on 200 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 9.898e-10\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model) * x^2, color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Quadratic Regression\") +\n  theme_minimal()\n\n\n\n\n\nExponential Regression\n\n# Generate some sample data\nset.seed(123)\nx &lt;- seq(0, 10, by = 0.1)\ny &lt;- exp(x) + rnorm(length(x), sd = 10)\n\n# Fit a non-linear regression model\nmodel &lt;- nls(y ~ a * exp(b * x), start = list(a = 1, b = 1))\n\n# Print the model summary\nprint(summary(model))\n\n\nFormula: y ~ a * exp(b * x)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \na 1.0042532  0.0033758   297.5   &lt;2e-16 ***\nb 0.9995746  0.0003516  2843.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.085 on 99 degrees of freedom\n\nNumber of iterations to convergence: 2 \nAchieved convergence tolerance: 4.948e-08\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model)[1] * exp(coef(model)[2] * x), color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Exponential Regression\") +\n  theme_minimal()\n\n\n\n\n\n\nLogarithmic Regression\n\n# Generate some sample data\nset.seed(123)\nx &lt;- seq(1, 10, by = 0.1)\ny &lt;- log(x) + rnorm(length(x), sd = 0.1)\n\n# Fit a non-linear regression model\nmodel &lt;- nls(y ~ a * log(b * x), start = list(a = 1, b = 1))\n\n# Print the model summary\nprint(summary(model))\n\n\nFormula: y ~ a * log(b * x)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na  1.00204    0.01572   63.73   &lt;2e-16 ***\nb  1.00306    0.02625   38.21   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08978 on 89 degrees of freedom\n\nNumber of iterations to convergence: 2 \nAchieved convergence tolerance: 4.398e-09\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model)[1] * log(coef(model)[2] * x), color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Logarithmic Regression\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/Non_Linear regression/index.html#lets-visualize-the-result-obtained-from-model",
    "href": "posts/Non_Linear regression/index.html#lets-visualize-the-result-obtained-from-model",
    "title": "Understanding Non-Linear Regression",
    "section": "Lets visualize the result obtained from model:",
    "text": "Lets visualize the result obtained from model:\n\n# Define the x values\nx &lt;- seq(from = 1960, to = 2015, length.out = 55)\nx &lt;- x / max(x)\n\n# Calculate the y values\ny &lt;- sigmoid(x, coef(fit)[1], coef(fit)[2])\n\n# Create a dataframe for plotting\ndf &lt;- data.frame(x = c(xdata, x), y = c(ydata, y), group = rep(c(\"data\", \"fit\"), each = 55))\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create the plot\nggplot(df, aes(x = x, y = y, color = group)) +\n  geom_point(data = df[df$group == \"data\", ]) +\n  geom_line(data = df[df$group == \"fit\", ], size = 1.5) +\n  scale_color_manual(values = c(\"darkgreen\", \"blue\")) +\n  labs(x = \"Year\", y = \"GDP\", color = \"Legend\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n# Define the sigmoid function\nsigmoid &lt;- function(x, Beta_1, Beta_2) {\n  y &lt;- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Split data into train/test\nset.seed(0) # for reproducibility\nmsk &lt;- runif(nrow(df)) &lt; 0.8\ntrain_x &lt;- xdata[msk]\ntest_x &lt;- xdata[!msk]\ntrain_y &lt;- ydata[msk]\ntest_y &lt;- ydata[!msk]\n\n# Initial parameter values\nstart &lt;- c(Beta_1 = 1, Beta_2 = 1)\n\n# Build the model using train set\nfit &lt;- nlsLM(train_y ~ sigmoid(train_x, Beta_1, Beta_2), start = start)\n\n# Print the final parameters\nprint(coef(fit))\n\n     Beta_1      Beta_2 \n698.7956319   0.9972452 \n\n# Predict using test set\ny_hat &lt;- predict(fit, list(x = test_x))\n\nprint(y_hat)\n\n [1] 7.073461e-08 1.000734e-07 1.415811e-07 2.833863e-07 8.024885e-07\n [6] 1.135339e-06 1.606245e-06 2.272471e-06 3.215028e-06 4.548528e-06\n[11] 6.435122e-06 9.104212e-06 1.288034e-05 1.822266e-05 3.647350e-05\n[16] 5.160093e-05 1.032782e-04 1.461089e-04 2.066984e-04 2.924063e-04\n[21] 4.136383e-04 5.851038e-04 8.275882e-04 1.655114e-03 2.340004e-03\n[26] 3.307365e-03 4.672760e-03 6.598106e-03 1.311995e-02 1.846129e-02\n[31] 2.592005e-02 3.628093e-02 5.056831e-02 9.633731e-02 1.310586e-01\n[36] 1.758585e-01 2.318858e-01 2.992809e-01 3.766586e-01 4.608835e-01\n[41] 5.474029e-01 6.311496e-01 7.076754e-01 8.289294e-01 8.726981e-01\n\n\n\nEvaluate the model we developed earlier:\n\n# Observed values\ny_obs &lt;- ydata\n\n# Fitted/predicted values\ny_pred &lt;- sigmoid(xdata, coef(fit)[1], coef(fit)[2])\n\n# Calculate RMSE\nrmse &lt;- sqrt(mean((y_obs - y_pred)^2))\n\n# R-squared\nSS_res &lt;- sum((y_obs - y_pred)^2) \nSS_tot &lt;- sum((y_obs - mean(y_obs))^2)\nrsq &lt;- 1 - SS_res/SS_tot\n\n# Print metrics\ncat(\"RMSE:\", rmse, \"\\n\")\n\nRMSE: 0.03953521 \n\ncat(\"R-squared:\", rsq)\n\nR-squared: 0.9726905"
  },
  {
    "objectID": "posts/anomaly detection/index.html#introduction-1",
    "href": "posts/anomaly detection/index.html#introduction-1",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection, also known as outlier detection, is a fascinating aspect of machine learning. It involves identifying data points, events, or observations that deviate significantly from the norm. These anomalies can often provide critical and actionable insights in various domains, such as fraud detection in banking, intrusion detection in network security, and fault detection in critical systems."
  },
  {
    "objectID": "posts/anomaly detection/index.html#what-is-anomaly-detection",
    "href": "posts/anomaly detection/index.html#what-is-anomaly-detection",
    "title": "Anomaly Detection in Machine Learning",
    "section": "What is Anomaly Detection?",
    "text": "What is Anomaly Detection?\nAnomaly detection is the process of identifying unexpected items or events in datasets, which differ from the norm. In other words, it‚Äôs about finding the ‚Äòoutliers‚Äô in your data. For example, in a manufacturing context, an anomalous event could be a sudden increase in defective products."
  },
  {
    "objectID": "posts/anomaly detection/index.html#types-of-anomalies",
    "href": "posts/anomaly detection/index.html#types-of-anomalies",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Types of Anomalies",
    "text": "Types of Anomalies\nThere are three main types of anomalies:\n\nPoint Anomalies: A single instance of data is anomalous if it‚Äôs too far off from the rest. For example, spending $100 on food every day during the holiday season is normal, but may be odd otherwise.\nContextual Anomalies: The abnormality is context-specific. This type of anomaly is common in time-series data. For example, spending $100 on food during the holiday season is normal, but may be odd otherwise.\nCollective Anomalies: A set of data instances collectively helps in detecting anomalies. For example, someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack."
  },
  {
    "objectID": "posts/anomaly detection/index.html#anomaly-detection-techniques",
    "href": "posts/anomaly detection/index.html#anomaly-detection-techniques",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Anomaly Detection Techniques",
    "text": "Anomaly Detection Techniques\nThere are several techniques used for anomaly detection, each with its strengths and weaknesses. Some of the most popular methods include:\n\nStatistical Methods: These methods model the normal data behavior using statistical parameters like mean, median, mode, variance, etc. Any data instance that doesn‚Äôt fit this model is considered an anomaly.\nMachine Learning-Based Methods: These include techniques like clustering, classification, and nearest neighbors. These methods can either be supervised (labels are available) or unsupervised (no labels).\nTime Series Analysis: This is particularly useful for sequential data, where some pattern or trend is expected. Techniques used here include state space models, decomposition methods, etc.\n\n\nLet‚Äôs get familiar with the fundamental idea underlying the hyperparameters before working with the data. We must examine a few of the hyperparameters that characterize the DBScan job in order to comprehend the idea of the core points. min_samples is the first hyperparameter (HP). This is the bare minimum of core points required for cluster formation. The second crucial HP is ‚Äòeps‚Äô. ‚Äúeps‚Äù is the greatest separation that two samples must have in order to be grouped together. Although border points are somewhat farther from the cluster center, they are nonetheless part of the same cluster as core points. All other data points are referred to as ‚ÄúNoise Points‚Äù because they are unrelated to any cluster. They require more research because they may be unusual or not."
  },
  {
    "objectID": "posts/anomaly detection/index.html#about-the-data",
    "href": "posts/anomaly detection/index.html#about-the-data",
    "title": "Anomaly Detection in Machine Learning",
    "section": "About the data:",
    "text": "About the data:\nThere are seven columns and 1338 rows in the data, which indicates that there are seven separate variables. The remaining seven variables‚Äîage, sex, bmi, children, smoker, region, and charges. You can access the data from here:"
  },
  {
    "objectID": "posts/anomaly detection/index.html#import-necessary-libraries",
    "href": "posts/anomaly detection/index.html#import-necessary-libraries",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Import necessary libraries:",
    "text": "Import necessary libraries:\n\nlibrary(fpc)\nlibrary(ggplot2)\nlibrary(dbscan)\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:fpc':\n\n    dbscan\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nlibrary(cluster)\n\nWarning: package 'cluster' was built under R version 4.3.3\n\nlibrary(FNN)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded"
  },
  {
    "objectID": "posts/anomaly detection/index.html#import-the-data-set-and-visualize-the-data",
    "href": "posts/anomaly detection/index.html#import-the-data-set-and-visualize-the-data",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Import the data set and visualize the data:",
    "text": "Import the data set and visualize the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/insurance.csv\")\n\n\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\n\nLets see if the data contains any null values:\n\nsum(is.na(data))\n\n[1] 0\n\n\nThis data set does not contain null values. Thus, we don‚Äôt have to deal with it further."
  },
  {
    "objectID": "posts/anomaly detection/index.html#explanatory-analysis",
    "href": "posts/anomaly detection/index.html#explanatory-analysis",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Explanatory analysis:",
    "text": "Explanatory analysis:\n\n# Select the relevant columns\ndata_selected &lt;- data[, c('age', 'bmi', 'charges', 'children')]\n\n# Calculate the correlation matrix\ncorrs &lt;- cor(data_selected)\n\n# Create a heatmap with correlation values\ncorrplot(corrs, method=\"color\", type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", # Change correlation coefficients color to white\n         tl.col=\"black\", tl.srt=45, # Text label color and rotation\n         col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(200), # Change color scheme\n         title=\"Correlation Heatmap\") # Add title\n\n\n\n\nFrom this, we can see there is not much strong correlation between the different variables.\n\nLets see the distribution of charges variable:\n\n# Create a histogram of charges\nggplot(data, aes(x=charges)) +\n  geom_histogram(binwidth=1000, color=\"black\", fill=\"lightblue\") +\n  labs(title=\"Distribution of Charges\", x=\"Charges\", y=\"Count\") +\n  theme_minimal()\n\n\n\n\nFrom this histogram, we can see the distribution of our charges data which is left skewed and there is the possibility that this data set contains the outlier.\nLet‚Äôs explore the data set for charges for males and female:\n\n# Create a boxplot of charges by sex\nggplot(data, aes(x=sex, y=charges, color=sex)) +\n  geom_boxplot() +  # Include outliers\n  geom_jitter(width=0.2, alpha=0.5) +  # Add jittered points for better visualization\n  labs(title=\"Charges by Sex\", x=\"Sex\", y=\"Charges\") +\n  theme_minimal() +\n  scale_color_manual(values=c(\"blue\", \"darkgreen\"))  # Specify colors for each sex\n\n\n\n\nNow, lets see the charges by bmi for further data exploration:\n\n# Create a scatter plot of BMI vs charges\nggplot(data, aes(x=bmi, y=charges, color=sex)) +\n  geom_point(alpha=0.5) +  # Add points with transparency for better visualization\n  labs(title=\"BMI vs Charges\", x=\"BMI\", y=\"Charges\") +\n  theme_minimal() +\n  scale_color_manual(values=c(\"red\", \"blue\"))  # Specify colors for each sex"
  },
  {
    "objectID": "posts/anomaly detection/index.html#calculate-the-epsilon-value-using-k-distance-graph",
    "href": "posts/anomaly detection/index.html#calculate-the-epsilon-value-using-k-distance-graph",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Calculate the epsilon value using K-distance graph:",
    "text": "Calculate the epsilon value using K-distance graph:\n\n# Load necessary libraries\nlibrary(FNN)\n\n# Select the relevant columns\ndata_selected &lt;- data[, c('age', 'bmi')]\n\n# Standardize the data\ndata_std &lt;- scale(data_selected)\n\n# Compute the nearest neighbors\nk &lt;- 2  # 2 because the point itself is included\nknn_dist &lt;- knn.dist(data_std, k=k)\n\n# Sort the distances\nknn_dist &lt;- sort(knn_dist[,k], decreasing=FALSE)  # Exclude the distance to the point itself\n\n# Plot the k-distance graph\nplot(knn_dist, main=\"K-distance Graph\", xlab=\"Data Points sorted by distance\", ylab=\"Epsilon\")\n\n\n\n\nThe optimum value of epsilon is at the point of maximum curvature in the K-Distance Graph, which is 0.6 in this case. Domain knowledge affects minPoints‚Äô value. I‚Äôm using 10 minPoints at this time around.\nBased on the above calculation, we can do the DBSCAN clustering\n\n# Select the relevant columns\ndata_selected &lt;- data[, c('age', 'bmi')]\n\n# Standardize the data\ndata_std &lt;- scale(data_selected)\n\n# Perform DBSCAN clustering\ndbscan_res &lt;- dbscan(data_std, eps=0.6, minPts=10)  \nprint(dbscan_res)\n\nDBSCAN clustering for 1338 objects.\nParameters: eps = 0.6, minPts = 10\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 1 cluster(s) and 3 noise points.\n\n   0    1 \n   3 1335 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints"
  },
  {
    "objectID": "posts/anomaly detection/index.html#visualization",
    "href": "posts/anomaly detection/index.html#visualization",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Visualization:",
    "text": "Visualization:\n\n# Add the DBSCAN results to our data\ndata$cluster &lt;- as.factor(dbscan_res$cluster)\n\n# Define colors for each cluster\ncolors &lt;- rainbow(length(unique(dbscan_res$cluster)))\nnames(colors) &lt;- levels(data$cluster)\n\n# Create the plot\nggplot(data, aes(x=age, y=bmi, color=cluster)) +\n  geom_point() +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  labs(x=\"Age\", y=\"BMI\", color=\"Cluster\") +\n  theme(legend.position=\"none\")\n\n\n\n\nFrom the above scatterplot, we can see that three data points are the noise and one cluster with no outliers.\n\n# Create an outliers data frame\noutliers &lt;- data[data$cluster == -1, ]\n\n# Print the outliers\nprint(outliers)\n\n[1] age      sex      bmi      children smoker   region   charges  cluster \n&lt;0 rows&gt; (or 0-length row.names)"
  },
  {
    "objectID": "posts/linear regression/index.html#load-necessary-libraries",
    "href": "posts/linear regression/index.html#load-necessary-libraries",
    "title": "Understanding Linear Regression",
    "section": "Load necessary libraries:",
    "text": "Load necessary libraries:\n\n# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(reshape2)\nlibrary(Metrics)\nlibrary(minpack.lm)\n\n\nLoad the data:\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ...\n\n\n\n\nLoad and clean the data:\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n# Handle missing values (if any)\n# data &lt;- na.omit(data)  # drops rows with missing values\n# data &lt;- data[complete.cases(data), ]  # drops rows with missing values\n# data$column &lt;- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n\nIn my case, the data does not contain the null values; however, if you select the data with the null values and want to analyze the data, it is important to deal with the null values before proceeding further analysis. Different ways to deal with the null values in R is given in the code above.\nUpon executing the above code, we got the value 0, which means there are no missing values in the data. Since we don‚Äôt have any null or missing values in this data, as indicated above. So, no further handling of missing data is needed.\n\n\nVisualize the data: for better understanding of the data before proceeding further:\n\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets take a look for the correlation between these two features: Our data contains one column ‚ÄúSpecies‚Äù which is non-numeric. Thus, we will firstly subset the data that doesnot contains ‚ÄúSpecies‚Äù column.\n\n# Subset the data to include only numeric columns\ndata_numeric &lt;- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n\n\n\n\nCreate the heatmap for better visualization:\n\n# Melt the correlation matrix into a long format\ndata_melt &lt;- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n\n\n\n\nWe can see no strong correlation between different variables from the heatmap created using the above code. However, this part is made to visualize the data only.\n\n\nLets explore the data for charges by region:\n\n# Sum 'charges' grouped by 'region'\ncharges &lt;- aggregate(data$charges, by=list(data$region), FUN=sum)\n\n# Sort the data frame by 'charges' in ascending order\ncharges &lt;- charges[order(charges$x, decreasing=FALSE), ]\n\n# Create a bar plot\nggplot(charges[1:5, ], aes(x=Group.1, y=x, fill=x)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  labs(title=\"Sum of Charges by Region\", x=\"Region\", y=\"Charges\") +\n  theme_minimal() +\n  scale_fill_gradient(low = \"lightgreen\", high = \"blue\")\n\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n\n\n\n\n\nTherefore, the Southeast has the highest overall medical costs, whereas the Southwest has the lowest.\nIt is always interesting to explore the data; in this case, I am interested in seeing how smoking habits affect the charges associated with insurance. Let‚Äôs explore this below:\n\n# Create a scatter plot for 'age' vs 'charges' colored by 'smoker'\np1 &lt;- ggplot(data, aes(x=age, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"red\", \"blue\")) +\n  labs(title=\"Scatter plot of Charges vs Age\", x=\"Age\", y=\"Charges\")\n\n# Create a scatter plot for 'bmi' vs 'charges' colored by 'smoker'\np2 &lt;- ggplot(data, aes(x=bmi, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"red\", \"blue\")) +\n  labs(title=\"Scatter plot of Charges vs BMI\", x=\"BMI\", y=\"Charges\")\n\n# Create a scatter plot for 'children' vs 'charges' colored by 'smoker'\np3 &lt;- ggplot(data, aes(x=children, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"green\", \"purple\")) +\n  labs(title=\"Scatter plot of Charges vs Children\", x=\"Children\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nprint(p2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nprint(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOverall, the insurance charge is significantly higher for an individual who smokes."
  },
  {
    "objectID": "posts/linear regression/index.html#section",
    "href": "posts/linear regression/index.html#section",
    "title": "Understanding Linear Regression",
    "section": "",
    "text": "# Create a box plot for 'charges' vs 'sex'\np1 &lt;- ggplot(data, aes(x=sex, y=charges)) +\n  geom_boxplot(fill=\"yellowgreen\") +\n  labs(title=\"Box plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a box plot for 'charges' vs 'smoker'\np2 &lt;- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_boxplot(fill=\"goldenrod1\") +\n  theme_minimal() +\n  labs(title=\"Box plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n\n\nprint(p2)\n\n\n\n\nAccording to the plot, the average insurance cost for men and women is roughly the same at $5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about $5,000. The minimum insurance premium for smokers is $5,000.\n\n# Create the box plot\nggplot(data, aes(x = as.factor(children), y = charges, fill = sex)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +\n  theme_minimal() +\n  labs(title = \"Box plot of charges vs children\", x = \"Children\", y = \"Charges\", fill = \"Sex\")\n\n\n\n\nWe can see some outliers data in different children age group."
  },
  {
    "objectID": "posts/linear regression/index.html#building-the-model-and-prediction",
    "href": "posts/linear regression/index.html#building-the-model-and-prediction",
    "title": "Understanding Linear Regression",
    "section": "Building the model and prediction:",
    "text": "Building the model and prediction:\nFrom the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\nModel evaluation: After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse) and R squared value.\n\n# Set the seed for reproducibility\nset.seed(0)\n\n# Create the independent variable data frame\nx &lt;- data[, !(names(data) %in% \"charges\")]\n\n# Create the dependent variable vector\ny &lt;- data$charges\n\n# Split the data into training and testing sets\nsplit &lt;- sample.split(y, SplitRatio = 0.8)\nx_train &lt;- x[split, ]\ny_train &lt;- y[split]\nx_test &lt;- x[!split, ]\ny_test &lt;- y[!split]\n\n# Fit a linear regression model\nLin_reg &lt;- lm(y_train ~ ., data = cbind(x_train, y_train))\n\n# Print the intercept and coefficients\nprint(paste(\"Intercept: \", Lin_reg$coefficients[1]))\n\n[1] \"Intercept:  -12169.8051075584\"\n\nprint(paste(\"Coefficients: \", Lin_reg$coefficients[-1]))\n\n[1] \"Coefficients:  262.305667539361\"  \"Coefficients:  -6.522038182275\"  \n[3] \"Coefficients:  344.253140133108\"  \"Coefficients:  453.766808768054\" \n[5] \"Coefficients:  23996.309146369\"   \"Coefficients:  -751.161736087778\"\n[7] \"Coefficients:  -1129.0287998095\"  \"Coefficients:  -1085.05633010975\"\n\n# Calculate R-squared on the test set\ny_pred &lt;- predict(Lin_reg, newdata = x_test)\nSSE &lt;- sum((y_pred - y_test)^2)\nSST &lt;- sum((mean(y_train) - y_test)^2)\nR_squared &lt;- 1 - SSE/SST\nprint(paste(\"R-squared: \", R_squared))\n\n[1] \"R-squared:  0.736401619028354\"\n\n# Calculate RMSE\nrmse_val &lt;- rmse(y_test, y_pred)\nprint(paste(\"RMSE: \", rmse_val))\n\n[1] \"RMSE:  5903.79948175291\"\n\n\nLets learn little bit about rmse: The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\nDuring model evaluation, RMSE serves as a measure to understand the model‚Äôs performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\nIn this case, the R-squared value is approximately 0.7364, which means that our model explains about 73.64% of the variance in the charges."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html",
    "href": "posts/Classification_crop  suitability/index.html",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#introduction",
    "href": "posts/Classification_crop  suitability/index.html#introduction",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#random-forest-rf",
    "href": "posts/Classification_crop  suitability/index.html#random-forest-rf",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Random Forest (RF)",
    "text": "Random Forest (RF)\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#support-vector-machines-svm",
    "href": "posts/Classification_crop  suitability/index.html#support-vector-machines-svm",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It‚Äôs effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#naive-bayes-nb",
    "href": "posts/Classification_crop  suitability/index.html#naive-bayes-nb",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Naive Bayes (NB)",
    "text": "Naive Bayes (NB)\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Na√Øve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area‚Äôs average temperature, humidity, pH level, rainfall, and NPK values."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#the-data",
    "href": "posts/Classification_crop  suitability/index.html#the-data",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "The Data",
    "text": "The Data\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type. You can find the data file in here.\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R‚Äôs head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R‚Äôs is.na() function."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#data-visualization",
    "href": "posts/Classification_crop  suitability/index.html#data-visualization",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo get a better understanding of the data, I visualized it using R‚Äôs ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#data-processing",
    "href": "posts/Classification_crop  suitability/index.html#data-processing",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Processing",
    "text": "Data Processing\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label."
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#model-selection",
    "href": "posts/Classification_crop  suitability/index.html#model-selection",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Model Selection",
    "text": "Model Selection\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\nLoad necessary libraries:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pheatmap)\n\nLoad the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/Crop_recommendation.csv\")\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\nset.seed(123)\n\nSplit the data into training and testing sets:\n\n# See the head and tail of data\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n\nCreate a data frame to store test data and predicted labels:\n\n# See the distribution of data \nsummary(data)\n\n       N                P                K           temperature    \n Min.   :  0.00   Min.   :  5.00   Min.   :  5.00   Min.   : 8.826  \n 1st Qu.: 21.00   1st Qu.: 28.00   1st Qu.: 20.00   1st Qu.:22.769  \n Median : 37.00   Median : 51.00   Median : 32.00   Median :25.599  \n Mean   : 50.55   Mean   : 53.36   Mean   : 48.15   Mean   :25.616  \n 3rd Qu.: 84.25   3rd Qu.: 68.00   3rd Qu.: 49.00   3rd Qu.:28.562  \n Max.   :140.00   Max.   :145.00   Max.   :205.00   Max.   :43.675  \n    humidity           ph           rainfall         label          \n Min.   :14.26   Min.   :3.505   Min.   : 20.21   Length:2200       \n 1st Qu.:60.26   1st Qu.:5.972   1st Qu.: 64.55   Class :character  \n Median :80.47   Median :6.425   Median : 94.87   Mode  :character  \n Mean   :71.48   Mean   :6.469   Mean   :103.46                     \n 3rd Qu.:89.95   3rd Qu.:6.924   3rd Qu.:124.27                     \n Max.   :99.98   Max.   :9.935   Max.   :298.56                     \n\n\nSee the relationship between different parameters such as Ph vs label or precipitation vs crop:\n\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n\n\n\n\n\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n\n\n\n\nData processing: separate feature and target variable:\n\nfeatures &lt;- data[,1:7]\ntarget &lt;- data$label\n\nSplit the data into training and testing sets:\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(target, p=0.8, list=FALSE)\ntrainData &lt;- features[trainIndex, ]\ntrainLabels &lt;- target[trainIndex]\ntestData &lt;- features[-trainIndex, ]\ntestLabels &lt;- target[-trainIndex]\n\nDefine training control:\n\ntrain_control &lt;- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm &lt;- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb &lt;- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels &lt;- as.factor(trainLabels)\ntestLabels &lt;- as.factor(testLabels)\n\nMake predictions on the test data:\nUsing Random Forest algorithm:\n\npred_rf &lt;- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf &lt;- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric &lt;- as.matrix(cm_rf)\ncm_rf_numeric[] &lt;- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n\n\n\n\nUsing Support Vector Machine algorithm:\n\npred_svm &lt;- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm &lt;- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\nUsing Naive Bayes algorithm:\n\npred_nb &lt;- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb &lt;- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric &lt;- as.matrix(cm_nb)\ncm_nb_numeric[] &lt;- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n\n\nCalculate and print misclassification rates:\n\nmis_rf &lt;- 1 - cm_rf$overall['Accuracy']\nmis_svm &lt;- 1 - cm_svm$overall['Accuracy']\nmis_nb &lt;- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification &lt;- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n          Model Misclassification_Rate\n1 Random Forest            0.004545455\n2           SVM            0.018181818\n3   Naive Bayes            0.004545455"
  },
  {
    "objectID": "posts/Classification_crop  suitability/index.html#summarize-the-results",
    "href": "posts/Classification_crop  suitability/index.html#summarize-the-results",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Summarize the results:",
    "text": "Summarize the results:\nIn this blog, as I mentioned before, we used different algorithms for classification, and based on the accuracy, a suitable algorithm will be used for further prediction. As you can see above, I employed different algorithms and calculated the prediction accuracy for each model. Now, let‚Äôs summarize the results and compare one another.\n\n# Summarize the results\nresults &lt;- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: RF, SVM, NB \nNumber of resamples: 10 \n\nAccuracy \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9886364 0.9943182 0.9943182 0.9954545 0.9985795    1    0\nSVM 0.9659091 0.9730114 0.9829545 0.9829545 0.9928977    1    0\nNB  0.9772727 0.9943182 0.9943182 0.9943182 1.0000000    1    0\n\nKappa \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9880952 0.9940476 0.9940476 0.9952381 0.9985119    1    0\nSVM 0.9642857 0.9717262 0.9821429 0.9821429 0.9925595    1    0\nNB  0.9761905 0.9940476 0.9940476 0.9940476 1.0000000    1    0\n\n\n\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n\n\n\n\nFrom this plot, we can see the prediction accuracy using different algorithms. From this, we can see that the prediction accuracy by random forest is significantly better than that of other algorithms. Thus, based on this, We will select a random forest for further prediction.\nMaking Predictions:\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n# Train the model\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData &lt;- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction &lt;- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\n\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\nConclusion\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "A. Oral Presentations:\n[1] Chhetri, K., Bishop, C., Clark, B., Langston, D., Zeng, Y. (2024). Evaluation of effects of foliar fungicide treatments on Tar spot disease in Virginia. In Graduate seminar, Virginia Tech, VA\n[2] Chhetri, K., Ghimire, D., Gautam, S., Payne, D., and Liedl, B. E. (2022). Challenges in Selecting a Seed Source to Use for Improving Vintage Tomato Varieties. In ASHS, Chicago, IL\n\n\nB. Poster Presentations:\n[1] Chhetri, K., Bishop, C., Clark, B., Langston, D., Zeng, Y. (2024). Evaluation of effects of foliar fungicide treatments and hybrid susceptibility on major corn foliar diseases in Virginia. In Plant Health 2024, Memphis, TN\n[2] Chhetri, K., Bishop, C., Clark, B., Langston, D., Zeng, Y. (2024). Evaluation of effects of foliar fungicide treatments on curvularia leaf spot and gray leaf spot diseases in Virginia. In SPES Symposium, Virginia Tech, VA\n[3] Chhetri, K., Zeng, Y. (2023). Can Corn Foliar Disease Be Outwitted? Fungicide Decision in Corn Production. In TPS Symposium, Virginia Tech, VA\n[4] Chhetri, K., Ghimire, D., Payne, D., and Liedl, B. E. (2022). Would the Real Cherokee Purple Please Stand up? Challenges in Using Vintage Varieties. In PAG30, San Diego, CA\n[5] Ghimire, D., Chhetri, K., Payne, D., and Liedl, B. E. (2022). Roadblocks to Improving Vintage Tomato Varieties. In PAG30, San Diego, CA\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "title: ‚ÄúAbout Me‚Äù date: ‚Äú2025-07-18‚Äù image: ‚Äúimg.jpg‚Äù image_alt: ‚ÄúHeadshot of Kamal Chhetri‚Äù excerpt: ‚ÄúSecond-year PhD student at Virginia Tech specializing in bioinformatics-driven corn disease management.‚Äù categories: - About - Plant Pathology - Bioinformatics tags: - Tar Spot - Metagenomics - Fungicide Efficacy - Corn Disease\nintroduction:\nMy work bridges field experimentation and high-throughput sequencing to deliver actionable insights for corn disease management.\nüå± Research_interests:\n\nEvaluating fungicide efficacies to optimize integrated disease management for corn - Profiling corn leaf endophyte communities and tracking their response to foliar treatments\nDeveloping decision-support tools for Tar Spot disease forecasting and management\nImplementing metagenomic sequencing and custom bioinformatics pipelines for microbial community analysis\n\nSkills_tools:\n\nBioinformatics: - Kaiju for taxonomic classification - KBase workflows with SLURM-based HPC integration - Nanopore (MinION) sequencing and real-time basecalling\nStatistical_analysis: - R (tidyverse, lme4) and Python (pandas, scikit-bio)\nExperimental designs: full factorial, fractional, and incomplete block designs genomic_analysis: - Reference genome alignment (Minimap2) - Metagenomic profiling without de novo assembly - Learning advanced taxonomic profiling techniques\n\nConferences_presentations: - year: 2024 event: American Society for Horticultural Science (ASHS) format: Oral Presentation title: ‚ÄúOptimizing Fungicide Programs for Corn Tar Spot‚Äù - year: 2025 event: Plant and Animal Genome Conference (PAG) format: Poster title: ‚ÄúAirborne Pathogen Diversity in Corn Fields‚Äù - year: 2025 event: American Phytopathological Society (APS) format: Poster title: ‚ÄúEndophyte Community Shifts Following Fungicide Application‚Äù\nAdditional_sections: - section: Education description: ‚ÄúB.Sc. in Plant Biology, Your University, 2022‚Äù - section: Selected Publications description: ‚ÄúList peer-reviewed articles with DOIs‚Äù - section: Funding & Awards description: ‚ÄúFellowships, grants, travel awards‚Äù - section: Teaching & Mentoring description: ‚ÄúCourses assisted, undergraduate mentees‚Äù - section: Outreach & Service description: ‚ÄúWorkshops led, peer-review contributions‚Äù - section: Contact & CV description: ‚ÄúLink to downloadable CV, email, ORCID, Google Scholar profile‚Äù\n\n\n Back to top"
  }
]