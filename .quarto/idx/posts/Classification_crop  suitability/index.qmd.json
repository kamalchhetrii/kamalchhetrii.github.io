{"title":"Predicting Crop Suitability using different machine learning algorithm in R","markdown":{"yaml":{"title":"Predicting Crop Suitability using different machine learning algorithm in R ","author":"Kamal Chhetri","date":"2023-10-28","categories":["R","Code","Analysis"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nMachine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring.\n\n## Random Forest (RF)\n\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees.\n\n## Support Vector Machines (SVM)\n\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It's effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\n## Naive Bayes (NB)\n\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\n\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area's average temperature, humidity, pH level, rainfall, and NPK values.\n\n## The Data\n\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type. [You can find the data file in here](https://github.com/kamalchhetrii/kamalchhetrii.github.io/blob/main/Crop_recommendation.csv).\n\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R's head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R's is.na() function.\n\n## Data Visualization\n\nTo get a better understanding of the data, I visualized it using R's ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns.\n\n## Data Processing\n\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label.\n\n## Model Selection\n\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\n\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\n\n[**Load necessary libraries:**]{.underline}\n\n```{r}\nlibrary(caret)\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\nlibrary(pheatmap)\n\n```\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \n```\n\n[**Load the data:**]{.underline}\n\n```{r}\ndata <- read.csv(\"/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/Crop_recommendation.csv\")\nhead(data)\n```\n\n```{r}\nset.seed(123)\n```\n\n[**Split the data into training and testing sets:**]{.underline}\n\n```{r}\n# See the head and tail of data\nhead(data)\n```\n\n```{r}\n# Check for missing values\nsum(is.na(data))\n```\n\n[**Create a data frame to store test data and predicted labels:**]{.underline}\n\n```{r}\n# See the distribution of data \nsummary(data)\n```\n\n[**See the relationship between different parameters such as Ph vs label or precipitation vs crop:**]{.underline}\n\n```{r}\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n```\n\n```{r}\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n```\n\n[**Data processing: separate feature and target variable:**]{.underline}\n\n```{r}\nfeatures <- data[,1:7]\ntarget <- data$label\n```\n\n[**Split the data into training and testing sets:**]{.underline}\n\n```{r}\nset.seed(123)\ntrainIndex <- createDataPartition(target, p=0.8, list=FALSE)\ntrainData <- features[trainIndex, ]\ntrainLabels <- target[trainIndex]\ntestData <- features[-trainIndex, ]\ntestLabels <- target[-trainIndex]\n```\n\n[**Define training control:**]{.underline}\n\n```{r}\ntrain_control <- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf <- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm <- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb <- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n```\n\n```{r}\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels <- as.factor(trainLabels)\ntestLabels <- as.factor(testLabels)\n```\n\n[**Make predictions on the test data:**]{.underline}\n\n**Using Random Forest algorithm:**\n\n```{r}\npred_rf <- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf <- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric <- as.matrix(cm_rf)\ncm_rf_numeric[] <- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n```\n\n**Using Support Vector Machine algorithm:**\n\n```{r}\npred_svm <- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm <- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\n```\n\n**Using Naive Bayes algorithm:**\n\n```{r}\npred_nb <- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb <- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric <- as.matrix(cm_nb)\ncm_nb_numeric[] <- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n```\n\n```{r}\n\n```\n\n[**Calculate and print misclassification rates:**]{.underline}\n\n```{r}\nmis_rf <- 1 - cm_rf$overall['Accuracy']\nmis_svm <- 1 - cm_svm$overall['Accuracy']\nmis_nb <- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification <- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n```\n\n```{r}\n\n\n```\n\n## Summarize the results:\n\nIn this blog, as I mentioned before, we used different algorithms for classification, and based on the accuracy, a suitable algorithm will be used for further prediction. As you can see above, I employed different algorithms and calculated the prediction accuracy for each model. Now, let's summarize the results and compare one another.\n\n```{r}\n# Summarize the results\nresults <- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n```\n\n```{r}\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n```\n\nFrom this plot, we can see the prediction accuracy using different algorithms. From this, we can see that the prediction accuracy by random forest is significantly better than that of other algorithms. Thus, based on this, We will select a random forest for further prediction.\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n```\n\n[**Making Predictions:**]{.underline}\n\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n```{r}\n# Train the model\nmodel_rf <- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData <- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction <- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n```\n\n```{r}\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n```\n\nConclusion\n\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant.\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nMachine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring.\n\n## Random Forest (RF)\n\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees.\n\n## Support Vector Machines (SVM)\n\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It's effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\n## Naive Bayes (NB)\n\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\n\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area's average temperature, humidity, pH level, rainfall, and NPK values.\n\n## The Data\n\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type. [You can find the data file in here](https://github.com/kamalchhetrii/kamalchhetrii.github.io/blob/main/Crop_recommendation.csv).\n\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R's head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R's is.na() function.\n\n## Data Visualization\n\nTo get a better understanding of the data, I visualized it using R's ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns.\n\n## Data Processing\n\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label.\n\n## Model Selection\n\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\n\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\n\n[**Load necessary libraries:**]{.underline}\n\n```{r}\nlibrary(caret)\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\nlibrary(pheatmap)\n\n```\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(warning = FALSE, message = FALSE) \n```\n\n[**Load the data:**]{.underline}\n\n```{r}\ndata <- read.csv(\"/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/Crop_recommendation.csv\")\nhead(data)\n```\n\n```{r}\nset.seed(123)\n```\n\n[**Split the data into training and testing sets:**]{.underline}\n\n```{r}\n# See the head and tail of data\nhead(data)\n```\n\n```{r}\n# Check for missing values\nsum(is.na(data))\n```\n\n[**Create a data frame to store test data and predicted labels:**]{.underline}\n\n```{r}\n# See the distribution of data \nsummary(data)\n```\n\n[**See the relationship between different parameters such as Ph vs label or precipitation vs crop:**]{.underline}\n\n```{r}\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n```\n\n```{r}\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n```\n\n[**Data processing: separate feature and target variable:**]{.underline}\n\n```{r}\nfeatures <- data[,1:7]\ntarget <- data$label\n```\n\n[**Split the data into training and testing sets:**]{.underline}\n\n```{r}\nset.seed(123)\ntrainIndex <- createDataPartition(target, p=0.8, list=FALSE)\ntrainData <- features[trainIndex, ]\ntrainLabels <- target[trainIndex]\ntestData <- features[-trainIndex, ]\ntestLabels <- target[-trainIndex]\n```\n\n[**Define training control:**]{.underline}\n\n```{r}\ntrain_control <- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf <- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm <- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb <- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n```\n\n```{r}\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels <- as.factor(trainLabels)\ntestLabels <- as.factor(testLabels)\n```\n\n[**Make predictions on the test data:**]{.underline}\n\n**Using Random Forest algorithm:**\n\n```{r}\npred_rf <- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf <- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric <- as.matrix(cm_rf)\ncm_rf_numeric[] <- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n```\n\n**Using Support Vector Machine algorithm:**\n\n```{r}\npred_svm <- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm <- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\n```\n\n**Using Naive Bayes algorithm:**\n\n```{r}\npred_nb <- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb <- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric <- as.matrix(cm_nb)\ncm_nb_numeric[] <- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n```\n\n```{r}\n\n```\n\n[**Calculate and print misclassification rates:**]{.underline}\n\n```{r}\nmis_rf <- 1 - cm_rf$overall['Accuracy']\nmis_svm <- 1 - cm_svm$overall['Accuracy']\nmis_nb <- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification <- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n```\n\n```{r}\n\n\n```\n\n## Summarize the results:\n\nIn this blog, as I mentioned before, we used different algorithms for classification, and based on the accuracy, a suitable algorithm will be used for further prediction. As you can see above, I employed different algorithms and calculated the prediction accuracy for each model. Now, let's summarize the results and compare one another.\n\n```{r}\n# Summarize the results\nresults <- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n```\n\n```{r}\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n```\n\nFrom this plot, we can see the prediction accuracy using different algorithms. From this, we can see that the prediction accuracy by random forest is significantly better than that of other algorithms. Thus, based on this, We will select a random forest for further prediction.\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n```\n\n[**Making Predictions:**]{.underline}\n\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n```{r}\n# Train the model\nmodel_rf <- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData <- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction <- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n```\n\n```{r}\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n```\n\nConclusion\n\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant.\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n\n```{r}\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":true,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"toc-depth":3,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","page-layout":"article","theme":{"light":["cosmo","light.scss"],"dark":["darkly","dark.scss"]},"toc-location":"right","code-copy":true,"smooth-scroll":true,"citations-hover":true,"footnotes-hover":true,"grid":{"sidebar-width":"280px","body-width":"900px","margin-width":"300px"},"mainfont":"Source Sans Pro","fontsize":"1.1em","linestretch":1.6,"title-block-banner":true,"title":"Predicting Crop Suitability using different machine learning algorithm in R ","author":"Kamal Chhetri","date":"2023-10-28","categories":["R","Code","Analysis"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}