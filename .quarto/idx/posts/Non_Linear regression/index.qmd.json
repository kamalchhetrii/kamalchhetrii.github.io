{"title":"Understanding Non-Linear Regression","markdown":{"yaml":{"title":"Understanding Non-Linear Regression","author":"Kamal Chhetri","date":"2023-10-18","categories":["R","Code","Analysis"]},"headingText":"A. Non Linear regression:","containsRefs":false,"markdown":"\n\n\n# Introduction:\n\nNon-linear regression is a form of polynomial regression that models a non-linear relationship between dependent and independent variables. It\\'s used when data shows a curvy trend, and linear regression wouldn\\'t yield accurate results due to its assumption of linearity.\n\nThis method can accommodate various types of regression, such as quadratic, cubic, and so on, to fit the dataset.\n\nUnlike simple linear regression, which uses a straight line to relate two variables (X and Y), non-linear regression captures more complex, curved relationships between these variables.\n\nThe main goal of non-linear regression is to minimize the sum of squared differences between the observed Y values and the predictions made by the non-linear model. This sum of squares serves as a measure of how well the model fits the data points. It\\'s calculated by finding the differences between the fitted non-linear function and each data point\\'s Y value, squaring these differences, and then summing them up. A smaller sum of squared differences indicates a better fit of the model to the data.\n\nNon-linear regression employs various mathematical functions such as logarithmic, trigonometric, exponential, power functions, Lorenz curves, Gaussian functions, and other fitting techniques to capture the underlying relationships in the data. This makes it a versatile tool for modeling complex relationships in data.\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Michaelis-Menten_saturation_curve_of_an_enzyme_reaction.svg/300px-Michaelis-Menten_saturation_curve_of_an_enzyme_reaction.svg.png)\n\nFigure: Non linear regression (Fig credit: [Wikipedia](https://en.wikipedia.org/wiki/Nonlinear_regression))\n\n## Working mechanism:\n\nNon-linear regression models the relationship between a dependent variable and one or more independent variables using a non-linear function. This function is typically a polynomial, exponential, logarithmic, or other non-linear function.\n\nThe goal of non-linear regression is to find the parameters that minimize the difference between the predicted and actual output values. This is often done using iterative optimization algorithms, such as gradient descent or Newton\\'s method.\n\n## Lets understand it by example from each:\n\n1.  **Polynomial Regression (Quadratic relationship)**\n\n```{r}\nlibrary(ggplot2)\n\n\n# Generate some sample data\nset.seed(123)\nx <- seq(-10, 10, by = 0.1)\ny <- x^2 + rnorm(length(x), sd = 10)\n\n# Fit a non-linear regression model\nmodel <- nls(y ~ a * x^2, start = list(a = 1))\n\n# Print the model summary\nprint(summary(model))\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model) * x^2, color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Quadratic Regression\") +\n  theme_minimal()\n\n\n```\n\n### **Exponential Regression**\n\n```{r}\n# Generate some sample data\nset.seed(123)\nx <- seq(0, 10, by = 0.1)\ny <- exp(x) + rnorm(length(x), sd = 10)\n\n# Fit a non-linear regression model\nmodel <- nls(y ~ a * exp(b * x), start = list(a = 1, b = 1))\n\n# Print the model summary\nprint(summary(model))\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model)[1] * exp(coef(model)[2] * x), color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Exponential Regression\") +\n  theme_minimal()\n\n\n\n```\n\n### **Logarithmic Regression**\n\n```{r}\n# Generate some sample data\nset.seed(123)\nx <- seq(1, 10, by = 0.1)\ny <- log(x) + rnorm(length(x), sd = 0.1)\n\n# Fit a non-linear regression model\nmodel <- nls(y ~ a * log(b * x), start = list(a = 1, b = 1))\n\n# Print the model summary\nprint(summary(model))\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model)[1] * log(coef(model)[2] * x), color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Logarithmic Regression\") +\n  theme_minimal()\n\n```\n\n# Lets understand Non- linear regression model by using real data:\n\nNon-linear regression is used when the relationship between the independent and dependent variables is not linear, or when the data is not normally distributed, or involves complex relationships. Non-linear regression can capture complex patterns and interactions and provide impressive results in performance, stability, and precision.\n\n### Data:\n\nYou can access the data [from here](https://www.kaggle.com/datasets/iansurii/china-gdp-dataset). This is the data about the china's GDP per year which is growing in an exponential rate.\n\n```{r}\n\n# Load the package\nlibrary(minpack.lm)\n\ndf <- read.csv(\"/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/china_gdp.csv\")\n```\n\n```{r}\nhead(df)\n```\n\n### Explanatory visualization:\n\n```{r}\n# Define the data\nx_data <- df[[\"Year\"]]\ny_data <- df[[\"Value\"]]\n\n# Create the plot\nggplot(df, aes(x = x_data, y = y_data)) +\n  geom_point(color = \"darkgreen\") +\n  labs(x = \"Year\", y = \"GDP\") +\n  theme_minimal()\n\n```\n\n### Building the model:\n\nThe logistic function appears to be a reasonable approximation based on the plot's first appearance. This is because the logistic function begins slowly, grows more rapidly in the middle, and then declines once more in the conclusion.\n\n![](https://raw.githubusercontent.com/Codecademy/docs/main/media/sigmoid-function.png){width=\"435\"}\n\nFigure source: [**Sigmoid Activation Function**](https://www.codecademy.com/resources/docs/ai/neural-networks/sigmoid-activation-function)\n\n```{r}\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Define the parameters\nbeta_1 <- 0.10\nbeta_2 <- 1990.0\n\n# Apply the logistic function\nY_pred <- sigmoid(x_data, beta_1, beta_2)\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create a dataframe for plotting\ndf <- data.frame(x = x_data, y = Y_pred * 15000000000000., y_actual = y_data)\n\n# Plot the initial prediction against the data points\nggplot(df, aes(x = x)) +\n  geom_line(aes(y = y), color = \"red\") +\n  geom_point(aes(y = y_actual), color = \"darkgreen\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\")\n\n```\n\nNow, we can see the sigmoid and the our actual data plotted in here. We have find the best parameter for our data. We can fit our sigmoid function to the data using curve_fit, which applies non-linear least squares. It is necessary to normalize our x and y variable before proceeding to further data analysis.\n\n```{r}\n# Normalize the data\nxdata <- x_data / max(x_data)\nydata <- y_data / max(y_data)\n\n```\n\n```{r}\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Initial parameter values\nstart <- c(Beta_1 = 1, Beta_2 = 1)\n\n# Non-linear least squares fit\nfit <- nlsLM(ydata ~ sigmoid(xdata, Beta_1, Beta_2), start = start)\n\n# Print the final parameters\nprint(coef(fit))\n```\n\n## Lets visualize the result obtained from model:\n\n```{r}\n# Define the x values\nx <- seq(from = 1960, to = 2015, length.out = 55)\nx <- x / max(x)\n\n# Calculate the y values\ny <- sigmoid(x, coef(fit)[1], coef(fit)[2])\n\n# Create a dataframe for plotting\ndf <- data.frame(x = c(xdata, x), y = c(ydata, y), group = rep(c(\"data\", \"fit\"), each = 55))\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create the plot\nggplot(df, aes(x = x, y = y, color = group)) +\n  geom_point(data = df[df$group == \"data\", ]) +\n  geom_line(data = df[df$group == \"fit\", ], size = 1.5) +\n  scale_color_manual(values = c(\"darkgreen\", \"blue\")) +\n  labs(x = \"Year\", y = \"GDP\", color = \"Legend\") +\n  theme_minimal()\n\n```\n\n```{r}\n\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Split data into train/test\nset.seed(0) # for reproducibility\nmsk <- runif(nrow(df)) < 0.8\ntrain_x <- xdata[msk]\ntest_x <- xdata[!msk]\ntrain_y <- ydata[msk]\ntest_y <- ydata[!msk]\n\n# Initial parameter values\nstart <- c(Beta_1 = 1, Beta_2 = 1)\n\n# Build the model using train set\nfit <- nlsLM(train_y ~ sigmoid(train_x, Beta_1, Beta_2), start = start)\n\n# Print the final parameters\nprint(coef(fit))\n\n# Predict using test set\ny_hat <- predict(fit, list(x = test_x))\n\nprint(y_hat)\n\n\n```\n\n### Evaluate the model we developed earlier:\n\n```{r}\n\n# Observed values\ny_obs <- ydata\n\n# Fitted/predicted values\ny_pred <- sigmoid(xdata, coef(fit)[1], coef(fit)[2])\n\n# Calculate RMSE\nrmse <- sqrt(mean((y_obs - y_pred)^2))\n\n# R-squared\nSS_res <- sum((y_obs - y_pred)^2) \nSS_tot <- sum((y_obs - mean(y_obs))^2)\nrsq <- 1 - SS_res/SS_tot\n\n# Print metrics\ncat(\"RMSE:\", rmse, \"\\n\")\ncat(\"R-squared:\", rsq)\n\n\n\n```\n\n```{r}\n\n```\n","srcMarkdownNoYaml":"\n\n# A. Non Linear regression:\n\n# Introduction:\n\nNon-linear regression is a form of polynomial regression that models a non-linear relationship between dependent and independent variables. It\\'s used when data shows a curvy trend, and linear regression wouldn\\'t yield accurate results due to its assumption of linearity.\n\nThis method can accommodate various types of regression, such as quadratic, cubic, and so on, to fit the dataset.\n\nUnlike simple linear regression, which uses a straight line to relate two variables (X and Y), non-linear regression captures more complex, curved relationships between these variables.\n\nThe main goal of non-linear regression is to minimize the sum of squared differences between the observed Y values and the predictions made by the non-linear model. This sum of squares serves as a measure of how well the model fits the data points. It\\'s calculated by finding the differences between the fitted non-linear function and each data point\\'s Y value, squaring these differences, and then summing them up. A smaller sum of squared differences indicates a better fit of the model to the data.\n\nNon-linear regression employs various mathematical functions such as logarithmic, trigonometric, exponential, power functions, Lorenz curves, Gaussian functions, and other fitting techniques to capture the underlying relationships in the data. This makes it a versatile tool for modeling complex relationships in data.\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Michaelis-Menten_saturation_curve_of_an_enzyme_reaction.svg/300px-Michaelis-Menten_saturation_curve_of_an_enzyme_reaction.svg.png)\n\nFigure: Non linear regression (Fig credit: [Wikipedia](https://en.wikipedia.org/wiki/Nonlinear_regression))\n\n## Working mechanism:\n\nNon-linear regression models the relationship between a dependent variable and one or more independent variables using a non-linear function. This function is typically a polynomial, exponential, logarithmic, or other non-linear function.\n\nThe goal of non-linear regression is to find the parameters that minimize the difference between the predicted and actual output values. This is often done using iterative optimization algorithms, such as gradient descent or Newton\\'s method.\n\n## Lets understand it by example from each:\n\n1.  **Polynomial Regression (Quadratic relationship)**\n\n```{r}\nlibrary(ggplot2)\n\n\n# Generate some sample data\nset.seed(123)\nx <- seq(-10, 10, by = 0.1)\ny <- x^2 + rnorm(length(x), sd = 10)\n\n# Fit a non-linear regression model\nmodel <- nls(y ~ a * x^2, start = list(a = 1))\n\n# Print the model summary\nprint(summary(model))\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model) * x^2, color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Quadratic Regression\") +\n  theme_minimal()\n\n\n```\n\n### **Exponential Regression**\n\n```{r}\n# Generate some sample data\nset.seed(123)\nx <- seq(0, 10, by = 0.1)\ny <- exp(x) + rnorm(length(x), sd = 10)\n\n# Fit a non-linear regression model\nmodel <- nls(y ~ a * exp(b * x), start = list(a = 1, b = 1))\n\n# Print the model summary\nprint(summary(model))\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model)[1] * exp(coef(model)[2] * x), color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Exponential Regression\") +\n  theme_minimal()\n\n\n\n```\n\n### **Logarithmic Regression**\n\n```{r}\n# Generate some sample data\nset.seed(123)\nx <- seq(1, 10, by = 0.1)\ny <- log(x) + rnorm(length(x), sd = 0.1)\n\n# Fit a non-linear regression model\nmodel <- nls(y ~ a * log(b * x), start = list(a = 1, b = 1))\n\n# Print the model summary\nprint(summary(model))\n\n# Plot the data and the fitted model\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  stat_function(fun = function(x) coef(model)[1] * log(coef(model)[2] * x), color = \"red\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\", title = \"Logarithmic Regression\") +\n  theme_minimal()\n\n```\n\n# Lets understand Non- linear regression model by using real data:\n\nNon-linear regression is used when the relationship between the independent and dependent variables is not linear, or when the data is not normally distributed, or involves complex relationships. Non-linear regression can capture complex patterns and interactions and provide impressive results in performance, stability, and precision.\n\n### Data:\n\nYou can access the data [from here](https://www.kaggle.com/datasets/iansurii/china-gdp-dataset). This is the data about the china's GDP per year which is growing in an exponential rate.\n\n```{r}\n\n# Load the package\nlibrary(minpack.lm)\n\ndf <- read.csv(\"/Users/test/Desktop/Kamal/Virginia_Tech_PhD/First semester/Machine_learning/mlblog/kamalchhetrii.github.io/china_gdp.csv\")\n```\n\n```{r}\nhead(df)\n```\n\n### Explanatory visualization:\n\n```{r}\n# Define the data\nx_data <- df[[\"Year\"]]\ny_data <- df[[\"Value\"]]\n\n# Create the plot\nggplot(df, aes(x = x_data, y = y_data)) +\n  geom_point(color = \"darkgreen\") +\n  labs(x = \"Year\", y = \"GDP\") +\n  theme_minimal()\n\n```\n\n### Building the model:\n\nThe logistic function appears to be a reasonable approximation based on the plot's first appearance. This is because the logistic function begins slowly, grows more rapidly in the middle, and then declines once more in the conclusion.\n\n![](https://raw.githubusercontent.com/Codecademy/docs/main/media/sigmoid-function.png){width=\"435\"}\n\nFigure source: [**Sigmoid Activation Function**](https://www.codecademy.com/resources/docs/ai/neural-networks/sigmoid-activation-function)\n\n```{r}\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Define the parameters\nbeta_1 <- 0.10\nbeta_2 <- 1990.0\n\n# Apply the logistic function\nY_pred <- sigmoid(x_data, beta_1, beta_2)\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create a dataframe for plotting\ndf <- data.frame(x = x_data, y = Y_pred * 15000000000000., y_actual = y_data)\n\n# Plot the initial prediction against the data points\nggplot(df, aes(x = x)) +\n  geom_line(aes(y = y), color = \"red\") +\n  geom_point(aes(y = y_actual), color = \"darkgreen\") +\n  labs(x = \"Independent Variable\", y = \"Dependent Variable\")\n\n```\n\nNow, we can see the sigmoid and the our actual data plotted in here. We have find the best parameter for our data. We can fit our sigmoid function to the data using curve_fit, which applies non-linear least squares. It is necessary to normalize our x and y variable before proceeding to further data analysis.\n\n```{r}\n# Normalize the data\nxdata <- x_data / max(x_data)\nydata <- y_data / max(y_data)\n\n```\n\n```{r}\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Initial parameter values\nstart <- c(Beta_1 = 1, Beta_2 = 1)\n\n# Non-linear least squares fit\nfit <- nlsLM(ydata ~ sigmoid(xdata, Beta_1, Beta_2), start = start)\n\n# Print the final parameters\nprint(coef(fit))\n```\n\n## Lets visualize the result obtained from model:\n\n```{r}\n# Define the x values\nx <- seq(from = 1960, to = 2015, length.out = 55)\nx <- x / max(x)\n\n# Calculate the y values\ny <- sigmoid(x, coef(fit)[1], coef(fit)[2])\n\n# Create a dataframe for plotting\ndf <- data.frame(x = c(xdata, x), y = c(ydata, y), group = rep(c(\"data\", \"fit\"), each = 55))\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create the plot\nggplot(df, aes(x = x, y = y, color = group)) +\n  geom_point(data = df[df$group == \"data\", ]) +\n  geom_line(data = df[df$group == \"fit\", ], size = 1.5) +\n  scale_color_manual(values = c(\"darkgreen\", \"blue\")) +\n  labs(x = \"Year\", y = \"GDP\", color = \"Legend\") +\n  theme_minimal()\n\n```\n\n```{r}\n\n# Define the sigmoid function\nsigmoid <- function(x, Beta_1, Beta_2) {\n  y <- 1 / (1 + exp(-Beta_1*(x-Beta_2)))\n  return(y)\n}\n\n# Split data into train/test\nset.seed(0) # for reproducibility\nmsk <- runif(nrow(df)) < 0.8\ntrain_x <- xdata[msk]\ntest_x <- xdata[!msk]\ntrain_y <- ydata[msk]\ntest_y <- ydata[!msk]\n\n# Initial parameter values\nstart <- c(Beta_1 = 1, Beta_2 = 1)\n\n# Build the model using train set\nfit <- nlsLM(train_y ~ sigmoid(train_x, Beta_1, Beta_2), start = start)\n\n# Print the final parameters\nprint(coef(fit))\n\n# Predict using test set\ny_hat <- predict(fit, list(x = test_x))\n\nprint(y_hat)\n\n\n```\n\n### Evaluate the model we developed earlier:\n\n```{r}\n\n# Observed values\ny_obs <- ydata\n\n# Fitted/predicted values\ny_pred <- sigmoid(xdata, coef(fit)[1], coef(fit)[2])\n\n# Calculate RMSE\nrmse <- sqrt(mean((y_obs - y_pred)^2))\n\n# R-squared\nSS_res <- sum((y_obs - y_pred)^2) \nSS_tot <- sum((y_obs - mean(y_obs))^2)\nrsq <- 1 - SS_res/SS_tot\n\n# Print metrics\ncat(\"RMSE:\", rmse, \"\\n\")\ncat(\"R-squared:\", rsq)\n\n\n\n```\n\n```{r}\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","page-layout":"full","theme":{"light":["flatly","light.scss"],"dark":["darkly","dark.scss"]},"grid":{"sidebar-width":"220px","body-width":"1600px","margin-width":"250px"},"title-block-banner":true,"title":"Understanding Non-Linear Regression","author":"Kamal Chhetri","date":"2023-10-18","categories":["R","Code","Analysis"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}