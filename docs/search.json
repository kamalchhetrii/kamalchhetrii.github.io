[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Kamal Chhetri, a PhD student in Plant Pathology at Virginia Tech University. My journey in the field of science began with a Master’s degree in Biotechnology from West Virginia State University.\nOriginally from Nepal, I’ve always been fascinated by the intricate workings of plant life and the ecosystem. This fascination led me to pursue a career in Plant Pathology, where I’m currently delving into the complex interactions between plants and their environment.\nThrough this blog, I aim to share machine learning and its application to the agriculture field. Enjoy my blog\n\n\n Back to top"
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html",
    "href": "posts/Classification_crop  suitability/crop.html",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#introduction",
    "href": "posts/Classification_crop  suitability/crop.html#introduction",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#random-forest-rf",
    "href": "posts/Classification_crop  suitability/crop.html#random-forest-rf",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Random Forest (RF)",
    "text": "Random Forest (RF)\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#support-vector-machines-svm",
    "href": "posts/Classification_crop  suitability/crop.html#support-vector-machines-svm",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It’s effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#naive-bayes-nb",
    "href": "posts/Classification_crop  suitability/crop.html#naive-bayes-nb",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Naive Bayes (NB)",
    "text": "Naive Bayes (NB)\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area’s average temperature, humidity, pH level, rainfall, and NPK values."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#the-data",
    "href": "posts/Classification_crop  suitability/crop.html#the-data",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "The Data",
    "text": "The Data\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type.\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R’s head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R’s is.na() function."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#data-visualization",
    "href": "posts/Classification_crop  suitability/crop.html#data-visualization",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo get a better understanding of the data, I visualized it using R’s ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#data-processing",
    "href": "posts/Classification_crop  suitability/crop.html#data-processing",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Processing",
    "text": "Data Processing\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#model-selection",
    "href": "posts/Classification_crop  suitability/crop.html#model-selection",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Model Selection",
    "text": "Model Selection\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\nLoad necessary libraries:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pheatmap)\n\nLoad the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/Crop_recommendation.csv\")\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\nset.seed(123)\n\nSplit the data into training and testing sets:\n\n# See the head and tail of data\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n\nCreate a data frame to store test data and predicted labels:\n\n# See the distribution of data \nsummary(data)\n\n       N                P                K           temperature    \n Min.   :  0.00   Min.   :  5.00   Min.   :  5.00   Min.   : 8.826  \n 1st Qu.: 21.00   1st Qu.: 28.00   1st Qu.: 20.00   1st Qu.:22.769  \n Median : 37.00   Median : 51.00   Median : 32.00   Median :25.599  \n Mean   : 50.55   Mean   : 53.36   Mean   : 48.15   Mean   :25.616  \n 3rd Qu.: 84.25   3rd Qu.: 68.00   3rd Qu.: 49.00   3rd Qu.:28.562  \n Max.   :140.00   Max.   :145.00   Max.   :205.00   Max.   :43.675  \n    humidity           ph           rainfall         label          \n Min.   :14.26   Min.   :3.505   Min.   : 20.21   Length:2200       \n 1st Qu.:60.26   1st Qu.:5.972   1st Qu.: 64.55   Class :character  \n Median :80.47   Median :6.425   Median : 94.87   Mode  :character  \n Mean   :71.48   Mean   :6.469   Mean   :103.46                     \n 3rd Qu.:89.95   3rd Qu.:6.924   3rd Qu.:124.27                     \n Max.   :99.98   Max.   :9.935   Max.   :298.56                     \n\n\nSee the relationship between different parameters such as Ph vs label or precipitation vs crop:\n\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n\n\n\n\n\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n\n\n\n\nData processing: separate feature and target variable:\n\nfeatures &lt;- data[,1:7]\ntarget &lt;- data$label\n\nSplit the data into training and testing sets:\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(target, p=0.8, list=FALSE)\ntrainData &lt;- features[trainIndex, ]\ntrainLabels &lt;- target[trainIndex]\ntestData &lt;- features[-trainIndex, ]\ntestLabels &lt;- target[-trainIndex]\n\nDefine training control:\n\ntrain_control &lt;- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm &lt;- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb &lt;- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels &lt;- as.factor(trainLabels)\ntestLabels &lt;- as.factor(testLabels)\n\nMake predictions on the test data:\nUsing Random Forest:\n\npred_rf &lt;- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf &lt;- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric &lt;- as.matrix(cm_rf)\ncm_rf_numeric[] &lt;- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n\n\n\n\nFor SVM\n\npred_svm &lt;- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm &lt;- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\nFor NB\n\npred_nb &lt;- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb &lt;- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric &lt;- as.matrix(cm_nb)\ncm_nb_numeric[] &lt;- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n\n\nCalculate and print misclassification rates:\n\nmis_rf &lt;- 1 - cm_rf$overall['Accuracy']\nmis_svm &lt;- 1 - cm_svm$overall['Accuracy']\nmis_nb &lt;- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification &lt;- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n          Model Misclassification_Rate\n1 Random Forest            0.004545455\n2           SVM            0.018181818\n3   Naive Bayes            0.004545455\n\n\n\n# Summarize the results\nresults &lt;- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: RF, SVM, NB \nNumber of resamples: 10 \n\nAccuracy \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9886364 0.9943182 0.9943182 0.9954545 0.9985795    1    0\nSVM 0.9659091 0.9730114 0.9829545 0.9829545 0.9928977    1    0\nNB  0.9772727 0.9943182 0.9943182 0.9943182 1.0000000    1    0\n\nKappa \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9880952 0.9940476 0.9940476 0.9952381 0.9985119    1    0\nSVM 0.9642857 0.9717262 0.9821429 0.9821429 0.9925595    1    0\nNB  0.9761905 0.9940476 0.9940476 0.9940476 1.0000000    1    0\n\n\n\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n\n\n\n\nMaking Predictions:\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n# Train the model\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData &lt;- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction &lt;- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\n\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\nConclusion\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Clustering/clustering.html",
    "href": "posts/Clustering/clustering.html",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data’s density.\n\n\n\nDBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least minPts within a radius of eps from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset.\n\n\n\nDBSCAN has several advantages over other clustering algorithms:\n\nNo need to specify the number of clusters: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\nAbility to find arbitrarily shaped clusters: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\nRobustness to noise: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria.\n\n\n\n\nDespite its advantages, DBSCAN also has some limitations:\n\nDifficulty handling varying densities: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single eps and minPts value may not be suitable for all clusters.\nSensitivity to parameter settings: The results of DBSCAN can be significantly affected by the settings of eps and minPts. Choosing appropriate values for these parameters can be challenging.\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\nLoad necessary libraries\n\nlibrary(fpc) \n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nThe dataset contains information about individuals such as their age, sex, BMI, number of children, smoking status, region, and charges.\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ..."
  },
  {
    "objectID": "posts/Clustering/clustering.html#introduction",
    "href": "posts/Clustering/clustering.html#introduction",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data’s density."
  },
  {
    "objectID": "posts/Clustering/clustering.html#how-dbscan-works",
    "href": "posts/Clustering/clustering.html#how-dbscan-works",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "DBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least minPts within a radius of eps from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset."
  },
  {
    "objectID": "posts/Clustering/clustering.html#advantages-of-dbscan",
    "href": "posts/Clustering/clustering.html#advantages-of-dbscan",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "DBSCAN has several advantages over other clustering algorithms:\n\nNo need to specify the number of clusters: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\nAbility to find arbitrarily shaped clusters: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\nRobustness to noise: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria."
  },
  {
    "objectID": "posts/Clustering/clustering.html#disadvantages-of-dbscan",
    "href": "posts/Clustering/clustering.html#disadvantages-of-dbscan",
    "title": "DNSCAN Clustering",
    "section": "",
    "text": "Despite its advantages, DBSCAN also has some limitations:\n\nDifficulty handling varying densities: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single eps and minPts value may not be suitable for all clusters.\nSensitivity to parameter settings: The results of DBSCAN can be significantly affected by the settings of eps and minPts. Choosing appropriate values for these parameters can be challenging.\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R.\nLoad necessary libraries\n\nlibrary(fpc) \n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nThe dataset contains information about individuals such as their age, sex, BMI, number of children, smoking status, region, and charges.\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ..."
  },
  {
    "objectID": "posts/Clustering/clustering.html#remove-label-form-dataset",
    "href": "posts/Clustering/clustering.html#remove-label-form-dataset",
    "title": "DNSCAN Clustering",
    "section": "Remove label form dataset",
    "text": "Remove label form dataset\n\nmed &lt;- data_num[-4] \n\nFitting DBSCAN clustering model:\n\nset.seed(0)  # Setting seed \nDbscan_cl &lt;- dbscan(med, eps = 0.45, MinPts = 5) \nDbscan_cl\n\ndbscan Pts=1338 MinPts=5 eps=0.45\n          0 1 2 3 4 5\nborder 1310 5 2 4 3 4\nseed      0 1 5 1 1 2\ntotal  1310 6 7 5 4 6\n\n\nIn this code, eps is the maximum distance between two samples for them to be considered as in the same neighborhood, and MinPts is the number of samples in a neighborhood for a point to be considered as a core point."
  },
  {
    "objectID": "posts/linear regression/lr.html",
    "href": "posts/linear regression/lr.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nLinear regression is a powerful statistical method that allows us to examine the relationship between two (simple linear regression) or more (multiple linear regression) variables. A key feature of linear regression is its simplicity and interpretability. It’s used in various fields, including machine learning, most medical fields, and social sciences.\nLinear regression models the relationship between two variables by fitting a linear equation to observed data. The steps to perform multiple linear regression are almost identical to those of simple linear regression.\n\nTypes of regression:\na. Simple linear regression: In this case, we use only one input variable\nb. Multiple linear regression: In this case, We use multiple input variable\n\n\nThe Dataset\nI will update about it soon.\n\n\nAssumptions:\nThere are several assumption for the linear regression model that I am using in this blog. They are explained below, lets take a look.\nLinearity: The relationship between the independent and dependent variables is linear. This assumption can be checked by plotting the variables and examining the data scatter.\nIndependence: The residuals (the differences between the observed and predicted values) are independent. In other words, there is not a specific pattern in the residuals.\nHomoscedasticity: The variance of the residuals is constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all predicted values.\nNormality: The residuals are normally distributed. If this assumption is violated, then the confidence intervals may not be accurate.\nAbsence of multicollinearity: In the case of multiple linear regression, the independent variables should not be too highly correlated with each other.\nLoad necessary libraries:\nSet the random seed for reproducibility:\n\n# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(reshape2)\n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\nLoad and clean the data:\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n# Handle missing values (if any)\n# data &lt;- na.omit(data)  # drops rows with missing values\n# data &lt;- data[complete.cases(data), ]  # drops rows with missing values\n# data$column &lt;- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n\nSince, we don’t have any null or missing value in this data as indicated above. So, no further handling of missing data is needed.\nVisualize the data: for better understanding of the data before proceeding further\n\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets take a look for the correlation between these two features: Our data contains one column “Species” which is non-numeric. Thus, we will firstly subset the data that doesnot contains “Species” column.\n\n# Subset the data to include only numeric columns\ndata_numeric &lt;- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n\n\nCreate the heatmap for better visualization:\n\n# Melt the correlation matrix into a long format\ndata_melt &lt;- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n\n\n\n\nFrom the heatmap created using the above code, we can see there is no strong correlation between different variables. However, this part is made to visualize the data only.\nBuilding the model: From the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\n\n# Create a violin plot for 'age'\nggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellow\") +\n  labs(title=\"Distribution of Charges by Sex\", x=\"Sex\", y=\"Charges\")\n\n\n\n# Create a violin plot for 'charges' vs 'sex'\np1 &lt;- ggplot(data, aes(x=sex, y=charges)) +\n  geom_violin(fill=\"yellowgreen\") +\n  labs(title=\"Violin plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a violin plot for 'charges' vs 'smoker'\np2 &lt;- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_violin(fill=\"goldenrod1\") +\n  labs(title=\"Violin plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n\n\nprint(p2)\n\n\n\n\nAccording to the left plot, the average insurance cost for men and women is roughly the same at $5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about $5,000. The minimum insurance premium for smokers is $5,000.\n\nNeed to check again on these\n\n# Split the data into training and test sets\nset.seed(0)\nsampleSplit &lt;- sample.split(Y=data$bmi, SplitRatio=0.7)\ntrainSet &lt;- subset(x=data, sampleSplit==TRUE)\ntestSet &lt;- subset(x=data, sampleSplit==FALSE)\n\n\n# Fit the model\nmodel &lt;- lm(formula=bmi ~ ., data=trainSet)\n\nBy using a linear combination of all other attributes, we hope to predict the bmi attribute. In this case, we don’t have to worry about the categorical attributes. R automatically manages the categorical attributes.\nLets see the summary of model:\n\n# Split the data into training and test sets\nsummary(model)\n\n\nCall:\nlm(formula = bmi ~ ., data = trainSet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.9234  -3.7146   0.0124   3.4208  20.9629 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.766e+01  6.883e-01  40.181  &lt; 2e-16 ***\nage             -2.573e-02  1.521e-02  -1.692   0.0910 .  \nsexmale          7.230e-01  3.679e-01   1.965   0.0497 *  \nchildren        -2.269e-02  1.504e-01  -0.151   0.8801    \nsmokeryes       -6.980e+00  7.894e-01  -8.841  &lt; 2e-16 ***\nregionnorthwest -2.051e-01  5.294e-01  -0.387   0.6986    \nregionsoutheast  3.923e+00  5.169e-01   7.589 7.58e-14 ***\nregionsouthwest  1.379e+00  5.307e-01   2.599   0.0095 ** \ncharges          2.803e-04  2.771e-05  10.119  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.73 on 967 degrees of freedom\nMultiple R-squared:  0.181, Adjusted R-squared:  0.1742 \nF-statistic: 26.71 on 8 and 967 DF,  p-value: &lt; 2.2e-16\n\n\nThe P-values that are shown in the Pr(&gt;|t|) column are the most intriguing aspect of this situation. The probability significance column Pr(&gt;|t|) in the above table is a crucial factor to take into account because it shows how important probability is for prediction. Here, we took a significance level of 5%  for the calculation.\n\nmodelResiduals &lt;- as.data.frame(residuals(model))\nggplot(modelResiduals, aes(residuals(model)))+\n  geom_histogram(fill = 'mediumorchid1', color = 'black')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe distribution of the residual data is approximately normally distributed.\nMake the prediction:\n\npreds &lt;- predict(model, testSet)\n\nNow, see the further details by creating a table for for actual and prediction values from the above created model.\n\nmodelEval &lt;- cbind(testSet$bmi, preds)\ncolnames(modelEval) &lt;- c('Acual', 'Predicted')\nmodelEval &lt;- as.data.frame(modelEval)\nhead(modelEval)\n\n    Acual Predicted\n4  22.705  33.49025\n14 39.820  33.24960\n16 24.600  29.76384\n23 34.100  32.15970\n24 31.920  30.35069\n27 23.085  30.08882\n\n\n\n\n\nNow lets evaluate the prediction:\nModel evaluation: After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse).\nLets learn little bit about rmse: The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\nDuring model evaluation, RMSE serves as a measure to understand the model’s performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\n\n# Calculate error metrics\nmse &lt;- mean((modelEval$Acual - modelEval$Predicted)^2)\nrmse &lt;- sqrt(mse)\nprint(rmse)\n\n[1] 4.962662\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Machine Learning blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Crop Suitability with Machine Learning using R\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nDNSCAN Clustering\n\n\n\n\n\n\n\nNews\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]