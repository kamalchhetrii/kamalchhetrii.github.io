[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! I’m Kamal Chhetri, a PhD student in Plant Pathology at Virginia Tech University. My journey in the field of science began with a Master’s degree in Biotechnology from West Virginia State University.\nOriginally from Nepal, I’ve always been fascinated by the intricate workings of plant life and the ecosystem. This fascination led me to pursue a career in Plant Pathology, where I’m currently delving into the complex interactions between plants and their environment.\nThrough this blog, I aim to share machine learning and its application to the agriculture field. Enjoy my blog\n\n\n Back to top"
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html",
    "href": "posts/Classification_crop  suitability/crop.html",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#introduction",
    "href": "posts/Classification_crop  suitability/crop.html#introduction",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "",
    "text": "Machine learning is a rapidly evolving field that is generating intense interest because of its increasing applications in businesses and scientific research. Three popular machine learning algorithms are Random Forest (RF), Support Vector Machines (SVM), and Naive Bayes (NB). These algorithms have found applications in various fields, including, but not limited to, medical diagnosis, spam filtering, image and speech recognition, and credit scoring."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#random-forest-rf",
    "href": "posts/Classification_crop  suitability/crop.html#random-forest-rf",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Random Forest (RF)",
    "text": "Random Forest (RF)\nRandom Forest is a widely used machine-learning algorithm developed by Leo Breiman and Adele Cutler. It combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems. The main idea behind RF is to construct a multitude of decision trees at training time and output the class, that is, the mode of the classes for classification or mean prediction of the individual trees for regression. Random forests generally outperform decision trees, but their accuracy is lower than gradient-boosted trees."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#support-vector-machines-svm",
    "href": "posts/Classification_crop  suitability/crop.html#support-vector-machines-svm",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Support Vector Machines (SVM)",
    "text": "Support Vector Machines (SVM)\nSupport Vector Machines (SVMs) are a set of supervised learning methods used for classification, regression, and outliers detection. Developed at AT&T Bell Laboratories by Vladimir Vapnik and colleagues, SVMs are one of the most robust prediction methods. SVM works by finding a hyperplane in a high-dimensional space that best separates data into different classes. It’s effective in high dimensional spaces and still effective in cases where the number of dimensions is greater than the number of samples. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#naive-bayes-nb",
    "href": "posts/Classification_crop  suitability/crop.html#naive-bayes-nb",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Naive Bayes (NB)",
    "text": "Naive Bayes (NB)\nNaive Bayes is a probabilistic machine learning algorithm that can be used in several classification tasks. Typical applications of Naive Bayes are the classification of documents, filtering spam, prediction, and so on. This algorithm is based on the discoveries of Thomas Bayes and, hence its name. The Naïve Bayes classifier is a supervised machine learning algorithm that is used for classification tasks like text classification. It is also part of a family of generative learning algorithms, meaning that it seeks to model the distribution of inputs of a given class or category.\nIn this project, I used machine learning to predict the suitability of different crops for a specific area. The area in question is Blackstone, VA, and the goal was to determine which crop would be most suitable given the area’s average temperature, humidity, pH level, rainfall, and NPK values."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#the-data",
    "href": "posts/Classification_crop  suitability/crop.html#the-data",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "The Data",
    "text": "The Data\nThe data used in this project was stored in a CSV file named Crop_recommendation.csv. This file contained various parameters for different crops, including N, P, K, temperature, humidity, pH, rainfall, and a label indicating the crop type. You can find the data file in here.\nBefore diving into the machine learning aspect of the project, I first explored the data. I used R’s head() and tail() functions to view the first and last few rows of the data. I also checked for missing or null values using R’s is.na() function."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#data-visualization",
    "href": "posts/Classification_crop  suitability/crop.html#data-visualization",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo get a better understanding of the data, I visualized it using R’s ggplot2 library. I created scatter plots to examine the relationship between different parameters and the crop label. For example, I plotted pH vs label and rainfall vs crop to see if there were any noticeable trends or patterns."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#data-processing",
    "href": "posts/Classification_crop  suitability/crop.html#data-processing",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Data Processing",
    "text": "Data Processing\nNext, I processed the data by separating it into feature and target variables. The feature variables included N, P, K, temperature, humidity, pH, and rainfall. The target variable was the crop label."
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#model-selection",
    "href": "posts/Classification_crop  suitability/crop.html#model-selection",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Model Selection",
    "text": "Model Selection\nI used three different machine learning algorithms for this project: Random Forest (RF), Support Vector Machine (SVM), and Naive Bayes (NB). I trained each model on the training data and then evaluated their performance based on their accuracy.\nTo visualize the accuracy of each model, I created a line plot using ggplot2. This plot showed that Random Forest had the highest accuracy of prediction.\nLoad necessary libraries:\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\nlibrary(e1071)\nlibrary(ggplot2)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(pheatmap)\n\nLoad the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/Crop_recommendation.csv\")\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\nset.seed(123)\n\nSplit the data into training and testing sets:\n\n# See the head and tail of data\nhead(data)\n\n   N  P  K temperature humidity       ph rainfall label\n1 90 42 43    20.87974 82.00274 6.502985 202.9355  rice\n2 85 58 41    21.77046 80.31964 7.038096 226.6555  rice\n3 60 55 44    23.00446 82.32076 7.840207 263.9642  rice\n4 74 35 40    26.49110 80.15836 6.980401 242.8640  rice\n5 78 42 42    20.13017 81.60487 7.628473 262.7173  rice\n6 69 37 42    23.05805 83.37012 7.073454 251.0550  rice\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n\nCreate a data frame to store test data and predicted labels:\n\n# See the distribution of data \nsummary(data)\n\n       N                P                K           temperature    \n Min.   :  0.00   Min.   :  5.00   Min.   :  5.00   Min.   : 8.826  \n 1st Qu.: 21.00   1st Qu.: 28.00   1st Qu.: 20.00   1st Qu.:22.769  \n Median : 37.00   Median : 51.00   Median : 32.00   Median :25.599  \n Mean   : 50.55   Mean   : 53.36   Mean   : 48.15   Mean   :25.616  \n 3rd Qu.: 84.25   3rd Qu.: 68.00   3rd Qu.: 49.00   3rd Qu.:28.562  \n Max.   :140.00   Max.   :145.00   Max.   :205.00   Max.   :43.675  \n    humidity           ph           rainfall         label          \n Min.   :14.26   Min.   :3.505   Min.   : 20.21   Length:2200       \n 1st Qu.:60.26   1st Qu.:5.972   1st Qu.: 64.55   Class :character  \n Median :80.47   Median :6.425   Median : 94.87   Mode  :character  \n Mean   :71.48   Mean   :6.469   Mean   :103.46                     \n 3rd Qu.:89.95   3rd Qu.:6.924   3rd Qu.:124.27                     \n Max.   :99.98   Max.   :9.935   Max.   :298.56                     \n\n\nSee the relationship between different parameters such as Ph vs label or precipitation vs crop:\n\nggplot(data, aes(x=ph, y=label, color=label)) + geom_point()\n\n\n\n\n\nggplot(data, aes(x=rainfall, y=label, color=label)) + geom_point()\n\n\n\n\nData processing: separate feature and target variable:\n\nfeatures &lt;- data[,1:7]\ntarget &lt;- data$label\n\nSplit the data into training and testing sets:\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(target, p=0.8, list=FALSE)\ntrainData &lt;- features[trainIndex, ]\ntrainLabels &lt;- target[trainIndex]\ntestData &lt;- features[-trainIndex, ]\ntestLabels &lt;- target[-trainIndex]\n\nDefine training control:\n\ntrain_control &lt;- trainControl(method=\"cv\", number=10)\n\n# Train the models\nset.seed(123)\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\")\nmodel_svm &lt;- train(trainData, trainLabels, trControl=train_control, method=\"svmRadial\")\nmodel_nb &lt;- train(trainData, trainLabels, trControl=train_control, method=\"naive_bayes\")\n\n\n# Ensure 'label' is a factor in both training and testing sets\ntrainLabels &lt;- as.factor(trainLabels)\ntestLabels &lt;- as.factor(testLabels)\n\nMake predictions on the test data:\nUsing Random Forest algorithm:\n\npred_rf &lt;- predict(model_rf, testData)\n\n# Generate confusion matrices\ncm_rf &lt;- confusionMatrix(pred_rf, testLabels)\n\n# Convert it to the numeric form and visualize using the heatmap\ncm_rf_numeric &lt;- as.matrix(cm_rf)\ncm_rf_numeric[] &lt;- as.numeric(cm_rf_numeric)\npheatmap(cm_rf_numeric, display_numbers = T)\n\n\n\n\nUsing Support Vector Machine algorithm:\n\npred_svm &lt;- predict(model_svm, testData)\n# Generate confusion matrices\ncm_svm &lt;- confusionMatrix(pred_svm, testLabels)\n# Print confusion matrices\n\nUsing Naive Bayes algorithm:\n\npred_nb &lt;- predict(model_nb, testData)\n# Generate confusion matrices\ncm_nb &lt;- confusionMatrix(pred_nb, testLabels)\n\n\n# Convert it to a numeric matrix and creating heatmap based on this:\ncm_nb_numeric &lt;- as.matrix(cm_nb)\ncm_nb_numeric[] &lt;- as.numeric(cm_nb_numeric)\npheatmap(cm_nb_numeric, display_numbers = T)\n\n\n\n\nCalculate and print misclassification rates:\n\nmis_rf &lt;- 1 - cm_rf$overall['Accuracy']\nmis_svm &lt;- 1 - cm_svm$overall['Accuracy']\nmis_nb &lt;- 1 - cm_nb$overall['Accuracy']\n\n\n# Create a data frame to store the misclassification rates\nmisclassification &lt;- data.frame(\n  Model = c(\"Random Forest\", \"SVM\", \"Naive Bayes\"),\n  Misclassification_Rate = c(mis_rf, mis_svm, mis_nb)\n)\n\n# Print the misclassification rates\nprint(misclassification)\n\n          Model Misclassification_Rate\n1 Random Forest            0.004545455\n2           SVM            0.018181818\n3   Naive Bayes            0.004545455"
  },
  {
    "objectID": "posts/Classification_crop  suitability/crop.html#summarize-the-results",
    "href": "posts/Classification_crop  suitability/crop.html#summarize-the-results",
    "title": "Predicting Crop Suitability with Machine Learning using R",
    "section": "Summarize the results:",
    "text": "Summarize the results:\nIn this blog, as I mentioned before, we used different algorithms for classification, and based on the accuracy, a suitable algorithm will be used for further prediction. As you can see above, I employed different algorithms and calculated the prediction accuracy for each model. Now, let’s summarize the results and compare one another.\n\n# Summarize the results\nresults &lt;- resamples(list(RF=model_rf, SVM=model_svm, NB=model_nb))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: RF, SVM, NB \nNumber of resamples: 10 \n\nAccuracy \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9886364 0.9943182 0.9943182 0.9954545 0.9985795    1    0\nSVM 0.9659091 0.9730114 0.9829545 0.9829545 0.9928977    1    0\nNB  0.9772727 0.9943182 0.9943182 0.9943182 1.0000000    1    0\n\nKappa \n         Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\nRF  0.9880952 0.9940476 0.9940476 0.9952381 0.9985119    1    0\nSVM 0.9642857 0.9717262 0.9821429 0.9821429 0.9925595    1    0\nNB  0.9761905 0.9940476 0.9940476 0.9940476 1.0000000    1    0\n\n\n\n# Plot the results\ndotplot(results, metric = \"Accuracy\")\n\n\n\n\nFrom this plot, we can see the prediction accuracy using different algorithms. From this, we can see that the prediction accuracy by random forest is significantly better than that of other algorithms. Thus, based on this, We will select a random forest for further prediction.\nMaking Predictions:\nFinally, I used the best model (Random Forest) to make predictions. Given the average temperature (21.7 - 31.7 C), average humidity (71), pH level (5), rainfall (118 cm), and NPK values (89, 41, 22) of Blackstone Area, VA, the model predicted that Maize would be the most suitable crop for this area.\n\n# Train the model\nmodel_rf &lt;- train(trainData, trainLabels, trControl=train_control, method=\"rf\", ntree=100)\n\nnewData &lt;- data.frame(N=89, P=41, K=22, temperature=26.7, humidity=71, ph=5, rainfall=118)\nprediction &lt;- predict(model_rf, newData)\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\n\nprint(paste(\"The suitable crop for Blackstone Area is:\", prediction))\n\n[1] \"The suitable crop for Blackstone Area is: maize\"\n\n\nConclusion\nThis project demonstrated how machine learning can be used to predict crop suitability based on various environmental parameters. While this was just one example, this approach could be applied to any area with known parameters to help farmers make informed decisions about which crops to plant."
  },
  {
    "objectID": "posts/linear regression/lr.html",
    "href": "posts/linear regression/lr.html",
    "title": "Linear Regression and medical charges",
    "section": "",
    "text": "Linear regression is a powerful statistical method that allows us to examine the relationship between two (simple linear regression) or more (multiple linear regression) variables. A key feature of linear regression is its simplicity and interpretability. It’s used in various fields, including machine learning, most medical fields, and social sciences.\nLinear regression models the relationship between two variables by fitting a linear equation to observed data. The steps to perform multiple linear regression are almost identical to those of simple linear regression.\n\nTypes of regression:\na. Simple linear regression: In this case, we use only one input variable\nb. Multiple linear regression: In this case, We use multiple input variable\n\n\nThere are seven columns and 1338 rows in the data, which indicates that there are seven separate variables. The remaining six variables—age, sex, bmi, children, smoker, and region—are independent variables, whereas charges is the target variable in this instance. To examine the data, multiple linear regression must be fitted because there are several independent variables. You can access this data from here.\n\n\n\nThere are several assumption for the linear regression model that I am using in this blog. They are explained below, lets take a look.\nLinearity: The relationship between the independent and dependent variables is linear. This assumption can be checked by plotting the variables and examining the data scatter.\nIndependence: The residuals (the differences between the observed and predicted values) are independent. In other words, there is not a specific pattern in the residuals.\nHomoscedasticity: The variance of the residuals is constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all predicted values.\nNormality: The residuals are normally distributed. If this assumption is violated, then the confidence intervals may not be accurate.\nAbsence of multicollinearity: In the case of multiple linear regression, the independent variables should not be too highly correlated with each other.\n\n\n\n\n# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\nlibrary(reshape2)\nlibrary(Metrics)\n\n\n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ...\n\n\n\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n# Handle missing values (if any)\n# data &lt;- na.omit(data)  # drops rows with missing values\n# data &lt;- data[complete.cases(data), ]  # drops rows with missing values\n# data$column &lt;- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n\nIn my case, the data does not contain the null values; however, if you select the data with the null values and want to analyze the data, it is important to deal with the null values before proceeding further analysis. Different ways to deal with the null values in R is given in the code above.\nUpon executing the above code, we got the value 0, which means there are no missing values in the data. Since we don’t have any null or missing values in this data, as indicated above. So, no further handling of missing data is needed.\n\n\n\n\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets take a look for the correlation between these two features: Our data contains one column “Species” which is non-numeric. Thus, we will firstly subset the data that doesnot contains “Species” column.\n\n# Subset the data to include only numeric columns\ndata_numeric &lt;- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n\n\n\n\n\n\n# Melt the correlation matrix into a long format\ndata_melt &lt;- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n\n\n\n\nWe can see no strong correlation between different variables from the heatmap created using the above code. However, this part is made to visualize the data only.\n\n\n\n\n# Sum 'charges' grouped by 'region'\ncharges &lt;- aggregate(data$charges, by=list(data$region), FUN=sum)\n\n# Sort the data frame by 'charges' in ascending order\ncharges &lt;- charges[order(charges$x, decreasing=FALSE), ]\n\n# Create a bar plot\nggplot(charges[1:5, ], aes(x=Group.1, y=x, fill=x)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  labs(title=\"Sum of Charges by Region\", x=\"Region\", y=\"Charges\") +\n  theme_minimal() +\n  scale_fill_gradient(low = \"lightgreen\", high = \"blue\")\n\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n\n\n\n\n\nTherefore, the Southeast has the highest overall medical costs, whereas the Southwest has the lowest.\nIt is always interesting to explore the data; in this case, I am interested in seeing how smoking habits affect the charges associated with insurance. Let’s explore this below:\n\n# Create a scatter plot for 'age' vs 'charges' colored by 'smoker'\np1 &lt;- ggplot(data, aes(x=age, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"red\", \"blue\")) +\n  labs(title=\"Scatter plot of Charges vs Age\", x=\"Age\", y=\"Charges\")\n\n# Create a scatter plot for 'bmi' vs 'charges' colored by 'smoker'\np2 &lt;- ggplot(data, aes(x=bmi, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"red\", \"blue\")) +\n  labs(title=\"Scatter plot of Charges vs BMI\", x=\"BMI\", y=\"Charges\")\n\n# Create a scatter plot for 'children' vs 'charges' colored by 'smoker'\np3 &lt;- ggplot(data, aes(x=children, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"green\", \"purple\")) +\n  labs(title=\"Scatter plot of Charges vs Children\", x=\"Children\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nprint(p2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nprint(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOverall, the insurance charge is significantly higher for an individual who smokes.\n\n\n\n\n\n# Create a box plot for 'charges' vs 'sex'\np1 &lt;- ggplot(data, aes(x=sex, y=charges)) +\n  geom_boxplot(fill=\"yellowgreen\") +\n  labs(title=\"Box plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a box plot for 'charges' vs 'smoker'\np2 &lt;- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_boxplot(fill=\"goldenrod1\") +\n  theme_minimal() +\n  labs(title=\"Box plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n\n\nprint(p2)\n\n\n\n\nAccording to the plot, the average insurance cost for men and women is roughly the same at $5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about $5,000. The minimum insurance premium for smokers is $5,000.\n\n# Create the box plot\nggplot(data, aes(x = as.factor(children), y = charges, fill = sex)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +\n  theme_minimal() +\n  labs(title = \"Box plot of charges vs children\", x = \"Children\", y = \"Charges\", fill = \"Sex\")\n\n\n\n\nWe can see some outliers data in different children age group.\n\n\n\nFrom the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\nModel evaluation: After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse) and R squared value.\n\n# Set the seed for reproducibility\nset.seed(0)\n\n# Create the independent variable data frame\nx &lt;- data[, !(names(data) %in% \"charges\")]\n\n# Create the dependent variable vector\ny &lt;- data$charges\n\n# Split the data into training and testing sets\nsplit &lt;- sample.split(y, SplitRatio = 0.8)\nx_train &lt;- x[split, ]\ny_train &lt;- y[split]\nx_test &lt;- x[!split, ]\ny_test &lt;- y[!split]\n\n# Fit a linear regression model\nLin_reg &lt;- lm(y_train ~ ., data = cbind(x_train, y_train))\n\n# Print the intercept and coefficients\nprint(paste(\"Intercept: \", Lin_reg$coefficients[1]))\n\n[1] \"Intercept:  -12169.8051075584\"\n\nprint(paste(\"Coefficients: \", Lin_reg$coefficients[-1]))\n\n[1] \"Coefficients:  262.305667539361\"  \"Coefficients:  -6.522038182275\"  \n[3] \"Coefficients:  344.253140133108\"  \"Coefficients:  453.766808768054\" \n[5] \"Coefficients:  23996.309146369\"   \"Coefficients:  -751.161736087778\"\n[7] \"Coefficients:  -1129.0287998095\"  \"Coefficients:  -1085.05633010975\"\n\n# Calculate R-squared on the test set\ny_pred &lt;- predict(Lin_reg, newdata = x_test)\nSSE &lt;- sum((y_pred - y_test)^2)\nSST &lt;- sum((mean(y_train) - y_test)^2)\nR_squared &lt;- 1 - SSE/SST\nprint(paste(\"R-squared: \", R_squared))\n\n[1] \"R-squared:  0.736401619028354\"\n\n# Calculate RMSE\nrmse_val &lt;- rmse(y_test, y_pred)\nprint(paste(\"RMSE: \", rmse_val))\n\n[1] \"RMSE:  5903.79948175291\"\n\n\nLets learn little bit about rmse: The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\nDuring model evaluation, RMSE serves as a measure to understand the model’s performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\nIn this case, the R-squared value is approximately 0.7364, which means that our model explains about 73.64% of the variance in the charges."
  },
  {
    "objectID": "posts/linear regression/lr.html#load-necessary-libraries",
    "href": "posts/linear regression/lr.html#load-necessary-libraries",
    "title": "Linear Regression and medical charges",
    "section": "",
    "text": "# Load packages\nlibrary(caTools)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(ggpubr)\nlibrary(corrplot)\nlibrary(reshape2)\nlibrary(Metrics)\n\n\n\n\n# Load the data into R\ndata &lt;- read.csv('/Users/test/Desktop/Machine_learning/mlblog/kchhetrii.github.io/insurance.csv')\n\n# Lets see the couple of rows of this data\nhead(data)\n\n  age    sex    bmi children smoker    region   charges\n1  19 female 27.900        0    yes southwest 16884.924\n2  18   male 33.770        1     no southeast  1725.552\n3  28   male 33.000        3     no southeast  4449.462\n4  33   male 22.705        0     no northwest 21984.471\n5  32   male 28.880        0     no northwest  3866.855\n6  31 female 25.740        0     no southeast  3756.622\n\n\n\nstr(data)\n\n'data.frame':   1338 obs. of  7 variables:\n $ age     : int  19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr  \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num  27.9 33.8 33 22.7 28.9 ...\n $ children: int  0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr  \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr  \"southwest\" \"southeast\" \"southeast\" \"northwest\" ...\n $ charges : num  16885 1726 4449 21984 3867 ...\n\n\n\n\n\n\n# Check for missing values\nsum(is.na(data))\n\n[1] 0\n\n# Handle missing values (if any)\n# data &lt;- na.omit(data)  # drops rows with missing values\n# data &lt;- data[complete.cases(data), ]  # drops rows with missing values\n# data$column &lt;- ifelse(is.na(data$column), mean(data$column, na.rm = TRUE), data$column)  # fills missing values with mean\n\nIn my case, the data does not contain the null values; however, if you select the data with the null values and want to analyze the data, it is important to deal with the null values before proceeding further analysis. Different ways to deal with the null values in R is given in the code above.\nUpon executing the above code, we got the value 0, which means there are no missing values in the data. Since we don’t have any null or missing values in this data, as indicated above. So, no further handling of missing data is needed.\n\n\n\n\n# Visualize the data\nggplot(data, aes(x=bmi, y=charges)) +\n  geom_point() +\n  geom_smooth(method=lm, col=\"red\") +\n  labs(x=\"Body Mass Index\",\n       y=\"Insurance Charges\",\n       title=\"Charge Vs BMI\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nLets take a look for the correlation between these two features: Our data contains one column “Species” which is non-numeric. Thus, we will firstly subset the data that doesnot contains “Species” column.\n\n# Subset the data to include only numeric columns\ndata_numeric &lt;- data[, sapply(data, is.numeric)]\n\n# Calculate the correlation matrix\ncorrelation_matrix &lt;- cor(data_numeric)\n\n# Print the correlation matrix\nprint(correlation_matrix)\n\n               age       bmi   children    charges\nage      1.0000000 0.1092719 0.04246900 0.29900819\nbmi      0.1092719 1.0000000 0.01275890 0.19834097\nchildren 0.0424690 0.0127589 1.00000000 0.06799823\ncharges  0.2990082 0.1983410 0.06799823 1.00000000\n\n\n\n\n\n\n# Melt the correlation matrix into a long format\ndata_melt &lt;- melt(correlation_matrix)\n\n# Create a heatmap with ggplot2\nggplot(data_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  geom_text(aes(label = round(value, 2)), size = 4) +\n  scale_fill_gradient2(low=\"blue\", high=\"red\", mid=\"white\", \n                       midpoint=0, limit=c(-1,1), space=\"Lab\", \n                       name=\"Pearson\\nCorrelation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   size = 12, hjust = 1),\n        axis.text.y = element_text(size = 12))\n\n\n\n\nWe can see no strong correlation between different variables from the heatmap created using the above code. However, this part is made to visualize the data only.\n\n\n\n\n# Sum 'charges' grouped by 'region'\ncharges &lt;- aggregate(data$charges, by=list(data$region), FUN=sum)\n\n# Sort the data frame by 'charges' in ascending order\ncharges &lt;- charges[order(charges$x, decreasing=FALSE), ]\n\n# Create a bar plot\nggplot(charges[1:5, ], aes(x=Group.1, y=x, fill=x)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  labs(title=\"Sum of Charges by Region\", x=\"Region\", y=\"Charges\") +\n  theme_minimal() +\n  scale_fill_gradient(low = \"lightgreen\", high = \"blue\")\n\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n\n\n\n\n\nTherefore, the Southeast has the highest overall medical costs, whereas the Southwest has the lowest.\nIt is always interesting to explore the data; in this case, I am interested in seeing how smoking habits affect the charges associated with insurance. Let’s explore this below:\n\n# Create a scatter plot for 'age' vs 'charges' colored by 'smoker'\np1 &lt;- ggplot(data, aes(x=age, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"red\", \"blue\")) +\n  labs(title=\"Scatter plot of Charges vs Age\", x=\"Age\", y=\"Charges\")\n\n# Create a scatter plot for 'bmi' vs 'charges' colored by 'smoker'\np2 &lt;- ggplot(data, aes(x=bmi, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"red\", \"blue\")) +\n  labs(title=\"Scatter plot of Charges vs BMI\", x=\"BMI\", y=\"Charges\")\n\n# Create a scatter plot for 'children' vs 'charges' colored by 'smoker'\np3 &lt;- ggplot(data, aes(x=children, y=charges, color=smoker)) +\n  geom_point() +\n  stat_smooth(method=\"lm\", col=\"red\") +\n  scale_color_manual(values=c(\"green\", \"purple\")) +\n  labs(title=\"Scatter plot of Charges vs Children\", x=\"Children\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nprint(p2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nprint(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOverall, the insurance charge is significantly higher for an individual who smokes."
  },
  {
    "objectID": "posts/linear regression/lr.html#section",
    "href": "posts/linear regression/lr.html#section",
    "title": "Linear Regression and medical charges",
    "section": "",
    "text": "# Create a box plot for 'charges' vs 'sex'\np1 &lt;- ggplot(data, aes(x=sex, y=charges)) +\n  geom_boxplot(fill=\"yellowgreen\") +\n  labs(title=\"Box plot of Charges vs Sex\", x=\"Sex\", y=\"Charges\")\n\n# Create a box plot for 'charges' vs 'smoker'\np2 &lt;- ggplot(data, aes(x=smoker, y=charges)) +\n  geom_boxplot(fill=\"goldenrod1\") +\n  theme_minimal() +\n  labs(title=\"Box plot of Charges vs Smoker\", x=\"Smoker\", y=\"Charges\")\n\n# Print the plots\nprint(p1)\n\n\n\nprint(p2)\n\n\n\n\nAccording to the plot, the average insurance cost for men and women is roughly the same at $5,000. The insurance costs for smokers in the right plot vary significantly from those for non-smokers; the average cost for a non-smoker is about $5,000. The minimum insurance premium for smokers is $5,000.\n\n# Create the box plot\nggplot(data, aes(x = as.factor(children), y = charges, fill = sex)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"blue\", \"green\")) +\n  theme_minimal() +\n  labs(title = \"Box plot of charges vs children\", x = \"Children\", y = \"Charges\", fill = \"Sex\")\n\n\n\n\nWe can see some outliers data in different children age group."
  },
  {
    "objectID": "posts/linear regression/lr.html#building-the-model-and-prediction",
    "href": "posts/linear regression/lr.html#building-the-model-and-prediction",
    "title": "Linear Regression and medical charges",
    "section": "",
    "text": "From the above code and analysis, we understand about the data and its distribution. Now, we can build model that fits for our data. In this particular case, we are going to use the linear regression model.\nModel evaluation: After building the model, its time to see how good the model is. We evaluate the model based on its performance which involved root mean square error (rmse) and R squared value.\n\n# Set the seed for reproducibility\nset.seed(0)\n\n# Create the independent variable data frame\nx &lt;- data[, !(names(data) %in% \"charges\")]\n\n# Create the dependent variable vector\ny &lt;- data$charges\n\n# Split the data into training and testing sets\nsplit &lt;- sample.split(y, SplitRatio = 0.8)\nx_train &lt;- x[split, ]\ny_train &lt;- y[split]\nx_test &lt;- x[!split, ]\ny_test &lt;- y[!split]\n\n# Fit a linear regression model\nLin_reg &lt;- lm(y_train ~ ., data = cbind(x_train, y_train))\n\n# Print the intercept and coefficients\nprint(paste(\"Intercept: \", Lin_reg$coefficients[1]))\n\n[1] \"Intercept:  -12169.8051075584\"\n\nprint(paste(\"Coefficients: \", Lin_reg$coefficients[-1]))\n\n[1] \"Coefficients:  262.305667539361\"  \"Coefficients:  -6.522038182275\"  \n[3] \"Coefficients:  344.253140133108\"  \"Coefficients:  453.766808768054\" \n[5] \"Coefficients:  23996.309146369\"   \"Coefficients:  -751.161736087778\"\n[7] \"Coefficients:  -1129.0287998095\"  \"Coefficients:  -1085.05633010975\"\n\n# Calculate R-squared on the test set\ny_pred &lt;- predict(Lin_reg, newdata = x_test)\nSSE &lt;- sum((y_pred - y_test)^2)\nSST &lt;- sum((mean(y_train) - y_test)^2)\nR_squared &lt;- 1 - SSE/SST\nprint(paste(\"R-squared: \", R_squared))\n\n[1] \"R-squared:  0.736401619028354\"\n\n# Calculate RMSE\nrmse_val &lt;- rmse(y_test, y_pred)\nprint(paste(\"RMSE: \", rmse_val))\n\n[1] \"RMSE:  5903.79948175291\"\n\n\nLets learn little bit about rmse: The Root Mean Square Error (RMSE) is a frequently used measure to evaluate the prediction errors of a regression model. It tells us about the distribution of the residuals (prediction errors), with a lower RMSE indicating a better fit for the data.\nDuring model evaluation, RMSE serves as a measure to understand the model’s performance. Specifically, it reveals how close the predicted values are to the actual ones. An RMSE of zero indicates perfect predictions, which, in practice, is highly unlikely if not impossible.\nIn this case, the R-squared value is approximately 0.7364, which means that our model explains about 73.64% of the variance in the charges."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#introduction-1",
    "href": "posts/anomaly detection/anomaly.html#introduction-1",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection, also known as outlier detection, is a fascinating aspect of machine learning. It involves identifying data points, events, or observations that deviate significantly from the norm. These anomalies can often provide critical and actionable insights in various domains, such as fraud detection in banking, intrusion detection in network security, and fault detection in critical systems."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#what-is-anomaly-detection",
    "href": "posts/anomaly detection/anomaly.html#what-is-anomaly-detection",
    "title": "Anomaly Detection in Machine Learning",
    "section": "What is Anomaly Detection?",
    "text": "What is Anomaly Detection?\nAnomaly detection is the process of identifying unexpected items or events in datasets, which differ from the norm. In other words, it’s about finding the ‘outliers’ in your data. For example, in a manufacturing context, an anomalous event could be a sudden increase in defective products."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#types-of-anomalies",
    "href": "posts/anomaly detection/anomaly.html#types-of-anomalies",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Types of Anomalies",
    "text": "Types of Anomalies\nThere are three main types of anomalies:\n\nPoint Anomalies: A single instance of data is anomalous if it’s too far off from the rest. For example, spending $100 on food every day during the holiday season is normal, but may be odd otherwise.\nContextual Anomalies: The abnormality is context-specific. This type of anomaly is common in time-series data. For example, spending $100 on food during the holiday season is normal, but may be odd otherwise.\nCollective Anomalies: A set of data instances collectively helps in detecting anomalies. For example, someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#anomaly-detection-techniques",
    "href": "posts/anomaly detection/anomaly.html#anomaly-detection-techniques",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Anomaly Detection Techniques",
    "text": "Anomaly Detection Techniques\nThere are several techniques used for anomaly detection, each with its strengths and weaknesses. Some of the most popular methods include:\n\nStatistical Methods: These methods model the normal data behavior using statistical parameters like mean, median, mode, variance, etc. Any data instance that doesn’t fit this model is considered an anomaly.\nMachine Learning-Based Methods: These include techniques like clustering, classification, and nearest neighbors. These methods can either be supervised (labels are available) or unsupervised (no labels).\nTime Series Analysis: This is particularly useful for sequential data, where some pattern or trend is expected. Techniques used here include state space models, decomposition methods, etc.\n\n\nLet’s get familiar with the fundamental idea underlying the hyperparameters before working with the data. We must examine a few of the hyperparameters that characterize the DBScan job in order to comprehend the idea of the core points. min_samples is the first hyperparameter (HP). This is the bare minimum of core points required for cluster formation. The second crucial HP is ‘eps’. “eps” is the greatest separation that two samples must have in order to be grouped together. Although border points are somewhat farther from the cluster center, they are nonetheless part of the same cluster as core points. All other data points are referred to as “Noise Points” because they are unrelated to any cluster. They require more research because they may be unusual or not."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#about-the-data",
    "href": "posts/anomaly detection/anomaly.html#about-the-data",
    "title": "Anomaly Detection in Machine Learning",
    "section": "About the data:",
    "text": "About the data:\nThere are seven columns and 1338 rows in the data, which indicates that there are seven separate variables. The remaining seven variables—age, sex, bmi, children, smoker, region, and charges. You can access the data from here:"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#import-necessary-libraries",
    "href": "posts/anomaly detection/anomaly.html#import-necessary-libraries",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Import necessary libraries:",
    "text": "Import necessary libraries:\n\nlibrary(fpc)\nlibrary(ggplot2)\nlibrary(dbscan)\nlibrary(cluster)\nlibrary(FNN)\nlibrary(corrplot)\n\ncorrplot 0.92 loaded"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#import-the-data-set-and-visualize-the-data",
    "href": "posts/anomaly detection/anomaly.html#import-the-data-set-and-visualize-the-data",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Import the data set and visualize the data:",
    "text": "Import the data set and visualize the data:\n\ndata &lt;- read.csv(\"/Users/test/Desktop/Machine_learning/mlblog/kamalchhetrii.github.io/insurance.csv\")\n\n\nhead(data)\n\n  Time         V1          V2        V3         V4          V5          V6\n1    0 -1.3598071 -0.07278117 2.5363467  1.3781552 -0.33832077  0.46238778\n2    0  1.1918571  0.26615071 0.1664801  0.4481541  0.06001765 -0.08236081\n3    1 -1.3583541 -1.34016307 1.7732093  0.3797796 -0.50319813  1.80049938\n4    1 -0.9662717 -0.18522601 1.7929933 -0.8632913 -0.01030888  1.24720317\n5    2 -1.1582331  0.87773675 1.5487178  0.4030339 -0.40719338  0.09592146\n6    2 -0.4259659  0.96052304 1.1411093 -0.1682521  0.42098688 -0.02972755\n           V7          V8         V9         V10        V11         V12\n1  0.23959855  0.09869790  0.3637870  0.09079417 -0.5515995 -0.61780086\n2 -0.07880298  0.08510165 -0.2554251 -0.16697441  1.6127267  1.06523531\n3  0.79146096  0.24767579 -1.5146543  0.20764287  0.6245015  0.06608369\n4  0.23760894  0.37743587 -1.3870241 -0.05495192 -0.2264873  0.17822823\n5  0.59294075 -0.27053268  0.8177393  0.75307443 -0.8228429  0.53819555\n6  0.47620095  0.26031433 -0.5686714 -0.37140720  1.3412620  0.35989384\n         V13        V14        V15        V16         V17         V18\n1 -0.9913898 -0.3111694  1.4681770 -0.4704005  0.20797124  0.02579058\n2  0.4890950 -0.1437723  0.6355581  0.4639170 -0.11480466 -0.18336127\n3  0.7172927 -0.1659459  2.3458649 -2.8900832  1.10996938 -0.12135931\n4  0.5077569 -0.2879237 -0.6314181 -1.0596472 -0.68409279  1.96577500\n5  1.3458516 -1.1196698  0.1751211 -0.4514492 -0.23703324 -0.03819479\n6 -0.3580907 -0.1371337  0.5176168  0.4017259 -0.05813282  0.06865315\n          V19         V20          V21          V22         V23         V24\n1  0.40399296  0.25141210 -0.018306778  0.277837576 -0.11047391  0.06692807\n2 -0.14578304 -0.06908314 -0.225775248 -0.638671953  0.10128802 -0.33984648\n3 -2.26185710  0.52497973  0.247998153  0.771679402  0.90941226 -0.68928096\n4 -1.23262197 -0.20803778 -0.108300452  0.005273597 -0.19032052 -1.17557533\n5  0.80348692  0.40854236 -0.009430697  0.798278495 -0.13745808  0.14126698\n6 -0.03319379  0.08496767 -0.208253515 -0.559824796 -0.02639767 -0.37142658\n         V25        V26          V27         V28 Amount Class\n1  0.1285394 -0.1891148  0.133558377 -0.02105305 149.62     0\n2  0.1671704  0.1258945 -0.008983099  0.01472417   2.69     0\n3 -0.3276418 -0.1390966 -0.055352794 -0.05975184 378.66     0\n4  0.6473760 -0.2219288  0.062722849  0.06145763 123.50     0\n5 -0.2060096  0.5022922  0.219422230  0.21515315  69.99     0\n6 -0.2327938  0.1059148  0.253844225  0.08108026   3.67     0\n\n\n\nLets see if the data contains any null values:\n\nsum(is.na(data))\n\n[1] 0\n\n\nThis data set does not contain null values. Thus, we don’t have to deal with it further."
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#explanatory-analysis",
    "href": "posts/anomaly detection/anomaly.html#explanatory-analysis",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Explanatory analysis:",
    "text": "Explanatory analysis:\n\n# Select the relevant columns\ndata_selected &lt;- data[, c('age', 'bmi', 'charges', 'children')]\n\n# Calculate the correlation matrix\ncorrs &lt;- cor(data_selected)\n\n# Create a heatmap with correlation values\ncorrplot(corrs, method=\"color\", type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", # Change correlation coefficients color to white\n         tl.col=\"black\", tl.srt=45, # Text label color and rotation\n         col = colorRampPalette(c(\"blue\", \"white\", \"red\"))(200), # Change color scheme\n         title=\"Correlation Heatmap\") # Add title\n\n\n\n\nFrom this, we can see there is not much strong correlation between the different variables.\n\nLets see the distribution of charges variable:\n\n# Create a histogram of charges\nggplot(data, aes(x=charges)) +\n  geom_histogram(binwidth=1000, color=\"black\", fill=\"lightblue\") +\n  labs(title=\"Distribution of Charges\", x=\"Charges\", y=\"Count\") +\n  theme_minimal()\n\n\n\n\nFrom this histogram, we can see the distribution of our charges data which is left skewed and there is the possibility that this data set contains the outlier.\nLet’s explore the data set for charges for males and female:\n\n# Create a boxplot of charges by sex\nggplot(data, aes(x=sex, y=charges, color=sex)) +\n  geom_boxplot() +  # Include outliers\n  geom_jitter(width=0.2, alpha=0.5) +  # Add jittered points for better visualization\n  labs(title=\"Charges by Sex\", x=\"Sex\", y=\"Charges\") +\n  theme_minimal() +\n  scale_color_manual(values=c(\"blue\", \"darkgreen\"))  # Specify colors for each sex\n\n\n\n\nNow, lets see the charges by bmi for further data exploration:\n\n# Create a scatter plot of BMI vs charges\nggplot(data, aes(x=bmi, y=charges, color=sex)) +\n  geom_point(alpha=0.5) +  # Add points with transparency for better visualization\n  labs(title=\"BMI vs Charges\", x=\"BMI\", y=\"Charges\") +\n  theme_minimal() +\n  scale_color_manual(values=c(\"red\", \"blue\"))  # Specify colors for each sex"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#calculate-the-epsilon-value-using-k-distance-graph",
    "href": "posts/anomaly detection/anomaly.html#calculate-the-epsilon-value-using-k-distance-graph",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Calculate the epsilon value using K-distance graph:",
    "text": "Calculate the epsilon value using K-distance graph:\n\n# Load necessary libraries\nlibrary(FNN)\n\n# Select the relevant columns\ndata_selected &lt;- data[, c('age', 'bmi')]\n\n# Standardize the data\ndata_std &lt;- scale(data_selected)\n\n# Compute the nearest neighbors\nk &lt;- 2  # 2 because the point itself is included\nknn_dist &lt;- knn.dist(data_std, k=k)\n\n# Sort the distances\nknn_dist &lt;- sort(knn_dist[,k], decreasing=FALSE)  # Exclude the distance to the point itself\n\n# Plot the k-distance graph\nplot(knn_dist, main=\"K-distance Graph\", xlab=\"Data Points sorted by distance\", ylab=\"Epsilon\")\n\n\n\n\nThe optimum value of epsilon is at the point of maximum curvature in the K-Distance Graph, which is 0.6 in this case. Domain knowledge affects minPoints’ value. I’m using 10 minPoints at this time around.\nBased on the above calculation, we can do the DBSCAN clustering\n\n# Select the relevant columns\ndata_selected &lt;- data[, c('age', 'bmi')]\n\n# Standardize the data\ndata_std &lt;- scale(data_selected)\n\n# Perform DBSCAN clustering\ndbscan_res &lt;- dbscan(data_std, eps=0.6, minPts=10)  \nprint(dbscan_res)\n\nDBSCAN clustering for 1338 objects.\nParameters: eps = 0.6, minPts = 10\nUsing euclidean distances and borderpoints = TRUE\nThe clustering contains 1 cluster(s) and 3 noise points.\n\n   0    1 \n   3 1335 \n\nAvailable fields: cluster, eps, minPts, dist, borderPoints"
  },
  {
    "objectID": "posts/anomaly detection/anomaly.html#visualization",
    "href": "posts/anomaly detection/anomaly.html#visualization",
    "title": "Anomaly Detection in Machine Learning",
    "section": "Visualization:",
    "text": "Visualization:\n\n# Add the DBSCAN results to our data\ndata$cluster &lt;- as.factor(dbscan_res$cluster)\n\n# Define colors for each cluster\ncolors &lt;- rainbow(length(unique(dbscan_res$cluster)))\nnames(colors) &lt;- levels(data$cluster)\n\n# Create the plot\nggplot(data, aes(x=age, y=bmi, color=cluster)) +\n  geom_point() +\n  scale_color_manual(values = colors) +\n  theme_minimal() +\n  labs(x=\"Age\", y=\"BMI\", color=\"Cluster\") +\n  theme(legend.position=\"none\")\n\n\n\n\nFrom the above scatterplot, we can see that three data points are the noise and one cluster with no outliers.\n\n# Create an outliers data frame\noutliers &lt;- data[data$cluster == -1, ]\n\n# Print the outliers\nprint(outliers)\n\n[1] age      sex      bmi      children smoker   region   charges  cluster \n&lt;0 rows&gt; (or 0-length row.names)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Probability/probability.html",
    "href": "posts/Probability/probability.html",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "",
    "text": "Probability theory serves as a cornerstone for Machine Learning (ML). It offers an approach to expressing statements and formulating the learning problem. In this blog post we will explore the aspects of probability theory and random variables and their significant impact on ML. Additionally we will provide an example, in R to demonstrate these concepts."
  },
  {
    "objectID": "posts/Probability/probability.html#probability-theory",
    "href": "posts/Probability/probability.html#probability-theory",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability theory is a field, within mathematics that tackles the concept of uncertainty. It offers a structure to help us quantify our beliefs or uncertainties. The probability of an event serves as a metric to gauge the likelihood of its occurrence.\nIn machine learning we frequently encounter situations where dealing with uncertainty becomes essential. Take the case of constructing a spam classifier for instance. We often find ourselves unsure whether an incoming email should be classified as spam or not. Probability theory acts as a framework to navigate through this uncertainty and make decisions."
  },
  {
    "objectID": "posts/Probability/probability.html#random-variables",
    "href": "posts/Probability/probability.html#random-variables",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable refers to a variable that takes on values based on the outcomes of an event. There are two types of variables; discrete and continuous. A discrete random variable has an countable number of values while a continuous random variable can have an infinite number of possible values, within a range along the real number line.\nWhen it comes to machine learning random variables can represent elements. For instance in a spam classifier a random variable may indicate whether an email is classified as spam (1) or not (0).\nTo better understand the comparison, between continuous variables you can refer to the table provided below.\n\nFigure credit: Discrete vs continuous"
  },
  {
    "objectID": "posts/Probability/probability.html#probability-distributions",
    "href": "posts/Probability/probability.html#probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nA probability distribution describes how a random variable is distributed. It tells us what the probabilities of each outcome are. For discrete random variables, we use a probability mass function (PMF). For continuous random variables, we use a probability density function (PDF).\nWith the facts at hand, one can utilize the decision tree depicted in the picture below to get an understanding of some common probability distributions:\n\nFigure credit: Probabilistic approaches to risk by Aswath Damodaran."
  },
  {
    "objectID": "posts/Probability/probability.html#discrete-probability-distributions",
    "href": "posts/Probability/probability.html#discrete-probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Discrete Probability Distributions:",
    "text": "Discrete Probability Distributions:\nA discrete probability distribution applies to scenarios where the set of possible outcomes is discrete. Common examples of discrete probability distributions include the Bernoulli, Binomial, and Poisson distributions.\nExample: Binomial distribution\nLet's create a hypothetical scenario where we toss a coin 10 times. We are interested in the number of times we get heads. This scenario follows a binomial distribution. Lets do the data analysis for better understanding of it.\nLoad necessary libraries:\n\n# Load necessary library\nlibrary(ggplot2)\n\nSet the random seed for reproducibility:\n\n# Set parameters\nsize &lt;- 10\nprob &lt;- 0.5\n\n# Generate binomial data\nset.seed(123)\ndata &lt;- rbinom(1000, size, prob)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\") +\n  labs(x = \"Number of Heads\", y = \"Frequency\", title = \"Binomial Distribution\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/Probability/probability.html#continuous-probability-distributions",
    "href": "posts/Probability/probability.html#continuous-probability-distributions",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Continuous Probability Distributions:",
    "text": "Continuous Probability Distributions:\nA continuous probability distribution applies to scenarios where the set of possible outcomes is an interval of real numbers. Common examples of continuous probability distributions include the Normal, Exponential, and Beta distributions.\nExample: Normal distiribution\nLet's create a hypothetical scenario where we measure the heights of a large group of individuals. The heights of individuals in a large population often follow a normal distribution.\n\n# Set parameters\nmean &lt;- 170\nsd &lt;- 10\n\n# Generate normal data\nset.seed(123)\ndata &lt;- rnorm(1000, mean, sd)\n\n# Create a histogram\nggplot(data.frame(x = data), aes(x = x)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\", aes(y = ..density..)) +\n  geom_density(alpha = 0.2, fill = \"#FF6666\") +\n  labs(x = \"Height (cm)\", y = \"Density\", title = \"Normal Distribution\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead."
  },
  {
    "objectID": "posts/Probability/probability.html#conclusion",
    "href": "posts/Probability/probability.html#conclusion",
    "title": "Probability Theory and Random Variables in Machine Learning",
    "section": "Conclusion:",
    "text": "Conclusion:\nProbability theory and random variables are essential tools. Understanding discrete and continuous probability distributions is crucial in machine learning. They provide a foundation for many machine learning algorithms and statistical tests. They provide a framework to handle uncertainty and formulate learning problems. Understanding these concepts can help you build more effective and robust machine learning models."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/DBSCAN/index.html",
    "href": "posts/DBSCAN/index.html",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data’s density."
  },
  {
    "objectID": "posts/DBSCAN/index.html#introduction",
    "href": "posts/DBSCAN/index.html#introduction",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm used in machine learning. Unlike other clustering algorithms such as K-means or hierarchical clustering, DBSCAN does not require the user to specify the number of clusters a priori. Instead, it infers the number of clusters based on the data’s density."
  },
  {
    "objectID": "posts/DBSCAN/index.html#how-dbscan-works",
    "href": "posts/DBSCAN/index.html#how-dbscan-works",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "How DBSCAN Works",
    "text": "How DBSCAN Works\nDBSCAN works by defining a cluster as a maximal set of density-connected points. It starts with an arbitrary point in the dataset. If there are at least minPts within a radius of eps from that point, a new cluster is created. The algorithm then iteratively adds all directly reachable points to the cluster. Once no more points can be added, the algorithm proceeds to the next unvisited point in the dataset."
  },
  {
    "objectID": "posts/DBSCAN/index.html#advantages-of-dbscan",
    "href": "posts/DBSCAN/index.html#advantages-of-dbscan",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Advantages of DBSCAN",
    "text": "Advantages of DBSCAN\nDBSCAN has several advantages over other clustering algorithms:\n\nNo need to specify the number of clusters: As mentioned earlier, DBSCAN does not require the user to specify the number of clusters a priori. This can be particularly useful when the number of clusters is not known beforehand.\nAbility to find arbitrarily shaped clusters: Unlike K-means, which tends to find spherical clusters, DBSCAN can find clusters of arbitrary shapes.\nRobustness to noise: DBSCAN is less sensitive to noise and outliers, as it only adds points that are directly reachable according to the density criteria."
  },
  {
    "objectID": "posts/DBSCAN/index.html#disadvantages-of-dbscan",
    "href": "posts/DBSCAN/index.html#disadvantages-of-dbscan",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Disadvantages of DBSCAN",
    "text": "Disadvantages of DBSCAN\nDespite its advantages, DBSCAN also has some limitations:\n\nDifficulty handling varying densities: DBSCAN struggles with datasets where clusters have significantly different densities. This is because a single eps and minPts value may not be suitable for all clusters.\nSensitivity to parameter settings: The results of DBSCAN can be significantly affected by the settings of eps and minPts. Choosing appropriate values for these parameters can be challenging.\n\nIn general, density-based clustering algorithms can be quite successful for a wide range of clustering tasks, particularly when the data is shaped and has different densities. When using the algorithm with a specific dataset, it is crucial to pay close attention to the parameters and take the algorithm’s constraints into account.\nAccording to the research report, the concept of dense regions forms the basis of DBSCAN. It is assumed that points in dense locations make up natural clusters. The term “dense region” has to be defined for this. These two parameters are necessary for the DBSCAN algorithm to function.\n\nEps, ε: distance\nMinPts: The bare minimum of points within a given distance Eps\n\n\n\n\nIn this blog post, we will perform a DBSCAN clustering analysis on an insurance dataset using R."
  },
  {
    "objectID": "posts/DBSCAN/index.html#data",
    "href": "posts/DBSCAN/index.html#data",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Data:",
    "text": "Data:\nIn this blog post, we will do the clustering analysis using the DBSCAN clustering method. Regarding the choice of this algorithm is explain above. Please have a look if you want to learn more. Regarding the data, you can access this data from here.\n\nLoad the necessary libraries:\n\nlibrary(fpc) \nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(corrplot)\nlibrary(dbscan)"
  },
  {
    "objectID": "posts/DBSCAN/index.html#load-the-data-into-r",
    "href": "posts/DBSCAN/index.html#load-the-data-into-r",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Load the data into R:",
    "text": "Load the data into R:\n\ndata &lt;- read.csv(\"/Users/test/Downloads/Mall_customers.csv\")\nhead(data)\n\n  CustomerID Gender Age Annual_Income Spending_Score\n1          1   Male  19            15             39\n2          2   Male  21            15             81\n3          3 Female  20            16              6\n4          4 Female  23            16             77\n5          5 Female  31            17             40\n6          6 Female  22            17             76\n\nstr(data)\n\n'data.frame':   200 obs. of  5 variables:\n $ CustomerID    : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Gender        : chr  \"Male\" \"Male\" \"Female\" \"Female\" ...\n $ Age           : int  19 21 20 23 31 22 35 23 64 30 ...\n $ Annual_Income : int  15 15 16 16 17 17 18 18 19 19 ...\n $ Spending_Score: int  39 81 6 77 40 76 6 94 3 72 ..."
  },
  {
    "objectID": "posts/DBSCAN/index.html#data-cleaning",
    "href": "posts/DBSCAN/index.html#data-cleaning",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Data Cleaning:",
    "text": "Data Cleaning:\n\n# To see if the given dataset contains any null values or not\nsum(is.na(data_num))\n\n[1] 0"
  },
  {
    "objectID": "posts/DBSCAN/index.html#explanatory-analysis",
    "href": "posts/DBSCAN/index.html#explanatory-analysis",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Explanatory analysis",
    "text": "Explanatory analysis\n\n# Select the relevant columns\ndata_selected &lt;- data_num[, c('Age', 'Annual_Income', 'Spending_Score')]\n\n# Calculate the correlation matrix\ncorrs &lt;- cor(data_selected)\n\n# Create a heatmap with correlation values\ncorrplot(corrs, method=\"color\", type=\"upper\", order=\"hclust\", \n         addCoef.col = \"black\", # Add correlation coefficients\n         tl.col=\"black\", tl.srt=45) # Text label color and rotation\n\n\n\n\nFrom the above correlation table, we can see that either we have negative values which indicated that they are negatively correlated or very low values indicating not a strong correlation between them\n\nlets explore the data more:\n\nDistribution of the variables:\n\n# Create a histogram for 'age'\nggplot(data, aes(x=Age)) +\n  geom_histogram(bins = 30, fill =  \"blue\", color = \"black\") +\n  labs(title=\"Distribution of Age\", x=\"Age\", y=\"Count\")\n\n\n\n# Create a histogram for 'income'\nggplot(data, aes(x=Annual_Income)) +\n  geom_histogram(bins = 30, fill = 'green', color = 'black') +\n  labs(title=\"Distribution of Income\", x=\"Income\", y=\"Count\")\n\n\n\n#Create a histogram for spending score:\nggplot(data, aes(x=Spending_Score))+\n  geom_histogram(bins = 30, fill='darkgreen', color='black')+\n  labs(title= \"Distribution of Spending Score\", x='spending score', y= 'count')\n\n\n\n\nFrom the above distribution, We can see that the age group near 30-40 has the highest density, most customers have income in the range of 50-80k, and most customers have a spending score of 50.\nLets see the box plot for Gender by Spending Score:\n\n# Subset the data\nmale_charges &lt;- data[data$Gender == \"Male\", \"Spending_Score\"]\nfemale_charges &lt;- data[data$Gender == \"Female\", \"Spending_Score\"]\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  Gender = rep(c(\"Male\", \"Female\"), times = c(length(male_charges), length(female_charges))),\n  Charges = c(male_charges, female_charges)\n)\n# Create the box plot\nggplot(plot_data, aes(x = Gender, y = Charges, fill = Gender)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Box Plot of Charges by Gender\", x = \"Gender\", y = \"Spending Score\", fill = \"Gender\")\n\n\n\n\n\n#Average Spending score by gender:\n# Calculate the average bmi for each region\navg_bmi_by_region &lt;- data %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(Average_Spending_score = mean(Spending_Score))\n\nprint(avg_bmi_by_region)\n\n# A tibble: 2 × 2\n  Gender Average_Spending_score\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 Female                   51.5\n2 Male                     48.5\n\n\nLets see the relationship between Annual income and spending score:\n\n# Create a scatter plot with regression line\nggplot(data, aes(x=Annual_Income, y=Spending_Score)) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE, color=\"red\") +\n  labs(title=\"Relation between Annual Income and Spending Score\", x=\"Annual Income\", y=\"Spending Score\")\n\n\n\n\nCalculate and print the correlation coefficient:\n\ncorrelation &lt;- cor(data$Annual_Income, data$Spending_Score)\nprint(paste(\"Correlation coefficient: \", correlation))\n\n[1] \"Correlation coefficient:  0.00990284809403761\"\n\n\nFrom this, we can see that there is no correlation between Annual income vs Spending Score."
  },
  {
    "objectID": "posts/DBSCAN/index.html#lets-calculate-silhouette-score",
    "href": "posts/DBSCAN/index.html#lets-calculate-silhouette-score",
    "title": "Understanding DBSCAN Clustering Analysis in Machine Learning",
    "section": "Lets calculate Silhouette Score:",
    "text": "Lets calculate Silhouette Score:\nBefore dealing with the calculation of the Silhouette Score, lets get acquainted with what the Silhouette Score is and its importance to us while doing the DBSCAN clustering:\nThe Silhouette Score measures how similar an object is to its cluster compared to others. It ranges from -1 to 1, where a high value indicates that the object is well-matched to its cluster and poorly matched to neighboring clusters. The clustering configuration is appropriate if most objects have a high value. If many points have a low or negative value, the clustering configuration may have too many clusters.\nThe silhouette score provides a succinct graphical representation of how well each object lies within its cluster. It is a way to track the validity of the clusters formed by the algorithm. It can be particularly useful in the context of DBSCAN, as the algorithm does not explicitly minimize or maximize any particular objective function.\n\n# Loop over all combinations of parameter values\nfor (eps in eps_values) {\n  for (min_samples in min_samples_values) {\n    # Perform DBSCAN clustering\n    dbscan_result &lt;- dbscan(df, eps = eps, minPts = min_samples)\n    \n    # Append the number of clusters to the 'clusters' vector\n    clusters &lt;- c(clusters, max(dbscan_result$cluster))\n    \n    # Calculate the silhouette score and append it to the 'sil_score' vector\n    sil_score &lt;- c(sil_score, cluster.stats(dist(df), dbscan_result$cluster)$avg.silwidth)\n  }\n}\n\n# Create a data frame with the results\ndbscan_df &lt;- expand.grid(Eps = eps_values, Min_Samples = min_samples_values)\ndbscan_df$Number_of_Clusters &lt;- clusters\ndbscan_df$Silhouette_Score &lt;- sil_score\n\n# Print the data frame\nprint(dbscan_df)\n\n   Eps Min_Samples Number_of_Clusters Silhouette_Score\n1   11           5                  5       0.15950489\n2   12           5                  4       0.18222045\n3   13           5                  5       0.04014986\n4   14           5                  4       0.14733884\n5   15           5                  4       0.13127178\n6   16           5                  4       0.09690751\n7   11           6                  3       0.03839425\n8   12           6                  4      -0.04371114\n9   13           6                  4      -0.04716035\n10  14           6                  5       0.17636596\n11  15           6                  3       0.18050065\n12  16           6                  4       0.20657473\n13  11           7                  4       0.20473300\n14  12           7                  4       0.15845864\n15  13           7                  3       0.19532476\n16  14           7                  3       0.19532476\n17  15           7                  3       0.09531062\n18  16           7                  3       0.05953447\n19  11           8                  3       0.19456187\n20  12           8                  4       0.23057899\n21  13           8                  4       0.12729075\n22  14           8                  3       0.19988329\n23  15           8                  4       0.21660975\n24  16           8                  4       0.18699359\n25  11           9                  3       0.24719452\n26  12           9                  3       0.21578135\n27  13           9                  4       0.19421971\n28  14           9                  2       0.26949650\n29  15           9                  4       0.27034017\n30  16           9                  4       0.24966531\n31  11          10                  3       0.25886559\n32  12          10                  3       0.21178070\n33  13          10                  3       0.19155599\n34  14          10                  4       0.22168976\n35  15          10                  3       0.25894850\n36  16          10                  3       0.23688910\n37  11          11                  2       0.27196809\n38  12          11                  2       0.27248722\n39  13          11                  4       0.28921007\n40  14          11                  3       0.26889522\n41  15          11                  4       0.15181851\n42  16          11                  3       0.21849659\n43  11          12                  4       0.24945312\n44  12          12                  4       0.21884485\n45  13          12                  3       0.26591736\n46  14          12                  1       0.32699544\n47  15          12                  1       0.33022559\n48  16          12                  2       0.27662364\n49  11          13                  2       0.25255951\n50  12          13                  3       0.22481604\n51  13          13                  2       0.26248415\n52  14          13                  2       0.22931199\n53  15          13                  2       0.21113928\n54  16          13                  3       0.20679545\n\n\n\n# Convert 'Silhouette_Score' to numeric\ndbscan_df$Silhouette_Score &lt;- as.numeric(as.character(dbscan_df$Silhouette_Score))\n\n# Find the maximum silhouette score\nmax_sil_score &lt;- max(dbscan_df$Silhouette_Score)\n\n# Filter the data frame for the maximum silhouette score\ndbscan_df[dbscan_df$Silhouette_Score == max_sil_score, ]\n\n   Eps Min_Samples Number_of_Clusters Silhouette_Score\n47  15          12                  1        0.3302256\n\n\nThe stronger the clustering, the closer the result is near 1. We have reached a maximum Silhouette Score of 0.3302256 with Eps = 15 and Min Samples = 12. To get the best clustering, we will fit these values into the DBSCAN algorithm.\nPerform DBSCAN clustering:\n\ndbscan_result &lt;- dbscan(df, eps = 15, minPts = 12)\n\n# Add the cluster assignments to the data frame\ndf$DBSCAN_Clusters &lt;- dbscan_result$cluster\n\n# Sort the data frame by the 'DBSCAN_Clusters' column\ndf &lt;- df[order(df$DBSCAN_Clusters), ]\n\n# Print the data frame\nprint(head(df))\n\n  Gender Age Annual_Income Spending_Score DBSCAN_Clusters\n1      0  19            15             39               0\n3      1  20            16              6               0\n5      1  31            17             40               0\n7      1  35            18              6               0\n8      1  23            18             94               0\n9      0  64            19              3               0\n\n\n\n# Convert 'DBSCAN_Clusters' to character\ndf$DBSCAN_Clusters &lt;- as.character(df$DBSCAN_Clusters)\n\n# Replace '-1' with 'Outliers'\ndf$DBSCAN_Clusters[df$DBSCAN_Clusters == \"-1\"] &lt;- \"Outliers\"\n\n\n# Create the scatter plot\np &lt;- ggplot(df, aes(x = Annual_Income, y = Spending_Score, color = DBSCAN_Clusters)) +\n  geom_point(size = 4, alpha = 0.8) +\n  scale_color_manual(values = c(\"black\", \"darkred\", \"#0091F7\", \"darkgreen\", \"#F7F700\")) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 20),\n    legend.title = element_text(size = 12),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12)\n  ) +\n  labs(\n    title = \"DBSCAN Clusters\",\n    x = \"Annual Income\",\n    y = \"Sum of Spending Scores\",\n    color = \"Clusters\"\n  )\n\n# Print the plot\nprint(p)\n\n\n\n\n\nConclusion:\nAs we can see, the lack of substantial density in our data causes DBSCAN to perform poorly. The black label indicates outliers, so it will mostly appear as such. Having a more dense data structure means we can get better performance from DBSCAN clustering."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my Machine Learning blog",
    "section": "",
    "text": "Understanding DBSCAN Clustering Analysis in Machine Learning\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nKamal\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Crop Suitability with Machine Learning using R\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables in Machine Learning\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection in Machine Learning\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression and medical charges\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nKamal Chhetri\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]